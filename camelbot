#!/usr/bin/perl
# author: seth
# description: a wikibot

use strict;
use warnings;
use Data::Dumper;          # for debugging purposes
use Encode;
use File::Path qw(make_path); # create directories
use File::Slurp qw(slurp write_file); # read/write files
use Getopt::Long qw(:config bundling); # cli params
use IPC::Run;              # used for irc bot
#use List::Util 'shuffle';
use LWP::UserAgent;        # fast, small web browser
use Mail::Sendmail;        # sending e-mails
use MediaWiki::Bot;        # bot for mediawiki, uses mediawiki-api-interface
use Pod::Usage;            # cli params help
use POSIX qw/strftime/;    # format timestamp
use Term::ReadKey;         # used for user input
use Text::Diff;
use Time::Local;           # timegm
use Tk::DiffText;          # graphical diff tool
use URI::Escape;           # uri_unescape
use WWW::Mechanize;        # big web browser

# global vars
$main::VERSION = '0.77.20150103';
# comment 2014-12-03, seth: char class $_re_url_class contained | char in function cleanup_wiki_page; now deleted
my $_re_url_class = qr/[^\]\[<>"'\x00-\x20\x7F)]|\)(?!\s)/;
my $_re_ASIN = qr/\bASIN[: ]+(?:B[0-9]{2}[A-Z0-9]{7}|[0-9]{9}[0-9X])/;

# functions
# =========
# mediawiki functions
# -------------------
# sub cat_add
# 	adds a given bunch of pages to a given category by adding [[category:new]] at the end
#
# sub cat_rename
# 	moves a category and replaces all [[category:old]] by [[category:new]]
#
# sub check_external_link
# 	given two successive parts $a1, $a2 of a string $a this function checks, 
# 	whether second part of string $a2 is an url with http status 200. the first 
# 	part of the string $a1 is used to detect the end of url in part two $a2.
#
# sub cleanup_wiki_page
# 	standard cleaning up
#
# sub delete_wiki_page
# 	deletes a wiki page
#
# sub download_css
# 	download a css file (not implemented yet: and all files loaded inside css)
#
# sub download_files
# 	downloads files from the wiki
#
# sub download_pages_by_prefix
# 	downloads all pages with the same prefix
#
# sub getAbuseFilterData
# 	not yet implemented: return some info on abuse filter
#
# sub get_tables_from_wikitext
# 	tries to return all tables from a given wikitext. (this will fail on tables in tables!)
#
# sub get_user_contribs
# 	get list of pages edited by user
#
# sub get_pages_by_prefix
# 	get all pages and their latest content by prefix
#
# sub is_allowed
# 	checks whether bot-template is placed on a page and forbids editing
#
# sub link_replacement
# 	replaces links in wiki pages
#
# sub login
# 	logs a bot in to some wiki
#
# sub newest_post_info
# 	\return hash ref with thread, author and date of newest post
# 	\param[in] $wikitext content of a talk page
#
# sub notifier
# 	notify on user or pre-defined wiki pages, if particular edits occur
#
# sub parse_wikitext
# 	use mediawiki-api of a wiki to parse wikitext
#
# sub parse_page
# 	use mediawiki-api of a wiki to get a parsed wikipage
#
# sub php_code_unused
# 	searches for all links in a given piece of wikitext
#
# sub post_process_html
# 	clean up html that was generated by parsing wikitext
#
# sub rebuild_table
# 	builds a table and pastes it to a section (overwriting the section)
#
# sub save_wiki_page
# 	\brief saves a wiki page
#		\param[in] $bot a logged in MediaWiki::Bot
#		\param[in] $page_utf8 page name in utf8
#		\param[in] $summary an edit summary (without 'Bot: ') that will be used when 
#			saving
#		\param[in] $text a ref to the new wikitext
#		\param[in] $orig_text a ref to the old wikitext
#		\param[in] $time_stack a ref to an array that is used for low frequency editing
#		\param[in] $user_answer a ref to a string containing '' (nothing) or 'y' to 
#			save the last user's answer
#
# sub table2wikitext
# 	converts a special table hash ref to wikitext
#
# sub table_body2array
# 	convert wiki table body to perl ref to 2d array
#
# sub test_and_replace
# 	tests whether a regexp matches and replaces it. give some verbose output.
# 	checks urls, breaks if http status != 200
# 	returns number of replacements
#
# sub text_replacement
# 	replaces text in wiki pages
#
# sub time_management
# 	forces that only x edits per minute are done 
#
# sub title2filename
# 	converts a page title to a filename
#
# sub title2url_part
# 	convert article name to part of url with article name
#
# sub update_edit_filter_index
# 	update page with index of present threads about edit filters
#
# sub update_maintenance_lists
# 	update some maintenance lists on given edit
#
# sub upload_file
# 	uploads a file to the wiki
#
# sub url2title
# 	convert url to article name
#
# sub wikitableHeader2array
# 	convert wiki table header to perl ref to 1d array
#
# sub wikitable2array
# 	convert wiki table to perl ref to hash, containing a header and a body containing a 2d array
#
# other functions
# ---------------
# sub get_http_status
# 	return http status code of a given url
#
# sub msg
# 	write message to STDOUT
#
# sub num_unique_elem
# 	return number of elements, without counting double entries
#
# sub parse_rc
# 	parse recent changes told by bot via irc. return ref to hash with content.
#
# sub read_file_binary
# 	read a file binary
#
# sub syntaxCheck
# 	parse cli params
#
# sub show_diff
# 	shows a diff via Tk
#
# sub write_csv
# 	write a 2d-array (= ref to array of refs to arrays) to a csv file

sub cat_add{
	my $bot = shift;
	my $pages = shift;
	my $cat = shift;
	utf8::decode($cat);
	my $globaloptions = shift;     # global options
	my %options = %$globaloptions; # local options inherit from global options
	my $summary = '+cat';
	my @time_stack = ();           # needed for edit/min constraint
	my $user_answer = '';          # user answer initially is "no"
	$options{'askuser'}           = $globaloptions->{'askuser'}           //  1; # 0 = don't ask user before each replacement, 1 = ask user every time
	$options{'simulation'}        = $globaloptions->{'simulation'}        //  1; # 0 = append on real pages, 1 = just simulate
	$options{'max_edits_per_min'} = $globaloptions->{'max_edits_per_min'} // -1; # maximum number of edits per minute (-1 = inf)
	$options{'verbose'}           = $globaloptions->{'verbose'}           //  1; # level of verbosity
	my $cat_first = lc substr($cat, 0, 1);
	my $cat_tail = substr($cat, 1);
	msg("start cat adding...") if $options{'verbose'} > 0;
	for my $page (@$pages){
		msg("page = $page") if $options{'verbose'} > 0;
		my $page_utf8 = $page;
		utf8::decode($page_utf8);
		my $text = $bot->get_text($page_utf8);
		next unless is_allowed(\$text, $options{'username'}, $page);
		# page is already in cat
		next if $text =~ /\[\[[Cc]ategory:(?i:\Q$cat_first\E)\Q$cat_tail\E\]\]/;
		msg("add page to cat.") if $options{'verbose'} > 0;
		# add page to cat
		my $text_bak = $text;
		if($text =~ /\[\[[cC]ategory:/){
			$text =~ s/.*\K(?=\[\[[cC]ategory:)/[[category:$cat]]\n/g;
		}else{
			$text =~ s/\s*$/\n\n[[category:$cat]]/g;
		}
		time_management(\@time_stack, $options{'max_edits_per_min'}, $options{'verbose'});
		save_wiki_page($bot, $page_utf8, $summary, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
	}
}

sub cat_rename{
	my $bot = shift;
	my $old_cat = shift;
	my $new_cat = shift;
	utf8::decode($old_cat);
	utf8::decode($new_cat);
	my $globaloptions = shift;     # global options
	my %options = %$globaloptions; # local options inherit from global options
	my $summary = 'cat change';
	my @time_stack = ();           # needed for edit/min constraint
	my $user_answer = '';          # user answer initially is "no"
	$options{'askuser'}           = $globaloptions->{'askuser'}           //  1; # 0 = don't ask user before each replacement, 1 = ask user every time
	$options{'simulation'}        = $globaloptions->{'simulation'}        //  1; # 0 = append on real pages, 1 = just simulate
	$options{'max_edits_per_min'} = $globaloptions->{'max_edits_per_min'} // -1; # maximum number of edits per minute (-1 = inf)
	$options{'verbose'}           = $globaloptions->{'verbose'}           //  1; # level of verbosity
	my $old_cat_first = lc substr($old_cat, 0, 1);
	my $old_cat_tail = substr($old_cat, 1);
	msg("start cat renaming...") if $options{'verbose'} > 0;
	my @pages = $bot->get_pages_in_category("Category:$old_cat");
	msg('found '.@pages." pages:") if $options{'verbose'} > 0;
	for my $page (@pages){
		msg("page = $page") if $options{'verbose'} > 0;
		my $text = $bot->get_text($page);
		next unless is_allowed(\$text, $options{'username'}, $page);
		my $text_bak = $text;
		$text =~ s/(\[\[[cC]ategory:(?i:$old_cat_first)$old_cat_tail\]\])/[[category:$new_cat]]/g;
		msg(" changing: $1 -> [[category:$new_cat]]") if $text ne $text_bak and $options{'verbose'} > 0;
		time_management(\@time_stack, $options{'max_edits_per_min'}, $options{'verbose'});
		save_wiki_page($bot, $page, $summary, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
	}
}

sub check_external_link{
	my $pretext   = shift;
	my $urlstring = shift;
	my $verb_lvl  = shift;
	my $ok = 1;
	if($$urlstring =~ /^https?:\/\//){
		my $url = $$urlstring;
		if($$pretext =~ /url\s*=\s*$/ and $$urlstring =~ /^([^|}]+)[|}]/){
			$url = $1;
		}
		my $response_code = get_http_status($url);
		msg("  status of '$url': $response_code") if $verb_lvl > 0;
		$ok = 0 if $response_code != 200;
	}
	return $ok;
}

sub cleanup_wiki_page{
	my $text     = shift; # ref to string
	my $page     = shift;
	my $verb_lvl = shift // 1;         # level of verbosity

	my $changes = {};

	# == link fixes ==
	# === double protocol ===
	$changes->{'double protocol'} = test_and_replace($text, 
		qr/http:\/\/http:?\/\/($_re_url_class+)/m, 
		'"http:\/\/$1"', $verb_lvl);
	#  "Infobox Band" and "Infobox Ort in der Ukraine" do not set prefix "http://" automatically
	#if($$text!~/\{\{Infobox (?:Band|Ort in der Ukraine|EU-Ratspr.{1,4}sidentschaft)/){
	#	test_and_replace($text, 
	#		qr/\|\s*(?:KreisWebseite|OFFIZIELLE_WEBSEITE|web|Webpr.{1,4}senz||[Ww]ebse?ite|WEBSITE|WWW)\s*=\s*\Khttp:\/\/($_re_url_class+)/m, 
	#		'"$1"', $verb_lvl);
	#}

	# === technically superfluous external links ===
	if($page ne 'Hypertext Transfer Protocol Secure'){
		$changes->{'lf pseudo-external links'} = test_and_replace($text, 
			qr/\[{1,2}(https?:\/\/de\.wikipedia\.org\/wiki\/[^\]| #]++(?:#(?!mediaviewer)[^\]| ]+)?) ([^\]]+)\]{1,2}/m, 
			'"[[".url2title($1).(url2title($1) eq $2 ? "" : "|$2")."]]"', 
			$verb_lvl);
	}

	# === specific domains ===
	# see [https://de.wikipedia.org/w/index.php?title=MediaWiki_Diskussion:Spam-blacklist&oldid=136131974#www.denkmalschutz.de_.22.26cHash.3D.5B0-9a-f.5D.2B.22_entfernen]
	$changes->{'lf denkmalschutz.de'} = test_and_replace($text, 
		qr/http:\/\/www.denkmalschutz.de\/($_re_url_class+?)\K&cHash=[0-9a-f]+/m, 
		'""', $verb_lvl);

	# === tests ===
	#test_and_replace($text, 
	#		qr/seth sagt: hallo camelbot!/m, 
	#		'"camelbot sagt: hallo seth!"', $verb_lvl);
	return $changes;
}

sub delete_wiki_page{
	my $mw = shift;
	my $page = shift;
	my $summary = shift;
	my $time_stack = shift;
	my $user_answer = shift;
	my $options = shift;
	if((not defined $options->{'simulation'}) or !$options->{'simulation'}){
		my $user_input;
		if((defined $options->{'askuser'}) && $options->{'askuser'}){
			print "execute? ('y' = yes, else = no) ";
			chomp($user_input = <STDIN>);
		}
		if((not defined $options->{'askuser'}) || !$options->{'askuser'} || $user_input=~/y(?:es)?/ || ($user_input eq '' && $$user_answer eq 'y')){
			$$user_answer = 'y';
			msg("  deleting page [[$page]] ...") if $options->{'verbose'} > 0;
			push @$time_stack, time;
			$mw->edit({
				'action' => 'delete',
				'minor'  => $options->{'minor'},
				'title'  => $page,
				'reason' => "Bot: $summary"
			}) || die $mw->{'error'}->{'code'} . ': ' . $mw->{'error'}->{'details'};
		}else{
			$$user_answer = '';
		}
	}
}

sub download_css{
	my $cssurl   = shift;
	$cssurl =~s/&amp;/&/g;
	my $css_dir = shift;
	my $images_dir = shift; # todo: save url{...}
	my $filename = $cssurl;
	$filename =~s/[^a-zA-Z0-9~_-]/_/g;
	$filename = "$css_dir/$filename.css";
	my $mech = WWW::Mechanize->new('keep_alive' => 1, 'autocheck' => 0, 'timeout' => 10);
	$mech->agent_alias('Linux Mozilla');
	$mech->get($cssurl);
	if($mech->status() != 200){
		msg('download of css failed. status: '.$mech->status(), 'error');
	}else{
		my $css_content = $mech->content();
		write_file($filename, $css_content);
	}
	return $filename;
}

sub download_files{
	my $bot           = shift;
	my $files         = shift;
	my $no_warn_files = shift;
	my $files_prefix  = shift // '';
	my $verbose       = shift // 1;
	for my $file(@$files){
		msg("getting '$file' from wiki") if $verbose > 1;
		my $img_data = $bot->get_image("File:$file");
		unless(defined $img_data){
			msg("could not read file '$file' from wiki", 'error');
			next;
		}
		$file = title2filename($file);
		if(-e $file){
			# unless($file ~~ $no_warn_files){ smartmatch operator is considered experimental as of 5.18 (seth, 2014-11-15)
			unless(grep {$_ eq $file} @$no_warn_files){
				msg("file '$file' exists already", 'warning');
				push @$no_warn_files, $file;
			}
			next;
		};
		push @$no_warn_files, $file;
		msg("writing file '$file'") if $verbose > 1;
		write_file($files_prefix.$file, {binmode => ':raw'}, \$img_data);
	}
	return 1;
}

sub download_pages_by_prefix{
	my $bot     = shift;
	my $prefix  = shift;
	my $verbose = shift // 1;
	my @prefix_pages = grep {!$_->{'redirect'} and $_ = $_->{'title'}} $bot->prefixindex($prefix);
	die "error: no pages found with prefix '$prefix}'.\n" if @prefix_pages==0;
	my $page_texts = $bot->get_pages(\@prefix_pages);
	if($verbose > 0){
		map {msg("getting '$_'")} sort keys %$page_texts;
	}
	my $no_warn_files = [];
	my $files_prefix = '';
	while(my ($title, $text) = each %$page_texts){
		$title = title2filename($title).'.wikitext';
		msg("saving '$title'") if $verbose > 1;
		die "error: file '$title' exists already\n" if -e $title;
		write_file($title, {binmode => ':utf8'}, $text);
		my @images = $text=~/
			\[\[
				(?:[fF]ile|[iI]mage|[mM]edia):
				(
					(?:
						(?!\]\])
						[^|]
					)*
				)/xg;
		download_files($bot, \@images, $no_warn_files, $files_prefix, $verbose);
	}
	return 1;
}

sub getAbuseFilterData{
	my $bot = shift;               # logged-in bot
	my $globaloptions = shift;     # global options
	# http://de.wikipedia.org/w/api.php?action=query&list=abusefilters&abfprop=id|description|pattern|actions|hits|comments|lasteditor|lastedittime|status|private&abflimit=500
}

sub get_tables_from_wikitext{
	my $wikitext = shift;
	return [grep /^\{\|/, split /^(?=\{\|)|^\|\}\K$/m, $$wikitext];
}

sub get_user_contribs{
	my $options = shift;
	my $ucstart = $options->{'ucstart'};
	my $ucend   = $options->{'ucend'};
	my $uclimit = $options->{'uclimit'};
	my $user    = $options->{'username'} // $options->{'usernameprefix'};
	msg("searching for contributions of user: $user");
	#my $ref = $bot->contributions($options->{'username'}, 0, undef);
	my $mwapi = MediaWiki::API->new();
	$mwapi->{config}->{api_url} = $options->{'api'};
	my $query = {
		action      => 'query',
		list        => 'usercontribs', 
		ucprop      => 'ids|title',
		ucstart     => $ucstart,
		'uclimit'   => defined $uclimit ? $uclimit : 100,
	};
	if(exists $options->{'username'}){
		$query->{'ucuser'}  = $user;
	}else{
		$query->{'ucuserprefix'}  = $user;
	}
	if(defined $options->{'namespace'}){
		$query->{'ucnamespace'}  = $options->{'namespace'};
	}
	$query->{'ucend'}   = $ucend if defined $ucend;
	my $mw_options = {'max' => 1};
	my %list_of_articles;
	#while((keys %list_of_articles)<$uclimit and (defined $ucend and $ucstart gt $ucend or not defined $ucend)){
		#msg("ucstart = $ucstart, ucend = $ucend") if defined $ucstart and defined $ucend;
		my $tmp = $mwapi->api($query, $mw_options);
		$ucstart = $tmp->{'query-continue'}->{'usercontribs'}->{'ucstart'};
		$query->{'ucstart'} = $ucstart;
		my $usercontribs = $tmp->{'query'}->{'usercontribs'};
		if(defined $usercontribs){
			for my $uc(@$usercontribs){
				next if exists $list_of_articles{$uc->{'title'}};
				$list_of_articles{$uc->{'title'}} = 1;
			}
		}
	#}
	return \%list_of_articles;
}

sub get_pages_by_prefix{
	my $mw     = shift;
	my $params = shift;
	my $query = {
		'action'         => 'query',
		'continue'       => '',
		'generator'      => 'allpages', 
		'gapnamespace'   => $params->{'namespace_id'}, 
		'gapprefix'      => $params->{'prefix'},
		'gapfilterredir' => 'nonredirects',
		'gaplimit'       => '500',
		'prop'           => 'info|revisions',
		'rvprop'         => 'content|timestamp',
	};
	my $mw_options = {'max' => 1};
	my $finished = 0;
	my @results;
	my $rvcont_old;
	while(!$finished){
		my $rvcont;
		my $pages = $mw->api($query, $mw_options) or die $mw->{'error'}->{'code'}.': '.$mw->{'error'}->{'details'};
		#print Dumper [keys $pages];
		if(defined $pages->{'continue'}){
			#print Dumper $pages->{'continue'};
			while(my ($k, $v) = each(%{$pages->{'continue'}})){
				$query->{$k} = $v;
			}
			if(defined $pages->{'continue'}->{'rvcontinue'}){
				$rvcont = $pages->{'continue'}->{'rvcontinue'};
				$rvcont =~s/\|.*//;
			}
		}else{
			$finished = 1;
		}
		push @results, grep {
			(not defined $params->{'title_re'} or $_->{'title'} =~ /$params->{'title_re'}/)
			and (not defined $rvcont or $_->{'pageid'} < $rvcont)
			and (not defined $rvcont_old or $_->{'pageid'} >= $rvcont_old)
		} values %{$pages->{'query'}->{'pages'}};
		$rvcont_old = $rvcont;
		#if(defined $pages->{'batchcomplete'}){
		#	print Dumper $pages->{'batchcomplete'};
		#}
	}
	return \@results;
}

sub is_allowed{
	my $textref = shift;
	my $user = shift;
	my $page = shift // 'unknown';
	my $allowed = 1;
	if($$textref =~ /{{[nN]obots}}|
		[bB]ots\s*+\|\s*+(?:
			deny\s*+=\s*+(?:
				all\s*+}}|
				(?<deny>.*?)}}
			)|
			allow\s*+=\s*+(?:
				none\s*+}}|
				(?<allow>.*?)}}
			)
		)/x){
		if(defined $+{'allow'}){
			$allowed = 0 if $+{'allow'} !~/(?:^|,\s*)$user(?:\s*,|$)/;
		}elsif(defined $+{'deny'}){
			$allowed = 0 if $+{'deny'} =~/(?:^|,\s*)$user(?:\s*,|$)/;
		}else{
			$allowed = 0;
		}
	}
	msg("bot is not allowed on page '$page'.") if $allowed==0;
	return $allowed;
}

sub link_replacement{
	my $mw  = shift;               # mediawiki api
	my $bot = shift;               # logged-in bot
	my $globaloptions = shift;     # global options
	my %options = %$globaloptions; # local options inherit from global options
	my $re_prot_part          = $globaloptions->{'re_prot_part'};      # regexp pattern of protocol, e.g., /http:\/\//
	my $re_url_part           = $globaloptions->{'re_url_part'};       # regexp pattern of searched url (without protocol), e.g., /(?:www\.)?example\.com/
	my $replacement           = $globaloptions->{'replacement'};       # replacement function
	$options{'delete_link'}   = $globaloptions->{'delete_link'};       # 1 = delete links
	$options{'ref2deadlink'}  = $globaloptions->{'ref2deadlink'};      # 1 = replace links in refs by 'dead link' template.
	$options{'searched_link'} = $globaloptions->{'searched_link'};     # link to use in api link search
	$options{'user_contribs'} = $globaloptions->{'user_contribs'};     # if user contributions shall be searched...
	$options{'summary'}       = $globaloptions->{'summary'};           # wiki summary of page edit
	$options{'askuser'}       = $globaloptions->{'askuser'}     //  1; # 0 = don't ask user before each replacement, 1 = ask user every time
	$options{'simulation'}    = $globaloptions->{'simulation'}  //  1; # 0 = append on real pages, 1 = just simulate
	$options{'articles'}      = $globaloptions->{'articles'}    //  0; # 0 = don't append on ns 0, 1 = append on ns 0
	$options{'nonarticles'}   = $globaloptions->{'nonarticles'} //  0; # 0 = don't append on ns!=0, 1 = append on ns!=0
	$options{'refs'}          = $globaloptions->{'refs'}        //  1; # 0 = don't touch refs; 1 = replace refs
	$options{'nonrefs'}       = $globaloptions->{'nonrefs'}     //  1; # 0 = don't touch non-refs; 1 = replace non-refs
	$options{'max_edits'}     = $globaloptions->{'max_edits'}   // -1; # maximum number of edits (-1 = inf)
	$options{'skip_edits'}    = $globaloptions->{'skip_edits'}  //  0; # skip first n pages.
	$options{'max_edits_per_min'} = -1 if not defined $globaloptions->{'max_edits_per_min'}; # maximum number of edits per minute (-1 = inf)
	my $verb_lvl              = $globaloptions->{'verbose'}     //  1; # level of verbosity
	my $edit_counter = 0;          # edit counter
	my $old_page_0 = {};           # list of edited pages in ns==0
	my $old_page_1 = {};           # list of edited pages in ns!=0
	# some regexp vars should make source more readable 
	my $re_url = $re_prot_part.$re_url_part.$_re_url_class;
	my $re_descr_class = qr/[^\]\x00-\x08\x0a-\x1F]/;
	my $re_ref_tag_name = qr/<ref [^>]*name\s*=\s*['"]?([^>]*?)['"]?>/;
	my $re_ref_inner_part = qr/(?s:(?!<ref)(?!<\/ref>).)+?$re_url_part.*?<\/ref>/;
	my $re_deadlink = qr/\Q{{dead link|date=March 2011}}\E/;
	my @time_stack = ();           # needed for edit/min constraint
	my $user_answer = '';          # user answer initially is "no"
	my %results = (
		'num_ns0_links'  => 0,
		'num_ns!0_links' => 0,
	);

	if($options{'articles'}){
		msg("starting linksearch in ns=0...");
		my @links;
		if(defined $options{'searched_link'}){
			@links = $bot->linksearch($options{'searched_link'}, 0, undef);
		}elsif(defined $options{'user_contribs'}){
			msg('searching for contributions of user:'.$options{'user_contribs'}->{'user'});
			#my $ref = $bot->contributions($options{'user_contribs'}->{'user'}, 0, undef);
			#$options{'ucstart'}  = '2011-03-18T00:00:00Z';
			#$options{'ucend'}    = '2011-03-16T00:00:00Z';
			#$options{'uclimit'}  = '10';
			#$options{'username'} = $options{'user_contribs'}->{'user'};
			#$options{'namespace'} = 0;
			#get_user_contribs($options);
			#my $mwapi = MediaWiki::API->new();
			#$mw->{config}->{api_url} = 'https://de.wikipedia.org/w/api.php';
			my $ucstart = '2011-03-18T00:00:00Z';
			my $ucend = '2011-03-16T00:00:00Z';
			my $query = {
				action      => 'query',
				list        => 'usercontribs', 
				ucuser      => $options{'user_contribs'}->{'user'},
				ucnamespace => 0,
				ucprop      => 'ids|title',
				ucstart     => $ucstart,
				ucend       => $ucend,
				uclimit     => '10',
			};
			my $options = {'max' => 1};
			while($ucstart gt $ucend){
				msg("ucstart = $ucstart, ucend = $ucend");
				my $tmp = $mw->api($query, $options);
				$ucstart = $tmp->{'query-continue'}->{'usercontribs'}->{'ucstart'};
				$query->{'ucstart'} = $ucstart;
				my $usercontribs = $tmp->{'query'}->{'usercontribs'};
				if(defined $usercontribs){
					for(@$usercontribs){
						next if exists $old_page_0->{$_->{'title'}};
						$old_page_0->{$_->{'title'}} = 1;
						# msg($_->{'title'});
						my $diff = $bot->diff({
							'revid' => $_->{'revid'},
							'oldid' => 'prev',
						});
						# my $found = 0;
						while($diff=~/<td class=\"diff-deletedline\">(.*?)<\/td>/g){
							if($1=~/($re_url*)/){
								push @links, {
									'title' => $_->{'title'},
									'url' => $1,
								};
								# $found = 1;
								last;
							}
						}
						# print Dumper($diff) if $found;
					}
				}
			}
		}
		msg('results (found '.@links.' links in '.
			num_unique_elem(map {$_->{'title'}} @links)." pages):");
		$results{'num_ns0_links'} = 0+@links;
		#push @links, {'url' => 'scheissegal', 'title' => 'LyX'};
		for my $hash (@links){
			my $page = $hash->{'title'};
			next if exists $old_page_0->{$page};
			$old_page_0->{$page} = 1;
			if(++$edit_counter<$options{'skip_edits'} || $options{'max_edits'}==0){
				next;
			}elsif($edit_counter>$options{'max_edits'} && $options{'max_edits'}!=-1){
				last;
			}
			#next unless $hash->{'url'}=~/$re_prot_part$re_url_part/;
			msg("$edit_counter: page= $page, url= ".$hash->{'url'});
			my $text = $bot->get_text($page);
			next unless is_allowed(\$text, $options{'username'}, $page);
			my $text_bak = $text;
			cleanup_wiki_page(\$text, $page, $verb_lvl);
			if(defined $replacement){
				#	test_and_replace(
				#		\$text, 
				#		qr/$re_prot_part$re_url_part(?<trail>$_re_url_class*)(?:(?<posturl>.*?)$re_deadlink)?/m, 
				#		$replacement, 
				#		$verb_lvl
				#	);
			}elsif($options{'delete_link'}){
				if($options{'refs'}){
					if($options{'ref2deadlink'}){
						if($text!~/url\s*=\s*$re_url/){ # special templates
							test_and_replace(\$text, 
								qr/
									(
										<(?i:ref)\b[^>]*>\s*          # <ref>
										(?s:(?!<ref)(?!<\/ref>).)*?   # blabla
									)(?|
										\[($re_url*+)(\x20[^\]]+|)\]| # [url] or 
										($re_url*+)()                 # url (that is not preceeded by "url = ")
									)(
										.*?                           # blabla
										<\/ref>                       # <\/ref>
									)/x,
								'"${1}[{{dead link|inline=yes|bot='.$options{'username'}.'|date='.
									strftime("%Y-%m-%d", gmtime()).'|url=$2}}$3]$4"', 
								$verb_lvl);
						}
					}else{
						# backrefs
						if($text=~/$re_ref_tag_name$re_ref_inner_part/){
							my $ref_name = $1;
							msg($ref_name, 'debug');
							test_and_replace(
								\$text, 
								qr/<(?i:ref) name\s*=\s*['"]?\s*$ref_name\s*['"]?\s*\/\s*>/, 
								'""', 
								$verb_lvl
							);
						}
						# refs
						test_and_replace(
							\$text, 
							qr/<(?i:ref)\b[^>]*>\s*$re_ref_inner_part/, 
							'""', 
							$verb_lvl
						);
					}
				}
				# <references />
				if($text!~/<(?i:ref)(?:>| name)|EinwohnerOrtQuelle|EinwohnerRef/){
					my $text_tmp = $text;
					test_and_replace(
						\$text_tmp, 
						qr/^=(=+)\s*(?:Einzelnachweise|Quellen)\s*\1=[\s\n]*\n<references(?: ?\/)?>\s*/m, 
						'""', 
						$verb_lvl
					);
					my $parsed = parse_wikitext($mw, $text_tmp);
					if($parsed !~ /class="error mw-ext-cite-error"/){
						test_and_replace(
							\$text, 
							qr/^=(=+)\s*(?:Einzelnachweise|Quellen)\s*\1=[\s\n]*\n<references(?: ?\/)?>\s*/m, 
							'""', 
							$verb_lvl
						);
					}else{
						msg("<references /> not deleted, because it would lead to an error.", 'warning');
					}
				}
				if($options{'nonrefs'}){
					# links like [link descr] in lists (as in "external links")
					test_and_replace(
						\$text, 
						qr/^\*(?i:.(?!<ref))*\[$re_url*?\s*$re_descr_class*\].*\n?/m, 
						'""', 
						$verb_lvl
					);
					# plain links in lists (as in "external links")
					test_and_replace(
						\$text, 
						qr/^\*(?i:.(?!<ref))*$re_url*?.*\n?/m, 
						'""', 
						$verb_lvl
					);
				}
				# heading "weblinks" without entries
				my $weblinks_heading_re = qr/^=(=+)\s*Weblinks?\s*=\1[\s\n]*\n(\[\[[Kk]atego|\{\{(?:Coordinate|DEFAULTSORT|Hinweis |(?:Vorlage:)?Navigations|Normdaten|(?:Vorlage:)?Orden,|SORTIERUNG)|(?:<!-- ?)?==)/m;
				if($text=~$weblinks_heading_re){
					test_and_replace(\$text, $weblinks_heading_re, '"$2"', $verb_lvl);
				}
				msg(" not resolved: $1") if $text=~/(.*(?<!\|url=$re_prot_part)$re_url_part.*)/;
			}
			time_management(\@time_stack, $options{'max_edits_per_min'}, $verb_lvl);
			save_wiki_page($bot, $page, $options{'summary'}, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
		}
		$results{'num_ns0_unique_links'} = $edit_counter;
	}

	if($options{'nonarticles'}){
		my $namespaces = {$bot->get_namespace_names()}; # get all namespaces
		$namespaces = [grep {$_!=0} keys %$namespaces]; # delete namespace 0 from list
		msg("starting linksearch in ns!=0...");
		my @links;
		if(defined $options{'searched_link'}){
			@links = $bot->linksearch($options{'searched_link'}, $namespaces, undef);
		}elsif(defined $options{'user_contribs'}){
		}
		msg('results (found '.@links." links in ".
			num_unique_elem(map {$_->{'title'}} @links)." pages):");
		$results{'num_ns!0_links'} = 0+@links;
		for my $hash (@links){
			my $page = $hash->{'title'};
			next if exists $old_page_1->{$page};
			$old_page_1->{$page} = 1;
			if(++$edit_counter<$options{'skip_edits'} || $options{'max_edits'}==0){
				next;
			}elsif($edit_counter>$options{'max_edits'} && $options{'max_edits'}!=-1){
				last;
			}
			msg("$edit_counter: page= $page, url= ".$hash->{'url'});
			my $text = $bot->get_text($page);
			next unless is_allowed(\$text, $options{'username'}, $page);
			my $text_bak = $text;

			#cleanup_wiki_page(\$text, $page, $verb_lvl);
			if(defined $replacement){
				#	test_and_replace(
				#		\$text, 
				#		qr/$re_prot_part$re_url_part(?<trail>$_re_url_class*)(?:(?<posturl>.*?)$re_deadlink)?/m, 
				#		$replacement, 
				#		$verb_lvl
				#	);
			}elsif($options{'delete_link'}){
				if($options{'nonrefs'}){
					# [link]
					test_and_replace(\$text, qr/\[$re_prot_part($re_url_part$_re_url_class*?)\]/, '$1', $verb_lvl);
					# [link descr]
					test_and_replace(\$text, qr/(?<!<nowiki>)(\[$re_url*\s*$re_descr_class*\])/, '"<nowiki>".$1."<\/nowiki>"', $verb_lvl);
					# plain_link or word:plainlink
					test_and_replace(\$text, qr/(?<!$_re_url_class)(?<!<nowiki>\[)([a-zA-Z0-9]*:|)$re_prot_part($re_url_part$_re_url_class*)/, '$1.$2', $verb_lvl);
					# ==plain_link==
					test_and_replace(\$text, qr/^(=+) *$re_prot_part($re_url_part$_re_url_class*) *\1 */m, '$1." ".$2." ".$1', $verb_lvl);
					# :*plain_link
					test_and_replace(\$text, qr/^([:\s*]*)$re_prot_part($re_url_part$_re_url_class*)/m, '$1.$2', $verb_lvl);
					# (plain_link)
					test_and_replace(\$text, qr/([()]\s*)$re_prot_part($re_url_part$_re_url_class*)(\s*\)|\s)/, '$1.$2.$3', $verb_lvl);
#					# {|plain_link|}
#					test_and_replace(\$text, qr/([|{]\s*)$re_prot_part($re_url_part$_re_url_class*)(\s*[|}])/, '$1.$2.$3', $verb_lvl);
				}
				# not good for large pages:
				msg(" not resolved: $1") if $text=~/^(.*(?<!<nowiki>)(?<!<nowiki>\[)$re_prot_part$re_url_part.*)/m;
				# better:
#				while($text=~/(^.*$re_prot_part$re_url_part.*)/gmo){
#					my $temp = $1;
#					msg(" not resolved: $temp") if $temp=~/(?<!<nowiki>)(?<!<nowiki>\[)$re_prot_part$re_url_part/;
#				}
			}
			time_management(\@time_stack, $options{'max_edits_per_min'}, $verb_lvl);
			save_wiki_page($bot, $page, $options{'summary'}, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
		}
		$results{'num_ns!0_unique_links'} = $edit_counter-($results{'num_ns0_unique_links'} // 0);
	}
	if($options{'results'}){
		print "\n";
		msg("results:");
		print " found ".($results{'num_ns0_links'}+$results{'num_ns!0_links'})." links in ".(0+keys(%$old_page_0)+keys(%$old_page_1))." pages:\n";
		if($options{'articles'} && $options{'nonarticles'}){
			print "       ".$results{'num_ns0_links'}." links in ".$results{'num_ns0_unique_links'}." pages in ns==0\n";
			print "       ".$results{'num_ns!0_links'}." links in ".$results{'num_ns!0_unique_links'}." pages in ns!=0\n";
		}
		if($options{'articles'}){
			print "\n pages in ns==0:\n";
			print "  $_\n" for sort keys %$old_page_0;
		}
		if($options{'nonarticles'}){
			print "\n pages in ns!=0:\n";
			print "  $_\n" for sort keys %$old_page_1;
		}
	}
}

sub login{
	my $wiki_user = shift;
	my $wiki_password = shift;
	my $globaloptions = shift;     # global options
	my %options = %$globaloptions; # local options inherit from global options
	my $verbose = $options{'verbose'};
	# if wiki user name is not defined, let user type in wiki user name
	if(not defined $wiki_user){
		msg("enter wiki username:");
		$wiki_user = <STDIN>;
		chomp($wiki_user);
	}
	msg("login user name is $wiki_user.") if $verbose > 0;
	# cope with auto-normalization (bug in MediaWiki::Bot)
	my $wiki_user_norm = ucfirst($wiki_user);
	$wiki_user_norm =~y/_/ /;
	my $cookie = '.mediawiki-bot-'.$wiki_user_norm.'-cookies';
	# if wiki user password is not defined, search typical password places or let user type in wiki user password
	if(not defined $wiki_password){
		# /bot/ needed for toolserver (for historical reasons)
		my $pwfile = [".password", "$ENV{HOME}/.password", "$ENV{HOME}/bot/.password"];
		for(@$pwfile){
			if(-e $_){
				msg("using standard password file $_") if $verbose > 0;
				open(my $INFILE, '<', $_) or die $!;
					chomp($wiki_password = <$INFILE>);
				close($INFILE);
				last;
			}
		}
		unless(defined $wiki_password or
				(	# $wiki_user =~/^(?:CamelBot)$/ and 
					-e $cookie and
					not ($options{'delete'} or $options{'upload'})
				)
		){
			if($verbose>0){
				print 'could not read password file at places: ';
				print $pwfile->[$_].', ' for 0..($#$pwfile-1);
				print $pwfile->[-1]."\n";
			}
			msg("enter password (will not be echoed):");
			ReadMode 'raw';
			$wiki_password = ReadLine(0);
			chomp($wiki_password);
			ReadMode 'restore';
			while($wiki_password=~/\x{007f}/){
				$wiki_password =~ s/(?:^|[^\x{007f}])\x{007f}//g;
			}
		}
	}

	my %address_params = (
		'protocol' => 'https',
		# wiki dependent
		'host'     => 'de.wikipedia.org',
		'path'     => 'w',
		#'host'     => 'wiki.selfhtml.org',
		#'path'     => 'mediawiki',
	);

	msg("logging in at $address_params{'host'}...") if $verbose > 1;
	my $bot = MediaWiki::Bot->new({
		assert      => 'bot',
		protocol    => $address_params{'protocol'},
		host        => $address_params{'host'},
		path        => $address_params{'path'},
		operator    => 'lustiger_seth',
		debug       => 1,
		login_data  => {
			username => $wiki_user_norm,
			password => $wiki_password,
			#lgdomain => ''
		},
	});

	my $mw = MediaWiki::API->new();
	my $wiki_base_url = $address_params{'protocol'}.'://'.$address_params{'host'};
	my $api_url = $wiki_base_url.'/'.$address_params{'path'}.'/api.php';
	if($options{'delete'} or $options{'upload'} or $options{'link-replacement'}){
		$mw->{'config'}->{'api_url'} = $api_url;
		$mw->{'config'}->{'upload_url'} = $wiki_base_url.'/wiki/Special:Upload';
		$mw->{'config'}->{'files_url'} = '';
		$mw->login({
			'lgname' => $wiki_user_norm,
			'lgpassword' => $wiki_password,
			#'lgdomain' => ''
		}) or die $mw->{'error'}->{'code'}.': '.$mw->{'error'}->{'details'};
	}else{
		unless(defined $bot){
			msg("maybe there's an expired cookie file. please delete '$cookie' and try again.", 'error') if -e $cookie;
			die("error: could not login.\n");
		}
	}
	msg("logged in...") if $verbose > 1;
	return ($bot, $mw, $api_url);
}

sub newest_post_info{
	my $wikitext = shift;
	my $threads = [grep {$_!~/^=+$/} split /\n(?=(=++)[^=].*\1(?:\n|$))/, $wikitext];
	my $newest_thread = {
		'date'   => 0, 
		'author' => '',
	};
	for my $t(@$threads){
		next unless $t=~/^(=++)\s*(.*?)\s*\1/;
		my $thread_title = $2;
		$newest_thread->{'thread'} = $thread_title unless defined $newest_thread->{'thread'};
		my $date;
		while($t=~/(?<pre_sig>.*)(?<hour>\d\d):(?<min>\d\d), (?<mday>[1-9]|[123][0-9])\. (?<mon>[a-zA-Zä\xe4]{3,4})\.? (?<year>20\d\d) \((?<tz>CES?T)\)/g){
			my $month = 
				$+{'mon'} eq 'Jan' ? 0 : 
				$+{'mon'} eq 'Feb' ? 1 : 
				($+{'mon'} eq "Mär" or $+{'mon'} eq "M\xe4r") ? 2 : 
				$+{'mon'} eq 'Apr' ? 3 : 
				$+{'mon'} eq 'Mai' ? 4 : 
				$+{'mon'} eq 'Jun' ? 5 : 
				$+{'mon'} eq 'Jul' ? 6 : 
				$+{'mon'} eq 'Aug' ? 7 : 
				$+{'mon'} eq 'Sep' ? 8 : 
				$+{'mon'} eq 'Okt' ? 9 : 
				$+{'mon'} eq 'Nov' ? 10 : 
				$+{'mon'} eq 'Dez' ? 11 : 12
			;
			next if $month == 12;
			$date = timegm(0, $+{'min'}, $+{'hour'}, $+{'mday'}, $month, $+{'year'});
			$date += $+{'tz'} eq 'CEST' ? 2 : 1;
			if($date>$newest_thread->{'date'}){
				$newest_thread = {
					'thread' => $thread_title,
					'author' => $+{'pre_sig'},
					'date'   => $date,
				};
			}
		}
	}
	my $username;
	while($newest_thread->{'author'} =~/\[\[
		(?:
			(?:[Bb]enutzer(?:in)?|[uU]ser)(?:\x20talk|Diskussion)?:| # 'user:...' or 'user talk:...' or
			[Ss]pe[zc]ial:(?:Beitr..?ge|[Cc]ontributions)\/   # 'special:contributions...'
		)
		([^|\/\]]+)
		/gx){
		$username = $1;
	}
	$newest_thread->{'author'} = $username if defined $username;
	return $newest_thread;
}

sub notifier{
	my $bot         = shift;
	my $page        = shift;
	my $time_stack  = shift;
	my $user_answer = shift;
	my $options     = shift;
	utf8::decode($page); # is this necessary?
	my @hist = $bot->get_history($page, 2);
	if(@hist==0){
		if($options->{'verbose'} > 0){
			msg("page '$page' doesn't seem to exist anymore.", 'notice');
		}
		return 0;
	}
	my $text = $bot->get_text($page, $hist[0]->{'revid'}); # newest version
	if(defined $text){
		# ASINs are unwanted in w:de
		my $notify = 0;
		my $num_matches =()= $text =~ /$_re_ASIN/g;
		if($num_matches > 0){
			# compare with older version
			if(@hist > 1 and defined $hist[1]->{'revid'}){
				my $text_prev_version = $bot->get_text($page, $hist[1]->{'revid'});
				my $num_matches_pv =()= $text_prev_version =~ /$_re_ASIN/g;
				if($num_matches_pv < $num_matches){
					$notify = 1;
				}
			}else{
				$notify = 1;
			}
		}
		if($notify){
			my $user_to_inform = $hist[0]->{'user'};
			my $userpage = 'User talk:'.$user_to_inform;
			$text = $bot->get_text($userpage);
			if(is_allowed(\$text, $options->{'username'}, $userpage)){
				my $text_bak = $text;
				my $heading = 'hinweis auf katalog-nummern von amazon';
				if($text =~ /\Q$heading\E/){
					msg("user '$user_to_inform' seems to be informed already.", 'notice') if $options->{'verbose'} > 0;
				}else{
					my $notice = $bot->get_text('user:CamelBot/notice-ASIN');
					my $title = title2url_part($page);
					$notice =~ s/\$diff\b/\/\/de.wikipedia.org\/w\/index.php?title=$title&diff=prev&oldid=$hist[0]->{'revid'}/g;
					$notice =~ s/\$article\b/$page/g;
					$notice =~ s/\$signature\b.*/-- ~~~~/sg;
					$text .= "\n== $heading ==\n$notice\n";
					#if($text_bak ne $text){
						time_management($time_stack, $options->{'max_edits_per_min'}, $options->{'verbose'});
						msg("notifying on '$userpage'") if $options->{'verbose'} > 1;
						save_wiki_page($bot, $userpage, $heading, \$text, \$text_bak, $time_stack, $user_answer, $options);
					#}
				}
			}
		}
	}else{
		msg("could not fetch page '$page'. maybe deleted already", 'notice') if $options->{'verbose'} > 0;
	}
	return 1;
}

sub parse_wikitext{
	my $mw = shift;
	my $wikitext = shift;
	my $text = $mw->api( {
    action => 'parse',
    text => $wikitext,
    prop => 'text',
	}) or die $mw->{error}->{code} . ': ' . $mw->{error}->{details};
	my $html = $text->{'parse'}->{'text'}->{'*'};
	# delete final comments
	$html =~s/\n\n<!--\s+NewPP.*?\n-->$//s;
	return $html;
}

sub parse_page{
	my $mw = shift;
	my $page = shift;
	my $text = $mw->api( {
    action => 'parse',
    page => $page,
    prop => 'text|headhtml',
	}) or die $mw->{error}->{code} . ': ' . $mw->{error}->{details};
	my $title = $text->{'parse'}->{'title'};
	my $html = $text->{'parse'}->{'text'}->{'*'};
	my $headhtml = $text->{'parse'}->{'headhtml'}->{'*'};
	# delete final comments
	$html =~s/\n\n<!--\s+NewPP.*?\n-->$//s;
	return ($html, $title, $headhtml);
}

# searches for all links in a given piece of wikitext
# original code got from http://svn.wikimedia.org/viewvc/mediawiki/trunk/phase3/includes/parser/Parser.php?view=markup&pathrev=74981
# ported to perl because phps regexp-handling (\\\\\\\\\\\\\\\\\) sucks.
sub php_code_unused{
	# get wikitext as param
	my $wikitext = shift; # $text in orig code
	# building regexp for matching of "[...]"-like links
	my @wgUrlProtocols = (
    'http://',
    'https://',
    'ftp://',
    'irc://',
    'gopher://',
    'telnet://',
    'nntp://',
    'worldwind://',
    'mailto:',
    'news:',
    'svn://',
    'git://',
    'mms://'
	);
	my $re_prot = join('|', @wgUrlProtocols);
	my $ext_link_url_class = qr/[^\][<>"\x00-\x20\x7F\p{Zs}]/; 
	my $re_url_wo_prot =     qr/$ext_link_url_class+/;
	my $re_descr = qr/[^\]\x00-\x08\x0a-\x1F]*?/;
	my $mExtLinkBracketedRegex = qr/\[(($re_prot)$re_url_wo_prot) *($re_descr)\]'/o; 
	# $1 = url, $2 = prot, $3 = descr

	# split wikitext by links
	my @bits = split /$mExtLinkBracketedRegex/, $wikitext;
	my $s = shift @bits; # first part is something non-link-like (i.e. an empty string or some wikitext).
	# loop over parts (of 4 different classes: wikitext, url, prot, descr, wikitext, url, prot, descr, ...)
	for(my $i=0; $i<@bits;){
		my $url = $bits[++$i];
		my $prot = $bits[++$i];
		my $descr = $bits[++$i]; # $text in orig code
		my $wtext = $bits[++$i] // ''; # $trail in orig code
		# will continue later on
	}
}

sub post_process_html{
	my $html = shift;
	my $image_prefix = shift // '';
	#  images
	$html=~s/<a\x20                                 # <a
				href="\/wiki\/File:(?<filename>[^"]++)"   # href="\/wiki\/File:..."
			[^>]*+>                                     # ...>
			(?<imgpre><img\x20[^>]*src=")               # <img ... src="
				[^"]++                                    # ...
			(?<imgpost>"[^>]*+>)                        # ...>
		<\/a>                                         # <\/a>
		/$+{'imgpre'}$image_prefix$+{'filename'}$+{'imgpost'}/gx;
	$html=~s/ srcset="[^"]++"//g;
	#  html tidy
	my $html_cleaned = '';
	my $err = '';
	IPC::Run::run([qw(tidy -iq --tidy-mark 0 -f /dev/null -w 5000 -utf8)], \$html, \$html_cleaned, \$err);
	return $html_cleaned;
}

sub rebuild_table{
	my $bot       = shift;         # logged-in bot
	my $page      = shift;         # wiki page
	my $tables    = shift;         # tables to insert
	my $summary   = shift;         # wiki summary
	my $globaloptions = shift;     # global options
	my %options = %$globaloptions; # local options inherit from global options
	$options{'simulation'}        = $globaloptions->{'simulation'}        //  1; # 0 = append on real pages, 1 = just simulate
	$options{'askuser'}           = $globaloptions->{'askuser'}           //  0; # 0 = don't ask user before each replacement, 1 = ask user every time
	$options{'max_edits_per_min'} = $globaloptions->{'max_edits_per_min'} // -1; # maximum number of edits per minute (-1 = inf)
	$options{'verbose'}           = $globaloptions->{'verbose'}           //  1;
	my $user_answer = '';          # user answer initially is "no"
	my @time_stack = ();           # needed for edit/min constraint
	my $text = $bot->get_text($page);
	if(not defined $text){
		msg("page $page does not exist.", 'notice') if $options{'verbose'} > 0;
		$text = '';
	}elsif($text eq ''){
		msg("page $page is empty.", 'notice') if $options{'verbose'} > 0;
	}else{
		return unless is_allowed(\$text, $options{'username'}, $page);
	}
	my $text_bak = $text;
	# convert table to wikitext string
	my $tablestyle_default = {
		'table'  => 'class="wikitable sortable"',
		#'header' => 'class="hintergrundfarbe6"',
	};
	my $tablestyle;
	if($text eq ''){
		for(@$tables){
			%$tablestyle = %$tablestyle_default;
			$tablestyle->{'table'} = 'class="'.$_->{'class'}.'"' if exists $_->{'class'};
			my $wikitable = table2wikitext($_, $tablestyle, 1);
			$text .= "== ".$_->{'section'}." ==\n" if defined $_->{'section'};
			$text .= $wikitable;
		}
	}else{
		# find place to insert table and replace table
		for(@$tables){
			%$tablestyle = %$tablestyle_default;
			$tablestyle->{'table'} = 'class="'.$_->{'class'}.'"' if exists $_->{'class'};
			my $wikitable = table2wikitext($_, $tablestyle, 1);
			if(defined $_->{'section'}){
				my $section = $_->{'section'};
				if($text =~/^=(=+) ?\Q$section\E ?\1=\n(?:(?!\n=|\n\[\[[cC]ategory:).)*/ms){
					$text =~ s/^=(=+) ?\Q$section\E ?\1=\n\K(?:(?!\n=|\n\[\[[cC]ategory:).)*/$wikitable/ms;
				}else{
					die "error: could not create/modify section. please ensure that wiki page [[$page]] either is empty or there exists the section '$section'!\n";
				}
			}else{
				if($text =~/(?:^<!--.*?-->\n)?^\{\|.*\|\}/ms){
					$text =~ s/(?:^<!--.*?-->\n)?^\{\|.*\|\}(\n|)\n*/$wikitable$1/ms;
				}else{
					die "error: could not create/modify section. please ensure that wiki page [[$page]] either is empty or there exists the appropriate section!\n";
				}
			}
		}
	}
	msg($text) if $options{'simulation'};
	time_management(\@time_stack, $options{'max_edits_per_min'}, $options{'verbose'});
	save_wiki_page($bot, $page, $summary, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
}

sub save_wiki_page{
	my $bot = shift;
	my $page_utf8 = shift;
	my $summary = shift;
	my $text = shift;
	my $orig_text = shift;
	my $time_stack = shift;
	my $user_answer = shift;
	my $options = shift;
	if($$text ne $$orig_text){
		my $user_answer_diff;
		# comment the following line if no X/tk is available.
		$user_answer_diff = show_diff($text, $orig_text, $options) if defined $options->{'showdiff'} and $options->{'showdiff'}>0;
		if(not defined $options->{'simulation'} or !$options->{'simulation'}){
			my $user_input;
			$user_input = $user_answer_diff ? 'yes' : 'no' if defined $user_answer_diff;
			if(not defined $user_input and defined $options->{'askuser'} and $options->{'askuser'}){
				print "execute? ('y' = yes, else = no) ";
				chomp($user_input = <STDIN>);
			}
			if(not defined $user_input or $user_input=~/y(?:es)?/ or ($user_input eq '' and $$user_answer eq 'y')){
				$$user_answer = 'y';
				my $page = $page_utf8;
				utf8::encode($page);
				msg("saving page [[$page]] ... ") if $options->{'verbose'} > 0;
				push @$time_stack, time;
				$bot->edit({
						is_minor => $options->{'minor'},
						page     => $page_utf8,
						text     => $$text,
						bot      => 1,
						summary  => "Bot: $summary",
				});
			}else{
				$$user_answer = '';
			}
		}
	}
	return 1;
}

sub table2wikitext{
	my $table     = shift;
	my $style     = shift;
	my $full_auto = shift // 1;
	$style->{'table'} = '' if not defined $style->{'table'};
	my $wikitable = '';
	if($full_auto){
		$wikitable .= "<!-- this table is generated automatically. any manual modifications will be deleted on next update. -->\n";
	}
	$wikitable .= "{| ".$style->{'table'}."\n";
	my $row = 0;
	if(exists $table->{header} and $table->{'header'}){
		$wikitable .= '|- '.$style->{'header'}."\n" if defined $style->{'header'};
		$wikitable .= '! '.$_."\n" for @{$table->{'body'}[$row]};
		++$row;
	}
	for(;$row<@{$table->{'body'}}; ++$row){
		$wikitable .= "|-\n";
		for(my $col = 0;$col<@{$table->{'body'}[$row]}; ++$col){
			$wikitable .= '| '.$table->{'body'}[$row][$col]."\n";
		}
	}
	$wikitable .= "|}\n";
	return $wikitable;
}

sub table_body2array{
	my $table_body = shift;
	# truncate table body
	$$table_body =~ s/\n\z//;
	$$table_body =~ s/^\s*\|-\s*|\s*\|-\s*$//g;
	return [
		map {
			s/^\s*\| ?//;
			s/\s+$//;
			if(index($_,'||')>-1){
				[split / *+\|\| *+/, $_];
			}else{
				[split /\s*\n\| *+/m, $_];
			}
		} split /\s*\n\|- *+\n/, $$table_body
	];
}

sub test_and_replace_with_funcref{
	my $replacement = shift; # function ref
	my $pre_url     = shift;
	my $old_url     = shift;
	my $post_url    = shift;
	my $verbose     = shift // 0;
	my $correction  = '';
	# cope with templates like {{something|url=http://example.org|...}} or {{something|url=http://example.org}} 
	if($pre_url =~/url\s*=\s*$/ and $old_url =~/^[^{|}]+([{|}].*)/){
		$correction = $1;
		$post_url = $1.$post_url;
		$old_url = substr($old_url, 0, -length($1));
	}
	# call function reference
	my $new_url = $replacement->($old_url, $post_url, $verbose);
	# print changes to stdout
	if($verbose){
		print "   ".$old_url."\n"; # print old text
		print " ->".$new_url."\n"; # print replaced text
	}
	# check http status; avoid change, if target not reachable
	if($new_url=~/^https?:\/\//){
		my $response_code = get_http_status($new_url);
		msg('  status: '.$response_code) if $verbose > 0;
		$new_url = $old_url if $response_code != 200;
	}
	return $new_url.$correction;
}

sub test_and_replace{
	my $text = shift;
	my $regexp_s = shift;
	my $regexp_r = shift;
	my $verb_lvl = shift // 0;
	my $strpos = 0;
	#my $test = $$text; # for debugging only
	my $array_of_changes = [];
	my $numChanges = $$text=~s/$regexp_s/
		my $match = ${^MATCH};
		my $prematch = ${^PREMATCH};
		my $replaced = eval($regexp_r);
		# cope with links
		if(!check_external_link(\$prematch, \$replaced, $verb_lvl)){
			$replaced = $match;
		}
		push(@$array_of_changes, [$match, $replaced]); 
		$replaced;
		/gpme;
	if($verb_lvl > 0){
		for my $repl(@$array_of_changes){
			msg("   ".$repl->[0]);
			msg(" ->".$repl->[1]);
		}
	}
	#$test=~s/$regexp_s/$regexp_r/gmee; # just for debugging
	#msg("wrong replacing with '$regexp_r'?", 'error') if $$text ne $test;
	return $numChanges;
}

sub text_replacement{
	my $bot      = shift;                      # logged-in bot
	my $globaloptions = shift;
	my %options  = %$globaloptions;
	my $botname  = $globaloptions->{'username'};     # bot name
	my $verb_lvl = $globaloptions->{'verbose'} // 1; # level of verbosity
	my $pages    = $globaloptions->{'pages'};        # pages to search
	my $patterns = $globaloptions->{'pattern'};      # patterns to search s/X//
	my $repls    = $globaloptions->{'replacement'};  # replacing texts s//X/e
	msg("loop over pages and replace text") if $verb_lvl > 0;
	my @time_stack = ();                             # needed for edit/min constraint
	my $user_answer = '';                            # user answer initially is "no"
	my $edit_counter = -1;
	if(@$patterns == 0+@$repls){
		for my $page(@$pages){
			if(++$edit_counter<$options{'skip_edits'} or $options{'max_edits'}==0){
				next;
			}elsif($edit_counter>=$options{'max_edits'} and $options{'max_edits'}!=-1){
				last;
			}
			msg("$edit_counter: page = '$page'") if $verb_lvl > 0;
			my $text = $bot->get_text($page);
			unless(defined $text){
				msg("page content not defined", 'error');
				next;
			}
			if(not is_allowed(\$text, $botname, $page)){
				msg("  not allowed") if $verb_lvl > 1;
				next;
			}
			my $text_bak = $text;
			cleanup_wiki_page(\$text, $page, $verb_lvl);
			my $found = 0;
			for(my $i=0; $i<@$patterns; ++$i){
				if($text=~/$patterns->[$i]/){
					test_and_replace(\$text, $patterns->[$i], $repls->[$i], $verb_lvl);
					++$found;
				}
			}
			msg("  $found replacements") if $verb_lvl > 1;
			time_management(\@time_stack, $options{'max_edits_per_min'}, $verb_lvl);
			save_wiki_page($bot, $page, $options{'summary'}, \$text, \$text_bak, \@time_stack, \$user_answer, \%options);
		}
	}
}

sub time_management{
	my $time_stack = shift;
	my $max_edits_per_min = shift;
	my $verbose = shift // 1;
	while($max_edits_per_min!=-1 && @$time_stack>=$max_edits_per_min){
		my $tmp_time = time-$time_stack->[0];
		if($tmp_time<60){
			msg('  ...waiting '.(60-$tmp_time)." seconds") if $verbose > 0;
			sleep(60-$tmp_time);
		}
		shift @$time_stack;
	}
}

sub title2filename{
	my $title = shift;
	$title =~s/ß/ss/g;
	$title =~s/ä/ae/g;
	$title =~s/Ä/Ae/g;
	$title =~s/ö/oe/g;
	$title =~s/Ö/Oe/g;
	$title =~s/ü/ue/g;
	$title =~s/Ü/Ue/g;
	$title =~s/[^a-zA-Z0-9_.~!-]/_/g;
	return $title;
}

sub title2url_part{
	my $title = shift;
	$title =~s/ /_/g;
	my $url_part = uri_escape($title);
	return $url_part;
}

sub update_edit_filter_index{
	my $bot = shift;
	my $mw = shift;
	my $options = shift;
	# first get recent changes
	my $max_num_talk_pages = 15;
	my $max_num_days_age = 5;
	my $edit_filter_name = 'Bearbeitungsfilter';
	# https://de.wikipedia.org/w/api.php?action=query&generator=allpages&gapnamespace=4&gapprefix=Bearbeitungsfilter/&gapfilterredir=nonredirects&gaplimit=500&prop=info|revisions&rvprop=content|timestamp
	$mw->{config}->{api_url} = $options->{'api'};
	my $pages = [
		sort {$b->{'revisions'}->[0]->{'timestamp'} cmp $a->{'revisions'}->[0]->{'timestamp'}}
		@{get_pages_by_prefix($mw, {
			'namespace_id' => 4,
			'prefix'       => $edit_filter_name.'/',
			'title_re'     => qr/^Wikipedia:$edit_filter_name\/\d+/,
		})}
	];
	my $i = 0;
	my $tables = [{
		'header' => 1,
		'body' => [['Regel', "letzte Seiten\xe4nderung", 'Thread', 'letzter Autor']],
	}];
	for my $page(@$pages){
		#msg(" $page->{'revisions'}->[0]->{'timestamp'}: $page->{title}") if $options->{'verbose'} > 1;
		my $npost = newest_post_info($page->{'revisions'}->[0]->{'*'});
		next unless defined $npost->{'thread'};
		++$i;
		# page title
		$page->{'title'}=~/\D(\d+)$/;
		$tables->[0]{'body'}[$i][0] = "[[$page->{'title'}|#$1]]";
		# date of last change
		my $date_of_last_change = '{{#timel:Y-m-d H:i:s|'.$page->{'revisions'}->[0]->{'timestamp'}.'}}';
		$tables->[0]{'body'}[$i][1] = $date_of_last_change;
		# thread title
		my $thread_anchor = "{{subst:anchorencode:$npost->{'thread'}}}";
		my $thread_title = $npost->{'thread'};
		$thread_title =~s/\[\[[^|\]]*\|([^|\]]*)\]\]/$1/g;
		$thread_title =~s/\[\[|\]\]//g;
		$tables->[0]{'body'}[$i][2] = "[[$page->{'title'}#$thread_anchor|".$thread_title.']]';
		# author
		$tables->[0]{'body'}[$i][3] = $npost->{'author'} eq '' ? '' : "[[special:contributions/$npost->{'author'}|$npost->{'author'}]]";
		last if $page->{'revisions'}->[0]->{'timestamp'} lt strftime("%Y-%m-%d", gmtime(time-60*60*24*$max_num_days_age)) and $i>$max_num_talk_pages;
	}
	#print table2wikitext($tables->[0], {'table' => 'class="wikitable sortable"'}, 1);
	#now update page
	my $page    = 'Wikipedia:'.$edit_filter_name.'/latest topics';
	my $summary = 'index table updated';
	rebuild_table($bot, $page, $tables, $summary, $options);
}

sub update_maintenance_lists{
	my $bot           = shift; # bot
	my $page          = shift; # page title or undef; undef means: force update without checking conditions
	my $text          = shift; # ref to string (or undef if $page is undef)
	my $type          = shift; # type of maintenance list
	my $maintenance_lists = shift; # maintenance lists
	my $time_stack    = shift;
	my $user_answer   = shift;
	my $options       = shift;
	if(defined $page and not defined $text){
		my $text = $bot->get_text($page);
		$text = \$text;
	}
	if(($type eq 'ASIN' or $type eq 'all_types') and 
		(not defined $page or $$text=~/$_re_ASIN/)
	){
		msg('check ASIN') if $options->{'verbose'} > 0;
		my $ml = 'User:CamelBot/maintenance list/ASIN';
		my $ml_text = $bot->get_text($ml) // '';
		my $ml_text_bak = $ml_text;
		my $intro = "dies ist eine durch CamelBot befuellte wartungsliste, die kuerzlich bearbeitete artikel auflistet, die mind. eine [[Amazon Standard Identification Number|ASIN]] enthalten.\n
CamelBot lauscht auf den recent changes, d.h. er schaut sich alle artikel an, die aktuell geaendert wurden. faellt ihm dabei auf, dass in einem artikel eine ASIN steht, meldet er das nach ca. 30 minuten hier auf dieser seite. dabei fuegt er gefundene artikel nur der liste hinzu. loeschen muss man derzeit noch manuell.\n
== liste ==\n";
		my $outro = "\n\n[[Kategorie:Wikipedia:Wartungskategorie|ASIN]]";
		my $table;
		my $wikitable;
		my $tablestyle = {
			'table'  => 'class="wikitable sortable"',
		};
		if($ml_text =~/^(.*\n== [lL]iste? ==\n+.*?)(\{\|.*?\|\})\n(.*)$/s){ # extract (save) old intro
			$intro = $1;
			$wikitable = $2;
			$outro = $3;
		}else{
			msg('could not fetch table. resetting page.', 'notice') if $options->{'verbose'} > 0;
			$wikitable = '{| '.$tablestyle->{'table'}."\n".
				"! page !! recognized ASIN by CamelBot\n".
				'|}';
		}
		$table = wikitable2array(\$wikitable);
		# check all old entries; search new entry, if not in array, then push
		my $already_in_list = 0;
		my $new_entry = defined $page ? "[[$page]]" : undef;
		$table->{'body'} = [grep {
			my $text = '';
			if(defined $new_entry and $_->[0] eq $new_entry){
				msg("page '$page' already in list") if $options->{'verbose'} > 0;
				$already_in_list = 1;
			}elsif($_->[0] =~ /^\[\[(.*)\]\]$/){
				my $page_with_asin = $1;
				$text = $bot->get_text($page_with_asin);
			}
			$text =~ /$_re_ASIN/ or $already_in_list;
			} @{$table->{'body'}}
		];
		if(defined $new_entry and $already_in_list==0){
			# add new list entry
			push @{$table->{'body'}}, [$new_entry, strftime("%Y-%m-%d", gmtime())];
		}
		$maintenance_lists->{'ASIN'} = {}; # reset and refill list
		for my $entry(@{$table->{'body'}}){
			$maintenance_lists->{'ASIN'}->{substr($entry->[0], 2, -2)} = 1;
		}
		unshift @{$table->{'body'}}, [@{$table->{'header'}}];
		$table->{'header'} = 1;
		my $full_auto = 0;
		$wikitable = table2wikitext($table, $tablestyle, $full_auto);
		# rebuild page
		$ml_text = $intro.$wikitable.$outro;
		time_management($time_stack, $options->{'max_edits_per_min'}, 
			$options->{'verbose'});
		save_wiki_page($bot, $ml, 'update maintenance list', \$ml_text, 
			\$ml_text_bak, $time_stack, $user_answer, $options);
	}
	return 1;
}

sub upload_file{
	my $mw = shift;
	my $filename_src = shift;
	my $summary = shift;
	my $filename_dest = shift // $filename_src;
	my $options = shift;
	my $data = read_file_binary($filename_src);
	# check, whether file exists already. If so, compare. don't upload, if same.
	msg(" downloading file if existant") if $options->{'verbose'} > 1;
	my $oldfile = $mw->download({'title' => 'File:'.$filename_dest});
	if(not defined $oldfile or $mw->{error}->{code}!=0){
		msg($mw->{error}->{code} . ': ' . $mw->{error}->{details});
		return 0;
	}
	msg(" downloaded file (if existant)") if $options->{'verbose'} > 2;
	if($oldfile eq '' or $oldfile ne $$data){
		msg(" trying to upload new file") if $options->{'verbose'} > 2;
		if(!$options->{'simulation'}){
			# upload
			#$mw->edit({
			# 'action'   => 'upload',
			# 'filename' => $filename_src,
			# 'comment'  => $summary,
			# 'file'     => [undef, $filename_dest, 'Content'=>$$data],
			$mw->upload({
					'data'    => $$data,
					'summary' => $summary,
					'title'   => $filename_dest,
				}) or die $mw->{error}->{code} . ': ' . $mw->{error}->{details};
		}
		return 1;
	}else{
		msg("won't force uploading. file is already online.");
		return 0;
	}
}

sub url2title{
	my $url     = shift;
	$url =~ /^(?:https?:\/\/de\.wikipedia\.org\/wiki\/)?(.*)/;
	my $article_name = uri_unescape($1);
	$article_name =~s/_/ /g;
	# 2014-12-23, seth: "https://de.wikipedia.org/wiki/K%C3%B6nigliche_und_barmherzige_Vereinigung_der_Ordens-_und_Medaillentr%C3%A4ger_von_Belgien" became: "KÃ¶nigliche und barmherzige Vereinigung der Ordens- und MedaillentrÃ¤ger von Belgien"
	utf8::decode($article_name);
	$article_name =~ s/^(?=Datei:|Bild:|Image:|File:)/:/;
	return $article_name;
}

sub wikitableHeader2array{
	my $header = shift;
	$header =~s/^\s*!\s*//s;
	$header =~s/\s+$//s;
	if(index($header, '!!') > -1){
		$header = [split /\s*!!\s*/, $header];
	}else{
		$header = [split /\s*!\s*/, $header];
	}
	return $header;
}

sub wikitable2array{
	my $wikitable = shift;
	$$wikitable =~/^\{\|   # begin of table
		(?-s:.*)\n+          # class, id, ...
		((?:!\s*[^\n]*\n+)*) # table header
		(.*)                 # table body
		(?<=\n)\|\}$         # end of table
	/sx;
	my $table = {
		'header' => wikitableHeader2array($1),
		'body' => $2,
	};
	# decomposing wiki-table to array
	$table->{'body'} = table_body2array(\$table->{'body'});
	return $table;
}

sub get_http_status{
	my $url = shift;
	my $lwp = LWP::UserAgent->new(
		'keep_alive' => 1, 
		'timeout' => 10, 
		'agent' => 'Mozilla/5.0'
	);
	my $response = $lwp->head($url);
	return $response->code;
}

sub msg{
	my $msg      = shift;
	my $type     = shift;
	$type = (defined $type ? "$type in ": '');
	my $timestamp = strftime("%Y-%m-%d %H:%M:%S", gmtime());
	# my ($package, $filename, $line, $subr, $has_args, $wantarray, $evaltext, 
	# $is_require, $hints, $bitmask, $hinthash) = caller(0);
	my @callers = caller(0);
	my $line = $callers[2];
	@callers = caller(1);
	my $subr = $callers[3];
	print "$timestamp $type$subr:$line: $msg\n";
	return 1;
}

sub num_unique_elem{
	my %hash;
	@hash{@_} = 1;
	return ''.(keys %hash);
}

sub parse_rc{
	my $msg = shift;
	my $verbose = shift;
	my %parsed_msg;
	if($msg=~/^\cC14
		\[\[\cC07([^\]]+)\cC14]]\cC4\x20   # page
		([a-z0-9_-]+|[!NMB]*)\cC10\x20     # flags
		\cC02([^\cC]*)\cC\x20\cC5\*\cC\x20 # url
		\cC03([^\cC]*)\cC\x20\cC5\*\cC\x20 # user
		(\(\cB?[+-]\d+\cB?\)|)\x20         # diffbytes
		\cC10(.*[^\cC]|)\cC?\z/x){         # summary
		$parsed_msg{'page'} = $1;
		$parsed_msg{'flags'} = $2;
		$parsed_msg{'url'} = $3;
		$parsed_msg{'user'} = $4;
		$parsed_msg{'diffbytes'} = $5;
		$parsed_msg{'summary'} = $6;
		if($parsed_msg{'diffbytes'} =~ /^\(\cB?([+-]\d+)\cB?\)$/){
			$parsed_msg{'diffbytes'} = $1;
		}
		print Data::Dumper->Dump([\%parsed_msg]) if $verbose>4;
	}else{
		msg($msg) if $verbose > 1;
	}
	return \%parsed_msg;
}

sub read_file_binary{
	my $filename = shift;
  open(my $FILE, '<:encoding(UTF-8)', $filename) or die $!;
		binmode $FILE;
		my $buffer;
		my $data = '';
		while(read($FILE, $buffer, 65536)){
			$data .= $buffer;
		}
  close($FILE);
	return \$data;
}

sub syntaxCheck{
	my %params = ( # default cli params
		'cat-add'            => 0,     # add pages to a category
		'cat-change'         => 0,     # cat change
		'create-user-page'   => undef, # create user page of given user if page does not exist
		'delete'             => 0,     # delete pages
		'download-by-prefix' => undef, # download pages by prefix
		'http-status'        => undef, # check reachability of url
		'irc'                => 0,     # start irc bot
		'link-replacement'   => 0,     # link replacing
		'minor'              => 1,     # mark as minor
		'parse'              => undef, # wikitext-file to parse
		'save-as-html'       => undef, # download pages and save as html pages
		'section'            => undef, # section in wikipage
		'text-replacement'   => 0,     # text replacing
		'udest'              => undef, # destination filename
		'update-editfilter-index' => 0,# update edit filter index in w:de
		'upload'             => 0,     # upload a file
		'usercontribs'       => 0,     # fetch user contributions
		'username'           => $ENV{'USER'}, # user name
		'usource'            => undef, # source filename
		'usummary'           => undef, # summary/description
		'wikipage'           => undef, # wikipage
		'test'               => 0,     # show result only (without renaming)
		'verbose'            => 1,     # trace; grade of verbosity
		'version'            => 0,     # diplay version and exit
	);

	pod2usage(-exitval=>2) if @ARGV==0;
	GetOptions(\%params,
		"cat-add",
		"cat-change|c",
		"create-user-page=s",
		"delete",
		"download-by-prefix=s",
		"http-status=s",
		"irc",
		"link-replacement|l",
		"minor!",
		"parse=s",
		"save-as-html=s",
		"section=s",
		"text-replacement",
		"udest=s",
		"update-editfilter-index",
		"upload",
		"usercontribs|usercontributions",
		"username=s",
		"usource=s",
		"usummary=s",
		"wikipage=s",
		"test|t",
		"silent|quiet|q" => sub { $params{'verbose'} = 0;},
		"very-verbose" => sub { $params{'verbose'} = 2;},
		"verbose|v:+",
		"version|V" => sub { Getopt::Long::VersionMessage();}, # auto_version will not auto make use of 'V'
		"help|?|h" => sub { Getopt::Long::HelpMessage(-verbose=>99, -sections=>"NAME|SYNOPSIS|EXAMPLES");}, # auto_help will not auto make use of 'h'
		"man" => sub { pod2usage(-exitval=>0, -verbose=>2);},
	) or pod2usage(-exitval=>2);
	$params{'verbose'} = 1 unless exists $params{'verbose'};
	my @additional_params = (0,0); # number of additional params (min, max);
	if(@ARGV<$additional_params[0] or ($additional_params[1]!=-1 and @ARGV>$additional_params[1])){
		if($additional_params[0]==$additional_params[1]){
			print "number of arguments must be exactly $additional_params[0], but is ".(0+@ARGV).".\n";
		}else{
			print "number of arguments must be at least $additional_params[0] and at most ".
				($additional_params[1] == -1 ? 'inf' : $additional_params[1]).", but is ".(0+@ARGV).".\n";
		}
		pod2usage(-exitval=>2);
	}
	return \%params;
}

sub show_diff{
	my $text      = shift;
	my $orig_text = shift;
	my $options   = shift;
	my $diff      = diff $text, $orig_text;
	my $result;
	print $diff if $options->{'showdiff'} & 1;
	if($options->{'showdiff'} & 2){
		my @text_lines = grep {s/^\s*(.*?)\s*$/$1/} split /\n/, $$text;
		my @orig_text_lines = grep {s/^\s*(.*?)\s*$/$1/} split /\n/, $$orig_text;
		my @lines_to_show = ();
		while($diff=~/@@ -(\d+),(\d+) \+\d+,\d+ @@/g){
			push @lines_to_show, [$1-1, $1+$2-2];
		}
		#print Dumper(\@lines_to_show);
		#print Dumper(\@text_lines);
		my $shortend_text = '';
		my $shortend_orig_text = '';
		for(@lines_to_show){
			for($$_[0]..$$_[1]){
				$shortend_text.=$text_lines[$_]."\n";
				$shortend_orig_text.=$orig_text_lines[$_]."\n";
			}
		}
		chomp($shortend_text);
		chomp($shortend_orig_text);
		my $mw = new MainWindow;
		my $w = $mw->DiffText(-width=>1500, -height=>500, -orient => 'horizontal', -map => 'scrolled')->pack();
		$w->load('a' => $shortend_text);
		$w->load('b' => $shortend_orig_text);
		$w->compare(-granularity => 'word');
		$result = '0';
		$mw->bind('<KeyPress-y>' => sub {$result = '1'; $mw->destroy;} );
		$mw->bind('<KeyPress-q>' => sub {exit 0;} );
		$mw->bind('<KeyPress>' => sub {$result = '0'; $mw->destroy;} );
		MainLoop;
	}
	return $result;
}

sub write_csv{
	my $array = shift;
	my $filename = shift;
	my $separator = shift // ';';
	open(my $OUTFILE, '>:encoding(UTF-8)', $filename) or die "$!\n";
		for my $row(@$array){
			map {
				s/([\\'])/\\$1/g; # escape \ and '
				$_ = "'$_'";  # entry -> 'entry'
			} @$row;
			print $OUTFILE join $separator, @$row;
			print $OUTFILE "\n";
		}
	close($OUTFILE);
	return 1;
}

# main
my $params = syntaxCheck(@ARGV);
my ($bot, $mw, $api) = login($params->{'username'}, undef, $params);
my %options = (
	'username'          => $params->{'username'},# username of bot account
	'minor'             => $params->{'minor'},   # 0 = non-minor; 1 = minor
	'simulation'        => $params->{'test'},    # 0 = do real edits; 1 = simulate only
	'max_edits_per_min' => 5,                  # maximum number of edits per minute (-1 = inf)
	'askuser'           => 0,                  # 0 = don't ask user; 1  = ask user for every action
	'showdiff'          => 0,                  # 0 = don't show diff; 1 = show a diff, 2 = gui
	'verbose'           => $params->{'verbose'},
	'api'               => $api,
);

if($params->{'cat-add'}){
	my $pages = [
		# list of articles {{{
		# }}}
	];
	my $category = 'safety data sheet';
	cat_add($bot, $pages, $category, \%options);
}

if($params->{'cat-change'}){
	cat_rename($bot, 'Software tool', 'Library or package', \%options);
}

if($params->{'delete'}){
	my @time_stack = ();           # needed for edit/min constraint
	my $user_answer = '';          # user answer initially is "no"
	my $summary = '';
	my $delete_cat = 'Category:Löschen';
	utf8::decode($delete_cat);
	my @pages = grep {!/Template:L.schen/} $bot->get_pages_in_category($delete_cat);
	msg('found '.@pages.' pages.') if $options{'verbose'} > 0;
	msg('adding manually set pages') if $options{'verbose'} > 1;
	push @pages, ();
	for(@pages){
		msg(" $_") if $options{'verbose'} > 2 or $options{'verbose'} > 0 and $options{'simulation'};
		time_management(\@time_stack, $options{'max_edits_per_min'}, $options{'verbose'});
		delete_wiki_page($mw, $_, $summary, \@time_stack, \$user_answer, \%options);
	}
}

if(defined $params->{'download-by-prefix'}){
	download_pages_by_prefix($bot, $params->{'download-by-prefix'}, $params->{'verbose'});
}

if(defined $params->{'http-status'}){
	my @urls = (
		#'http://de.wikipedia.org/example',
		split / /, $params->{'http-status'}
	);
	for my $url(@urls){
		my $response_code = get_http_status($url);
		msg($response_code.' '.$url);
		sleep 5;
	}
}

if($params->{'irc'}){
	update_edit_filter_index($bot, $mw, \%options);
	package CamelBot;
	use base qw(Bot::BasicBot); # irc bot
	$| = 1; # deactivate buffering, so flush all the time

	my $edit_filter_name_re = qr/(?:Bearbeitung|Missbrauch)sfilter/;
	my %rc_pages_shortterm;   # rc pages to check
	my %rc_pages_midterm;     # rc pages to check
	my %rc_pages_maintenance; # rc special pages to check
	my @time_stack = ();      # needed for edit/min constraint
	my $user_answer = '';     # user answer initially is "no"
	# initially update and fetch maintenance lists 
	::update_maintenance_lists($bot, undef, undef, 'all_types', 
		\%rc_pages_maintenance, \@time_stack, \$user_answer, \%options);
	my $delay_short = 10; # seconds
	my $delay = 30*60; # seconds
	my $start_ts = time; # only for debugging purposes
	my $namespaces_re = join '|', grep {$_ ne ''} values {$bot->get_namespace_names()}; # get all namespaces except 0
	$namespaces_re = qr/^$namespaces_re:/;

	# bot help
	sub help {
		return 'i\'m just listening to recent changes in w:de and sometimes those changes trigger some of my functions. if you have any questions, please ask my master at https://de.wikipedia.org/wiki/user_talk:lustiger_seth.';
	}

	sub rc_hook{
		my $rc_msg = shift;
		$rc_pages_midterm{$rc_msg->{'page'}} = time;
		$rc_pages_shortterm{$rc_msg->{'page'}} = time;
		::msg('irc: '.($rc_pages_midterm{$rc_msg->{'page'}}-$start_ts).
			' (#pages in stack: '.scalar(keys %rc_pages_midterm).') '.$rc_msg->{'page'}.
			', editor = '.$rc_msg->{'user'}) if $options{'verbose'} > 1;
		while(my ($type, $pages) = each %rc_pages_maintenance){
			if(exists $pages->{$rc_msg->{'page'}}){
				::update_maintenance_lists($bot, $rc_msg->{'page'}, undef, $type, 
					\%rc_pages_maintenance, \@time_stack, \$user_answer, \%options);
			}
		}
		# delayed clean up:
		my @updList_midterm;
		my @updList_shortterm;
		# the $now variable should be defined outside of following loop, because 
		# otherwise this could cause parallel (redundant?) loops for the same page. maybe
		# time should be even more precise (Time::HiRes) to prevent from that effect.
		my $now = time;
		# after delay transfer pages from $rc_pages_shortterm to @updList_shortterm
		while(my ($title, $timestamp) = each %rc_pages_shortterm){
			if($timestamp + $delay_short < $now){
				delete $rc_pages_shortterm{$title};
				::msg(' short term update of page: '. $title) if $options{'verbose'} > 1;
				push @updList_shortterm, $title;
			}
		}
		# after delay transfer pages from $rc_pages_midterm to @updList_midterm
		while(my ($title, $timestamp) = each %rc_pages_midterm){
			if($timestamp + $delay < $now){
				delete $rc_pages_midterm{$title};
				::msg(' mid term update of page: '. $title) if $options{'verbose'} > 1;
				push @updList_midterm, $title;
			}
		}
		# all pages in @updList_shortterm shall be treated now.
		for my $page(@updList_shortterm){
			# notifier on particular edits; there needs to be some delay, because 
			# otherwise it may occur that the notification is faster than the saving 
			# process itself.
			::notifier($bot, $page, \@time_stack, \$user_answer, \%options);
		}
		# all pages in @updList_midterm shall be treated now.
		for my $page(@updList_midterm){
			my $text = $bot->get_text($page);
			if(defined $text and ::is_allowed(\$text, $options{'username'}, $page)){
				my $text_bak = $text;
				my $changes = ::cleanup_wiki_page(\$text, $page, $options{'verbose'});
				if($text_bak ne $text){
					my $summary = 'kleinere korrekturen ('.
						(join ', ', grep {$changes->{$_} > 0} keys %$changes).
						'). feedback bitte auf [[user talk:lustiger_seth]] melden.';
					::time_management(\@time_stack, $options{'max_edits_per_min'}, $options{'verbose'});
					#::msg("* save $page_");
					::save_wiki_page($bot, $page, $summary, \$text, \$text_bak, \@time_stack, 
						\$user_answer, \%options);
				}
				::update_maintenance_lists($bot, $page, \$text, 'all_types', 
					\%rc_pages_maintenance, \@time_stack, \$user_answer, \%options);
			}else{
				::msg("could not fetch page '$page'. maybe deleted already", 'notice') if $options{'verbose'} > 1;
			}
		}
	}

	# bot handler: if somebody said something
	sub said {
		my ($self, $message) = @_;
		# print everything what is said to you
		::msg("$message->{who}: $message->{body}") if $options{'verbose'} > 3;

		# general
		if(defined $message->{address} && $message->{address} eq $self->{nick}){
			::msg($message->{who}.': '.$message->{body}) if $options{'verbose'} > 1;
			# help
			if($message->{body}=~/^(?:help|was machst du\?|was kannst du\?)/){
				$self->reply($message, help());
			}
		}

		# rc-channel
		if($self->{'alias'} eq 'rc'){
			# reporting of specific edits
			if($message->{who} =~/^rc-\w+$/){
				# parse rc message
				my $rc_msg = ::parse_rc($message->{body}, $options{'verbose'});
				if(# $rc_msg->{'user'} eq 'Lustiger seth' or # for debugging only
					$rc_msg->{'user'} ne $options{'username'} and # don't track own edit
					$rc_msg->{'summary'} !~ /\bCamelBot\b.*\b(?:rückgängig gemacht|revertiert)\b/ and # don't play edit war
					(index($rc_msg->{'page'}, ':') == -1 or $rc_msg->{'page'} !~ /$namespaces_re/)
				){
					rc_hook($rc_msg);
				}
				#print ::Dumper $rc_msg;
				# update edit filter index
				if($rc_msg->{'page'} =~ /^Wikipedia:$edit_filter_name_re\/\d+$/){
					::update_edit_filter_index($bot, $mw, \%options);
				}
			}
		}
	}

	my $username = 'CamelBot';
	my $bot_rc = CamelBot->new(
		server => 'irc.wikimedia.org',
		channels => ['#de.wikipedia'],
		nick => $username,
		username => $username,
		name => $username,
		alias => 'rc',
		# no_run => 1,
	);
	$bot_rc->run();
	#use POE;
	#$poe_kernel->run();
}

if($params->{'link-replacement'}){
	$options{'articles'}      = 1;  # 0 = don't work on namespace 0; 1 = work on namespace 0
	$options{'nonarticles'}   = 1;  # 0 = don't work on namespace!=0; 1 = work on namespace!=0
	$options{'refs'}          = 1;  # 0 = don't touch refs; 1 = replace refs
	$options{'nonrefs'}       = 1;  # 0 = don't touch non-refs; 1 = replace non-refs
	$options{'max_edits'}     = -1;  # maximum number of edits (-1 = inf)
	$options{'skip_edits'}    = 0;  # skip number of pages
	$options{'results'}       = 1;  # print a summary at the end
	# user to use in api contributions
	$options{'user_contribs'} = undef; # {'user' => 'Gary Dee'};
	# searched_link = link to use in api link search
	# re_prot_part  = regexp pattern of protocol
	# re_url_part   = regexp pattern of searched url (without protocol)
	# replacement   = replacement part of s///
	# summary       = summary of page edit
	
	$options{'re_prot_part'}  = qr/http:\/\//;
	# some typical cases:
	# 1. linkfix
	#$options{'searched_link'} = '*.bgblportal.de/BGBL/bgbl1f/';
	#$options{'searched_link'} = '217.160.60.235';
	#$options{'searched_link'} = 'eur-lex.europa.eu/legal-content/';
	#$options{'searched_link'} = 'eur-lex.europa.eu/LexUriServ/site';
	##$options{'re_url_part'}   = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/bgbl(\d+s\d+[a-z]?)\.?pdf/;
	##$options{'re_url_part'}   = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/b(\d+)[a-z]\.?pdf/;
	#$options{'re_url_part'}   = qr/217.160.60.235\/BGBL\/bgbl1f\/b(?:gbl)?(\d+s\d+[a-z]?|\d+)[a-z]?\.?pdf/;
	#$options{'re_url_part'}   = qr/eur-lex.europa.eu\/legal-content\/.*?uniserv/;
	#$options{'re_url_part'}   = qr/eur-lex.europa.eu\/LexUriServ\/site\//;
	#$options{'delete_link'}   = undef;
	#$options{'replacement'}   = # function disabled, has to be re-implemented
	# $from = qr/(\Qeur-lex.europa.eu\/legal-content\/\E[A-Z]{2}\/TXT\/PDF\/\?uri=)uniserv(:OJ\.[A-Z_]+\.[0-9]{4}\.[0-9]+\.01)([0-9]{4}\.01\.[A-Z]{3})$/;
	# $to   = 'http://'.$1.'uriserv'.$2.'.'.$3;
	# $from = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/bgbl(\d+s\d+[a-z]?)\.?pdf/;
	# $to   = 'http://www.bgbl.de/Xaver/start.xav?startbk=Bundesanzeiger_BGBl&start=//*%5B\@attr_id=%27bgbl'.$1.'.pdf%27%5D';
	#$options{'summary'}       = 'link fixes, see [[WP:FZW#Mehrere_Hundert_Weblinks_auf_EUR-Lex_defekt]]';

	# 2. linkfix
	#$options{'searched_link'} = '*.google.com';
	#$options{'re_url_part'}   = qr/[a-z0-9]+\.google\.[a-z]+\/.*?[?&]url=[^&]+/;
	#$options{'replacement'}   = sub {
	# $from = qr/[a-z0-9]+\.google\.[a-z]+\/.*?[?&]url=([^&]+)/;
	# $to   = $1;
	#$options{'summary'}       = 'resolve google redirects, see [[mTalk:Spam_blacklist#Google_redirect_spam]]';
	
	# 3. linkfix: remove double http://
	#$options{'searched_link'} = "http//";
	#$options{'summary'}       = 'link fixes, siehe [[Wikipedia:Bots/Anfragen/Archiv/2011-1#fehlerhafte_externe_links_mit_doppeltem_protokoll]]';
	#$options{'re_url_part'}   = qr/./;
	
	# 4. delete/unlink blacklisted links
	my $simple_domain         = 'example.com';
	$options{'searched_link'} = "*.$simple_domain";
	$options{'re_url_part'}   = qr/(?:[a-zA-Z0-9-]+\.)?(?:\Q$simple_domain\E)/;
	$options{'delete_link'}   = 1;
	#$options{'ref2deadlink'}  = 0;
	#$options{'summary'}       = "website is not compatible with WP:EL and content has changed, see [[WP:SBL#$simple_domain]]";
	$options{'summary'}       = "domain is on blacklist, see [[WP:SBL#$simple_domain]]";
	#$options{'summary'}       = "1. link fixes; 2. domain $simple_domain is on blacklist, see [[WP:SBL#$simple_domain]]";
	link_replacement($mw, $bot, \%options);
}

if(defined $params->{'parse'}){
	# filenames
	my $filename = $params->{'parse'};
	my $htmlfilename = $filename;
	$htmlfilename =~s/\.[a-zA-Z0-9_-]*$/.html/;
	$htmlfilename = $filename.'.html' if $htmlfilename eq $filename;
	# read wikitext
	my $wikitext = slurp $filename;
	# convert to html
	$mw->{config}->{api_url} = $options{'api'};
	my $html = parse_wikitext($mw, $wikitext);
	my $html_cleaned = post_process_html($html);
	$html_cleaned=~s/\n\K\n+//g;
	$html_cleaned=~s/.*?<body>\n?//sg;
	$html_cleaned=~s/<\/body>\n<\/html>\s*$//sg;
	# write result
	write_file($htmlfilename, {binmode => ':utf8'}, $html_cleaned);
}

if(defined $params->{'save-as-html'}){
	$mw->{config}->{api_url} = $options{'api'};
	my ($html, $title, $headhtml) = parse_page($mw, $params->{'save-as-html'});
	# set html title
	$headhtml=~s/<title>\K[^<]+(?=<\/title>)/$title/;
	# delete js and other useless stuff
	$headhtml =~ s/<script\b[^>]*>.*?<\/script>//gs;
	$headhtml =~ s/<link rel="(?:search|alternate|EditURI|shortcut icon)" [^>]+>//g;
	$headhtml =~ s/<meta name="(?:generator|ResourceLoaderDynamicStyles)"[^>]+>//g;
	$headhtml =~ s/\n\/\* cache key: .*?\*\///g;
	$headhtml =~ s/\n\K\n+//g;
	# first heading
	$headhtml .= "\n".'<h1 id="firstHeading" class="firstHeading"><span dir="auto">'.$title.'</span></h1>'."\n";
	# get css files
	my $css_directory = 'css';
	my $images_directory = 'images';
	mkdir $images_directory unless -d $images_directory;
	mkdir $css_directory unless -d $css_directory;
	$headhtml =~ s/<link rel="stylesheet" href=\K"([^"]*)"/download_css($1, $css_directory, $images_directory)/ge;
	# get image files
	my @images = ($html =~ /<a href="\/wiki\/File:([^"]+)" [^>]*\bclass="image"/g);
	my $no_warn_files = [];
	download_files($bot, \@images, $no_warn_files, "$images_directory/", $params->{'verbose'});
	# remove navbar
	$html =~ s/<tr>\s*<td[^>]*>\s*<div [^>]*class="[^"]*navbar[^"]*"[^>]*>\s*.*?<\/div>\s*<\/td>\s*<\/tr>//s;
	# remove edit-links
	$html =~ s/<span class="editsection">\[<a href="[^"]*\/index.php?[^"]*\baction=edit[^"]*"[^>]*>(?i:edit|bearbeiten)<\/a>\]<\/span> *//g;
	# clean up html source
	my $html_cleaned = post_process_html($headhtml.$html, "$images_directory/");
	# remove icon-like images ("magnify")
	$html_cleaned =~ s/<div class="magnify">\s*<img src="[^"]*"[^>]*>\s*<\/div>//gs;
	# change internal links
	$html_cleaned =~ s/<a [^>]*\bhref="\K\/wiki\/([^"]+)/$1.html/g;
	# save to file
	my $htmlfilename = $params->{'save-as-html'}.'.html';
	$htmlfilename =~y/ /_/;
	# create directory if necessary
	make_path($1) if $htmlfilename =~/^(.*)\/[^\/]*$/;
	write_file($htmlfilename, {binmode => ':utf8'}, $html_cleaned);
}

if($params->{'text-replacement'}){
# choose pages
# by user
	#$options{'username'}      = 'seth';
	#$options{'ucstart'}       = '2011-12-14T00:00:00Z';
	#$options{'ucend'}         = undef; #'2010-12-14T00:00:00Z';
	#$options{'uclimit'}       = 269;
	#$options{'pages'}         = [keys %{get_user_contribs(\%options)}];
# by whatlinkshere
	use utf8;
	$options{'pages'}         = [
		map {$_->{'title'}} ($bot->what_links_here('Perl/Module/Einführung in Perl-Module'),
		 $bot->what_links_here('Perl/Module/Einführung in Perl-Modul'),
		 $bot->what_links_here('Perl/Module/Hinweise zum Arbeiten mit Modulen'),
		 $bot->what_links_here('Perl/Listen bzw. Arrays (Variablen)'),
		 $bot->what_links_here('Perl/Hashes (Variablen)'),
		 $bot->what_links_here('Doku:Perl/Module/CPAN-Module'),
		 $bot->what_links_here('Perl/Skalare (Variablen)'),
		)
	];
# or explicite
	#$options{'pages'}         = [
	#	'Perl',
	#	'Perl Dingens',
	#];

	print Dumper $options{'pages'};

# search and replace
#  case 0: link replacement
	#$options{'pattern'}       = [
	#	qr/<nowiki>(http:\/\/world.guns.ru\b[^\]]*?\].*?)<\/nowiki>&nbsp;<small>'''\(Achtung: Bitte diese Website nicht aufrufen, da sie gef.hrliche Software verbreitet!\)'''<\/small>/,
	#	qr/<nowiki>(http:\/\/world.guns.ru\b.*?)<\/nowiki>&nbsp;<small>'''\(Achtung: Bitte diese Website nicht aufrufen, da sie gef.hrliche Software verbreitet!\)'''<\/small>/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]]+\])/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)( - '''erledigt)/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+?)( auf Modern Firearms \('*en(?:gl.*?)?\))/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+?)( \('*en(?:gl.*?)?\))/,
	#	qr/(\*\s*)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+)/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)\s*\n/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*),/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)/,
	#]; # search patterns s/X//
	#$options{'replacement'}   = [
	#	'"[".$1',
	#	'$1',
	#	'"[".$1',
	#	'"[".$1."]".$2',
	#	'"[".$1."]".$2',
	#	'"[".$1."]".$2',
	#	'$1."[".$2."]"',
	#	'"[".$1."]"',
	#	'"[".$1."],"',
	#	'"[".$1."]"',
	#];   # replace texts s//X/e
#  case 1: internal linkfixes
	$options{'pattern'}       = [
		qr/\[\[Perl\/Module\/Standardmodule\K[_ ]von[_ ]Perl/,
		qr/\[\[Perl\/Module\/Einführung[_ ]in[_ ]Perl-Module?/,
		qr/\[\[Perl\/Module\/Hinweise[_ ]zum[_ ]Arbeiten[_ ]mit[_ ]Modulen/,
		qr/\[\[Doku:Perl\/Module\/CPAN-Module/,
		qr/\[\[Perl\/Skalare\K[_ ]\(Variablen\)/,
		qr/\[\[Perl\/Hashes\K[_ ]\(Variablen\)/,
		qr/\[\[Perl\/Listen[_ ]bzw\.[_ ]Arrays\K[_ ]\(Variablen\)/,
	]; # search patterns s/X//
	$options{'replacement'}   = [
		"''",
		"'[[Perl/Module/Einführung'",
		"'[[Perl/Module/Einführung'",
		"'[[Perl/Module/Einführung'",
		"''",
		"''",
		"''",
	]; # replace texts s//X/e

	# edit behaviour
	$options{'max_edits'}     = -1;  # maximum number of edits (-1 = inf)
	$options{'skip_edits'}    = 0;  # skip number of pages
	$options{'summary'}       = 'linkfixes (grmpf utf-8-gedoens)';
	text_replacement($bot, \%options);
}

if($params->{'update-editfilter-index'}){
	update_edit_filter_index($bot, $mw, \%options);
}

if($params->{'update-table'}){
	# get information somehow
	# write all information to a table structure
	my $tables;
	# table header
	$tables->[0]{'header'} = 1;
	$tables->[0]{'section'} = 'somesection';
	$tables->[0]{'body'}[0] = ['row0', 'row1', 'row2'];
	$tables->[0]{'class'} = 'wikitable';
	# table body
	my $col = 1;
	#for(sort keys %packages){
	#	$tables->[0]{'body'}[$col][0] = '[['.$packages{$_}{'name'}.']]';
	#	$tables->[0]{'body'}[$col][1] = $packages{$_}{'archs'};
	#	$tables->[0]{'body'}[$col][2] = $packages{$_}{'description'};
	#	++$col;
	#}
	# select page, section, ...
	my $page    = '';
	my $summary = 'table updated';
	rebuild_table($bot, $page, $tables, $summary, \%options);
}

if($params->{'upload'}){
	my $source  = $params->{'usource'};
	my $summary = $params->{'usummary'};
	my $dest    = $params->{'udest'};
	die "error in upload: source file not defined!\n" unless defined $source;
	die "error in upload: file '$source' not found!\n" unless -e $source;
	die "error in upload: summary not defined!\n" unless defined $summary;
	die "error in upload: destination filename not defined!\n" unless defined $dest;
	msg("uploading file '$source' as '$dest' in wiki") if $options{'verbose'} > 0;
	upload_file($mw, $source, $summary, $dest, \%options);
}

if($params->{'usercontribs'}){
	$options{'username'}      = 'seth';
	$options{'ucstart'}       = '2011-12-14T00:00:00Z';
	$options{'ucend'}         = undef; #'2010-12-14T00:00:00Z';
	$options{'uclimit'}       = 269;
	$options{'pages'}         = get_user_contribs(\%options);
}

msg('finished') if $params->{'verbose'} > 1;

__END__

=head1 NAME

camelbot manipulates wiki pages

=head1 DESCRIPTION

this program is a CLI tool for manipulation pages of a MediaWiki. One of the main 
tasks is the replacement of external links (urls)

=head1 SYNOPSIS

camelbot [options]

general options:

     --http-status=s           check http status of given url
                                (status 200 or 301)
 -t, --test                    don't change anything, just print possible changes

mediawiki/wikipedia options:

 -c, --cat-change              replaces categories 
     --delete                  delete some pre-defined pages
     --download-by-prefix=s    download pages (and used images) with given prefix
     --irc                     start irc bot (for w:de)
 -l, --link-replacement        replaces links 
     --text-replacement        replaces text 
 -m, --minor=[01]              mark edit(s) as minor (1) or not (0), default = 1
     --parse=s                 parses wikitext from file and saves result as new 
                                file
     --save_as_html=s          saves a given wiki page as local html-file
 -t, --test                    don't change anything, just print possible changes
     --update-editfilter-index update edit filter index (at w:de)
     --upload                  upload a file
                               iff this param is set, you should additionally set 
                               the following params, too.
       --usource=s              source filename (e.g. '../somefile.txt')
       --usummary=s             a summary/description of the file
                                 (e.g. "robot cat with hat\\n\\n[[Category:Nonsense]]")
       --udest=s                destination filename (e.g. 'a_descriptive_name.txt')
     --usercontribs            fetch user contributions
     --username=s              login as different user (default = shell login name)

meta options:

 -V, --version                 display version and exit.
 -h, --help                    display brief help
     --man                     display long help (man page)
 -q, --silent                  same as --verbose=0
 -v, --verbose                 same as --verbose=1 (default)
 -vv,--very-verbose            same as --verbose=2
 -v, --verbose=x               grade of verbosity
                                x=0: no output
                                x=1: default output
                                x=2: much output

=head1 EXAMPLES

camelbot -cl
  replaces links and cats.

=head1 OPTIONS

=head2 GENERAL

=over 8

=item B<--cat-change>, B<-c>

replaces categories 

=item B<--delete>

delete some pre-defined pages

=item B<--download-by-prefix>=I<string>

download pages by given prefix and save the pages wieth the extension '.wikitext'.
all images used in those pages will be downloaded too.

=item B<--http-status>=I<string>

prints the http response status code of a given URL I<string>. If I<string> 
contains spaces, they are treated as separators between multiple URLs.

=item B<--irc>

start an irc bot. this will only be useful in de-wikipedia.

=item B<--link-replacement>, B<-l>

replace links

=item B<--minor>, B<--no-minor>

mark edit(s) as minor or not, default = B<--minor>.

=item B<--parse>=I<filename>

parses wikitext from file and saves result to file with same name but with 
extension .html.

=item B<--save-as-html>=I<pagename>

parses wikitext of given page and saves result to file with same name but with 
extension .html.

=item B<--test>, B<-t>

don't change anything, just print possible changes.

=item B<--text-replacement>

replace content of pages

=item B<--update-editfilter-index>

update overview of discussions concerning single rules of edit filter (in w:de)

=item B<--upload>

upload a file iff this param is set, you should additionally set the following 
params, too.

=item B<--usource>=I<string>

requires B<--upload> to be set.
source filename (e.g. '../somefile.txt')

=item B<--usummary>=I<string>

requires B<--upload> to be set.
a summary/description of the file (e.g. "robot cat with hat\\n\\n[[Category:Nonsense]]")

=item B<--udest>=I<string>

requires B<--upload> to be set.
destination filename (e.g. 'a_descriptive_name.txt')

=item B<--username=>I<string>

login as different user. default: I<string> = shell login name

=back

=head2 META

=over 8

=item B<--version>, B<-V>

prints version and exits.

=item B<--help>, B<-h>, B<-?>

prints a brief help message and exits.

=item B<--man>

prints the manual page and exits.

=item B<--verbose>=I<number>, B<-v> I<number>

set grade of verbosity to I<number>. if I<number>==0 then no output
will be given, except hard errors. the higher I<number> is, the more 
output will be printed. default: I<number> = 1.

=item B<--silent, --quiet, -q>

same as B<--verbose=0>.

=item B<--very-verbose, -vv>

same as B<--verbose=2>. you may use B<-vvv> for B<--verbose=3> a.s.o.

=item B<--verbose, -v>

same as B<--verbose=1>.

=back

=head1 LICENCE

Copyright (c) 2014, seth
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

originally written by seth (see https://github.com/wp-seth/camelbot)

=cut
