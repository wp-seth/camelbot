#!/usr/bin/perl
# author: seth
# description: a wikibot

use strict;
use warnings;
use Data::Dumper;          # for debugging purposes
use File::Path qw(make_path); # create directories
use File::Slurp qw(slurp write_file); # read/write files
use Getopt::Long qw(:config bundling); # cli params
use Pod::Usage;            # cli params help

# global vars
$main::VERSION = '1.14.20150222';

# packages: CamelBot, CamelBotRC, CamelBotIRC
#
# functions
# =========
# global functions
# -------------------
# sub syntaxCheck
# 	parse cli params
#
# CamelBot: mediawiki functions
# -------------------
# sub new
# 	constructor
#
# sub api_cont
# 	use MediaWiki::API using 'continue' for a loop
#
# sub cat_add
# 	adds a given bunch of pages to a given category by adding [[category:new]] at 
# 	the end
#
# sub cat_rename
# 	moves a category and replaces all [[category:old]] by [[category:new]]
#
# sub check_external_link
# 	given two successive parts $a1, $a2 of a string $a this function checks, 
# 	whether second part of string $a2 is an url with http status 200. the first 
# 	part of the string $a1 is used to detect the end of url in part two $a2.
#
# sub cleanup_wiki_page
# 	standard cleaning up
#
# sub convert_ns
# 	converts namespace id to its name or vice versa
#
# sub _db_init
# 	init db connection
#
# sub _db_query
# 	send query to db
#
# sub _db_simplequery
# 	wrapper for simple db queries
#
# sub db_fetch_recentchanges
# 	use db to get recent changes for a given time period
#
# sub db_fetch_sbl_log
# 	use db to get all entries of the sbl log of blocked edits; optional 
# 	(but recommended) given a regexp for a searched url
#
# sub delete_marked_pages
# 	delete pages that are in a special category
#
# sub delete_wiki_page
# 	deletes a wiki page
#
# sub download_css
# 	download a css file (not implemented yet: and all files loaded inside css)
#
# sub download_files
# 	downloads files from the wiki
#
# sub download_pages_by_prefix
# 	downloads all pages with the same prefix
#
# sub get_abuse_filter_info
# 	return some info on abuse filter rules
#
# sub get_namespace_id
# 	return namespace id of a given page
#
# sub get_page_info
# 	get information about a given page (or given pages)
#
# sub get_pages_by_prefix
# 	get all pages and their latest content by prefix
#
# sub get_tables_from_wikitext
# 	tries to return all tables from a given wikitext. (this will fail on tables in 
# 	tables!)
#
# sub get_user_contribs
# 	get list of pages edited by user
#
# sub is_allowed
# 	checks whether bot-template is placed on a page and forbids editing
#
# sub link_replacement
# 	replaces links in wiki pages
#
# sub login
# 	logs a bot in to some wiki
#
# sub newest_post_info
# 	\return hash ref with thread, author and date of newest post
# 	\param[in] $wikitext content of a talk page
#
# sub notifier
# 	notify on user or pre-defined wiki pages, if particular edits occur
#
# sub parse_wikitext
# 	use mediawiki-api of a wiki to parse wikitext
#
# sub parse_page
# 	use mediawiki-api of a wiki to get a parsed wikipage
#
# sub parse_irc_rc
# 	parse recent changes told by bot via irc. return ref to hash with content.
#
# sub php_code_unused
# 	searches for all links in a given piece of wikitext
#
# sub post_process_html
# 	clean up html that was generated by parsing wikitext
#
# sub rebuild_table
# 	builds a table and pastes it to a section (overwriting the section)
#
# sub save_wiki_page
# 	\brief saves a wiki page
# 	\param[in] $bot a logged in MediaWiki::Bot
# 	\param[in] $page_utf8 page name in utf8
# 	\param[in] $summary an edit summary (without 'Bot: ') that will be used when 
# 		saving
# 	\param[in] $text a ref to the new wikitext
# 	\param[in] $orig_text a ref to the old wikitext
# 	\param[in] $time_stack a ref to an array that is used for low frequency editing
# 	\param[in] $user_answer a ref to a string containing '' (nothing) or 'y' to 
# 		save the last user's answer
#
# sub search_sbl_blocked_log
# 	search the list of blocked edits (blocked by sbl) for a given url-regexp
#
# sub table2wikitext
# 	converts a special table hash ref to wikitext
#
# sub table_body2array
# 	convert wiki table body to perl ref to 2d array
#
# sub test_and_replace
# 	tests whether a regexp matches and replaces it. give some verbose output.
# 	checks urls, breaks if http status != 200
# 	returns number of replacements
#
# sub text_replacement
# 	replaces text in wiki pages
#
# sub time_management
# 	forces that only x edits per minute are done 
#
# sub title2filename
# 	converts a page title to a filename
#
# sub title2url_part
# 	convert article name to part of url with article name
#
# sub update_edit_filter_index
# 	update page with index of present threads about edit filters
#
# sub update_maintenance_lists
# 	update some maintenance lists on given edit
#
# sub upload_file
# 	uploads a file to the wiki
#
# sub url2title
# 	convert url to article name
#
# sub wikitableHeader2array
# 	convert wiki table header to perl ref to 1d array
#
# sub wikitable2array
# 	convert wiki table to perl ref to hash, containing a header and a body 
# 	containing a 2d array
#
# CamelBot: other functions
# ---------------
# sub get_http_status
# 	return http status code of a given url
#
# sub msg
# 	write message to STDOUT
#
# sub num_unique_elem
# 	return number of elements, without counting double entries
#
# sub read_file_binary
# 	read a file binary
#
# sub show_diff
# 	shows a diff via Tk
#
# sub write_csv
# 	write a 2d-array (= ref to array of refs to arrays) to a csv file

sub syntaxCheck{
	my %params = ( # default cli params
		'cat-add'            => 0,     # add pages to a category
		'cat-change'         => 0,     # cat change
		'delete'             => 0,     # delete pages
		'download-by-prefix' => undef, # download pages by prefix
		'http-status'        => undef, # check reachability of url
		'link-replacement'   => 0,     # link replacing
		'minor'              => 1,     # mark as minor
		'parse'              => undef, # wikitext-file to parse
		'rc-monitoring'      => undef, # start rc monitoring via irc or db
		'save-as-html'       => undef, # download pages and save as html pages
		'search-sbl-blocked-log' => undef, # search the sbl log of blocked edits
		'search-sbl-blocked-log-lang' => undef, # search the sbl logs of several wikis of blocked edits
		'section'            => undef, # section in wikipage
		'text-replacement'   => 0,     # text replacing
		'udest'              => undef, # destination filename
		'update-editfilter-index' => 0,# update edit filter index in w:de
		'upload'             => 0,     # upload a file
		'usercontribs'       => 0,     # fetch user contributions
		'username'           => $ENV{'USER'}, # user name
		'usource'            => undef, # source filename
		'usummary'           => undef, # summary/description
		'wikipage'           => undef, # wikipage
		'test'               => 0,     # show result only (without renaming)
		'verbose'            => 1,     # trace; grade of verbosity
		'version'            => 0,     # diplay version and exit
	);
	pod2usage(-exitval=>2) if @ARGV==0;
	GetOptions(\%params,
		'cat-add',
		'cat-change|c',
		'delete',
		'download-by-prefix=s',
		'http-status=s',
		'irc' => sub { $params{'rc-monitoring'} = 'irc';},
		'link-replacement|l',
		'minor!',
		'parse=s',
		'rc-monitoring=s',
		'save-as-html=s',
		'search-sbl-blocked-log=s',
		'search-sbl-blocked-log-lang=s',
		'section=s',
		'text-replacement',
		'udest=s',
		'update-editfilter-index',
		'upload',
		'usercontribs|usercontributions',
		'username=s',
		'usource=s',
		'usummary=s',
		'wikipage=s',
		'test|t',
		'silent|quiet|q' => sub { $params{'verbose'} = 0;},
		'very-verbose' => sub { $params{'verbose'} = 2;},
		'verbose|v:+',
		# auto_version will not auto make use of 'V'
		'version|V' => sub { Getopt::Long::VersionMessage();},
		# auto_help will not auto make use of 'h'
		'help|?|h' => sub { Getopt::Long::HelpMessage(
				-verbose  => 99,
				-sections => 'NAME|SYNOPSIS|EXAMPLES');},
		'man' => sub { pod2usage(-exitval=>0, -verbose=>2);},
	) or pod2usage(-exitval=>2);
	$params{'verbose'} = 1 unless exists $params{'verbose'};
	my @additional_params = (0,0); # number of additional params (min, max);
	if(@ARGV < $additional_params[0] or 
		($additional_params[1] != -1 and @ARGV>$additional_params[1])){
		if($additional_params[0]==$additional_params[1]){
			print "error: number of arguments must be exactly $additional_params[0]," . 
				" but is ".(0+@ARGV).".\n";
		}else{
			print "error: number of arguments must be at least $additional_params[0]" . 
				" and at most " . 
				($additional_params[1] == -1 ? 'inf' : $additional_params[1]) . 
				", but is ".(0+@ARGV).".\n";
		}
		print "please use -h for help\n";
		exit 2;
	}
	return \%params;
}

{
	package CamelBot;
	use Data::Dumper;           # for debugging purposes
	use DBI;                    # db connection
	use File::Slurp qw(write_file); # read/write files
	use IPC::Run;               # used for tidy
	use LWP::UserAgent;         # fast, small web browser
	use MediaWiki::Bot;         # bot for mediawiki, uses mediawiki-api-interface
	use POSIX qw/strftime/;     # format timestamp
	use Term::ReadKey;          # used for user input
	use Text::Diff;
	use Time::Local;            # timegm
	use Tk::DiffText;           # graphical diff tool
	use URI::Escape;            # uri_unescape
	use WWW::Mechanize;         # big web browser

	sub new{
		my $class = shift;
		my $params = shift;
		my $current_year = strftime("%Y", gmtime());
		my $self = bless {
			# 0 = don't ask user; 1  = ask user for every action
			'ask_user'       => $params->{'ask_user'} // 0, 
			# maximum number of edits per minute (-1 = inf)
			'max_edits_per_min' => $params->{'max_edits_per_min'} // 5, 
			'minor'          => $params->{'minor'} // 1, # 0 = non-minor; 1 = minor
			'mw_api'         => undef, # instance of MediaWiki::API
			'mw_apiurl'      => undef, # url of mw_api
			'mw_bot'         => undef, # instance of MediaWiki::Bot
			#'mw_password'    => $mw_password, # not needed permanently
			'mw_username'    => $params->{'mw_username'},
			're_editfilter'  => qr/^Wikipedia:Bearbeitungsfilter\/
					(?:\d+|Antr\xe4ge$|Fehlerkennungen$|(?:Regelp|P)r\xfcfung$)/x,
			're_url_class'   => qr/[^\]\[<>"'\x00-\x20\x7F)]|\)(?!\s)/,
			're_url_rear'    => qr/ # no trailing points
				(?:[^\]\[<>"'\x00-\x20\x7F)]|\)(?!\s))*
				(?:[^\]\[<>"'\x00-\x20\x7F).]|\)(?!\s))/x,
			're_maintenance' => {
				'ASIN'      => qr/\bASIN[: ]+(?:B[0-9]{2}[A-Z0-9]{7}|[0-9]{9}[0-9X])/,
				'cat dead'  => qr/\[\[\s*(?i:category|kategorie):\s*[Gg]estorben[\s|_]$current_year\s*(?:\|[^\]]*)?\]\]/,
			},
			# 0 = don't show diff; 1 = show a diff, 2 = gui
			'showdiff'       => $params->{'showdiff'} // 0, 
			# 0 = do real edits; 1 = simulate only
			'simulation'     => $params->{'simulation'} // 0, 
			'time_stack'     => [], # needed for edit/min constraint
			# user answer initially is "no"
			'user_answer'    => $params->{'user_answer'} // '', 
			'verbosity'      => $params->{'verbosity'} // 1,
			'cliparams'      => $params->{'cliparams'},
		}, $class;
		$self->login($params->{'mw_password'}); # may change mw_username!
		return $self;
	}

	sub api_cont{
		my $self       = shift;
		my $query      = shift;
		my $mw_options = shift;
		my $finished = 0;
		my $pages = $self->{'mw_api'}->api($query, $mw_options) 
			or die $self->{'mw_api'}->{'error'}->{'code'}.': ' . 
				$self->{'mw_api'}->{'error'}->{'details'};
		if(defined $pages->{'continue'}){
			while(my ($k, $v) = each(%{$pages->{'continue'}})){
				$query->{$k} = $v;
			}
		}else{
			$finished = 1;
		}
		#if(defined $pages->{'batchcomplete'}){
		#	print Dumper $pages->{'batchcomplete'};
		#}
		return ($pages, $finished);
	}

	sub cat_add{
		my $self  = shift;
		my $pages = shift;
		my $cat   = shift;
		utf8::decode($cat);
		my $summary = '+cat';
		my $cat_first = lc substr($cat, 0, 1);
		my $cat_tail = substr($cat, 1);
		$self->msg(1, "start cat adding...");
		for my $page (@$pages){
			$self->msg(1, "page = $page");
			my $page_utf8 = $page;
			utf8::decode($page_utf8);
			my $text = $self->{'mw_bot'}->get_text($page_utf8);
			next unless $self->is_allowed(\$text, $page);
			# page is already in cat
			next if $text =~ /\[\[[Cc]ategory:(?i:\Q$cat_first\E)\Q$cat_tail\E\]\]/;
			$self->msg(1, "add page to cat.");
			# add page to cat
			my $text_bak = $text;
			if($text =~ /\[\[[cC]ategory:/){
				$text =~ s/.*\K(?=\[\[[cC]ategory:)/[[category:$cat]]\n/g;
			}else{
				$text =~ s/\s*$/\n\n[[category:$cat]]/g;
			}
			$self->time_management();
			$self->save_wiki_page($page_utf8, $summary, \$text, \$text_bak);
		}
		return 1;
	}

	sub cat_rename{
		my $self = shift;
		my $old_cat = shift;
		my $new_cat = shift;
		utf8::decode($old_cat);
		utf8::decode($new_cat);
		my $summary = 'cat change';
		my $old_cat_first = lc substr($old_cat, 0, 1);
		my $old_cat_tail = substr($old_cat, 1);
		$self->msg(1, "start cat renaming...");
		# get pages in (old) category
		my @pages = $self->{'mw_bot'}->get_pages_in_category("Category:$old_cat");
		$self->msg(1, 'found '.@pages." pages:");
		# for all pages in category change category
		for my $page (@pages){
			$self->msg(1, "page = $page");
			my $text = $self->{'mw_bot'}->get_text($page);
			next unless $self->is_allowed(\$text, $page);
			my $text_bak = $text;
			$text =~ s/
				\[\[[cC]ategory:((?i:\Q$old_cat_first\E)\Q$old_cat_tail\E)\]\]
				/[[category:$new_cat]]/gx;
			if($text ne $text_bak){
				$self->msg(1, "changing cat: $1 -> $new_cat");
				$self->time_management();
				$self->save_wiki_page($page, $summary, \$text, \$text_bak);
			}
		}
	}

	sub check_external_link{
		my $self      = shift;
		my $pretext   = shift;
		my $urlstring = shift;
		my $ok = 1;
		if($$urlstring =~ /^https?:\/\//){
			my $url = $$urlstring; #for vim: { {
			if($$pretext =~ /url\s*=\s*$/ and $$urlstring =~ /^([^|}]+)[|}]/){
				$url = $1;
			}
			my $response_code = $self->get_http_status($url);
			$self->msg(1, "  status of '$url': $response_code");
			$ok = 0 if $response_code != 200;
		}
		return $ok;
	}

	sub cleanup_wiki_page{
		my $self     = shift;
		my $text     = shift; # ref to string
		my $page     = shift;
		my $changes = {};
		# == link fixes ==
		# === double protocol ===
		$changes->{'double protocol'} = $self->test_and_replace($text, 
			qr/(?:(?:http:|(?<=\[))\/\/)+(https?):?\/\/($self->{'re_url_rear'})/m, 
			'"$1:\/\/$2"');
		# === missing brackets ===
		$changes->{'missing bracket'} = $self->test_and_replace($text, 
			qr/(<ref>)\s*\[\s*(https?:\/\/$self->{'re_url_rear'})[\s.]*+(<\/ref>)/m, 
			'"$1\x5b$2]$3"');
		# === specific domains ===
		# see [https://de.wikipedia.org/w/index.php?title=MediaWiki_Diskussion:
		# Spam-blacklist&oldid=136131974#
		# www.denkmalschutz.de_.22.26cHash.3D.5B0-9a-f.5D.2B.22_entfernen]
		$changes->{'lf denkmalschutz.de'} = $self->test_and_replace($text, 
			qr/http:\/\/www.denkmalschutz.de\/$self->{'re_url_class'}+?\K
			&cHash=[0-9a-f]+/mx, 
			'""',);
		# === unicode character 0x200e before categories ===
		$changes->{'u200e'} = $self->test_and_replace($text, 
			qr/\x{200e}(?=\]\])/m, 
			'""');
		# == optional fixes==
		if(scalar(grep {$_>0} values %$changes) >= 0){
			# === technically superfluous external links ===
			if($page ne 'Hypertext Transfer Protocol Secure' and 
				 $page ne 'Hypertext_Transfer_Protocol_Secure'){
				$changes->{'lf pseudo-external links'} = $self->test_and_replace($text, 
					qr/(?<!<ref>)\[{1,2}
						(https?:\/\/de\.wikipedia\.org\/wiki\/[^\]|\x20\x23?]++
						(?:\x23(?!mediaviewer)[^\]|\x20?]+)?)\s
						([^\]]+)
					\]{1,2}/mx, 
					'"[[".$self->url2title($1).($self->url2title($1) eq $2 ? "" : "|$2")."]]"');
			}
			# === redundant internal link description ===
			$changes->{'redundant link description'} = $self->test_and_replace($text, 
				qr/\[\[\s*+([^|\]\[]+\S)\s*\|\s*+\1\s*+\]\]/m, 
				'"[[$1]]"');
		}
		# === tests ===
		#$self->test_and_replace($text, 
		#		qr/seth sagt: hallo camelbot!/m, 
		#		'"camelbot sagt: hallo seth!"');
		return $changes;
	}

	sub convert_ns{
		my $self = shift;
		my $ns = shift;
		my $converted;
		unless(defined $self->{'namespaces'}){
			# get namespaces
			$self->{'namespaces'} = {$self->{'mw_bot'}->get_namespace_names()};
			my $res = $self->{'mw_api'}->api({
				'action' => 'query',
				'meta'   => 'siteinfo',
				'siprop' => 'namespacealiases',
			}) or die $self->{'mw_api'}->{'error'}->{'code'} . ': ' . 
				$self->{'mw_api'}->{'error'}->{'details'};
			$self->{'namespacealiases'} = {
				map {
					($_->{'*'} => $_->{'id'});
				} @{$res->{'query'}->{'namespacealiases'}}
			};
		}
		if($ns =~ /^[0-9]+$/){
			$converted = $self->{'namespaces'}->{$ns};
		}else{
			my %rev_ns = reverse(%{$self->{'namespaces'}});
			$converted = $rev_ns{$ns} // $self->{'namespacealiases'}->{$ns};
		}
		$converted = 0 unless defined $converted;
		#$self->msg(1, "could not resolve '$ns'", 'error') unless defined $converted;
		return $converted;
	}

	sub _db_init{
		my $self    = shift;
		my $db_file = shift // $ENV{'HOME'}.'/replica.my.cnf';
		my $lang    = shift // 'de';
		# db connection parameters
		my $db = {
			'host' => $lang.'wiki.labsdb',
			'name' => $lang.'wiki_p',
		};
		my $opened = open(my $INFILE, '<', $db_file);
		unless($opened){
			$self->msg(0, "could not read db conn file: $!", 'error');
			return 0;
		}
		while(my $line = <$INFILE>){
			$db->{'user'} = $1 if $line =~ /^\s*user='(.*)'/;
			$db->{'pw'} = $1 if $line =~ /^\s*password='(.*)'/;
		}
		close($INFILE);
		my $dbh = DBI->connect( # db connect
			'DBI:mysql:'.$db->{'name'}.':'.$db->{'host'}.';mysql_client_found_rows=0',
			$db->{'user'},
			$db->{'pw'},
			{	'RaiseError' => 1,
				'AutoCommit' => 1,
				'mysql_enable_utf8' => 1
			}
		) or $self->msg(0, 'cannot connect to db: '.$DBI::errstr);
		return $dbh;
	}

	sub _db_query{
		my $self  = shift;
		my $dbh   = shift;
		my $query = shift;
		my $return_header = shift // 1;
		my $sth = $dbh->prepare($query) 
			or $self->msg(0, $dbh->errstr.' query = \''.$query.'\'');
		return 0 unless defined $sth;
		$sth->execute;
		my $names = $sth->{'NAME'}; # or NAME_lc if needed
		my $rows_ref = $sth->fetchall_arrayref();
		#$sth->finish;
		unshift @$rows_ref, $names if $return_header;
		return $rows_ref;
	}

	sub _db_simplequery{
		my $self          = shift;
		my $db_query      = shift;
		my $return_header = shift // 1;
		my $lang          = shift // 'de';
		$self->msg(3, "lang = $lang");
		my $dbh = $self->_db_init(undef, $lang) or return undef;
		#print Dumper $db_query;
		if(ref(\$db_query) ne 'SCALAR'){
			#$db_query->[0] =~ s/\s+/ /g;
			$db_query->[0] =~ s/^\s+|\s+\z//g;
			$db_query = sprintf $db_query->[0], map {
				/^-?(?:\d*\.\d+|\d+\.?)\z/ ? $_ :
				$dbh->quote($_); #(not defined $_) ? 'NULL'  (will be done by quote, so ensure, using '%s' even on numbers)
			} @$db_query[1..@$db_query-1];
		}
		$self->msg(2, $db_query);
		my $db_table = $self->_db_query($dbh, $db_query, $return_header);
		$dbh->disconnect;
		return $db_table;
	}

	#sub db_fetch_recentchanges_detailed{
	#	my $self = shift;
	#	my $last_seconds = shift // 60*60*12; # default = last 12 hours
	#	my $timestamp = strftime("%Y%m%d%H%M%S", gmtime($time()-$last_seconds));
	#	# check whether token exists:
	#	my $db_table = db_simplequery([
	#			'SELECT `rc_timestamp`, `rc_user_text`, `rc_namespace`, `rc_title`, `rc_comment`, `rc_minor`, `rc_bot`, `rc_new`, `rc_cur_id`, `rc_this_oldid`, `rc_last_oldid`, `rc_type`, `rc_source`, `rc_patrolled`, `rc_old_len`, `rc_new_len`, `rc_deleted` FROM `recentchanges` WHERE `rc_timestamp`>%s', $timestamp], 
	#		!$_return_header);
	#	if(defined $db_table->[0]){
	#		my $email = lc $db_table->[0][0];
	#		my $user_id = $db_table->[0][1];
	#	}
	#}

	sub db_fetch_recentchanges{
		my $self = shift;
		my $timestamp_begin = shift; # format yyyymmddhhmmss
		my $timestamp_end   = shift; # format yyyymmddhhmmss
		unless(defined $timestamp_begin){
			# default = last 12 hours
			$timestamp_begin = strftime("%Y%m%d%H%M%S", gmtime(time()-60*60*12));
		}
		my $return_header = 1;
		my $db_table;
		my $rows = [
			'rc_timestamp', 
			'rc_user_text', 
			'rc_namespace', 
			'rc_title', 
			'rc_comment', 
			'rc_this_oldid', 
			'rc_last_oldid', 
			'rc_old_len', 
			'rc_new_len',
		];
		$rows = join ', ', map {$_ = '`'.$_.'`'} @$rows;
		if(defined $timestamp_end){
			$db_table = $self->_db_simplequery(['
					SELECT '.$rows.'
					FROM `recentchanges`
					WHERE `rc_timestamp`>=%s
						AND `rc_timestamp`<=%s
						AND `rc_namespace`=0
						AND `rc_log_type` IS NULL
					ORDER BY `rc_timestamp`
			', $timestamp_begin, $timestamp_end], !$return_header);
		}else{
			$db_table = $self->_db_simplequery(['
					SELECT '.$rows.'
					FROM `recentchanges`
					WHERE `rc_timestamp`>=%s
						AND `rc_namespace`=0
						AND `rc_log_type` IS NULL
					ORDER BY `rc_timestamp`
			', $timestamp_begin], !$return_header);
		}
		return $db_table;
	}

	sub db_fetch_sbl_log{
		my $self = shift;
		my $lang = shift // 'de';
		my $url_re = shift // '.';
		# mariadb does not know perl's /(?^)/ pattern
		$url_re =~ s/\(\?\K\^//g;
		my $return_header = 1;
		my $db_table;
		my $rows = [
			'log_timestamp',
			'log_namespace',
			'log_title',
			'log_comment',
			'log_params',
			'log_user_text',
		];
		$rows = join ', ', map {$_ = '`'.$_.'`'} @$rows;
		$db_table = $self->_db_simplequery(['
				SELECT '.$rows.'
				FROM `logging`
				WHERE `log_type` = \'spamblacklist\'
					AND log_params regexp %s
				ORDER BY `log_timestamp`
		', $url_re], !$return_header, $lang);
		return $db_table;
	}

	sub delete_marked_pages{
		my $self = shift;
		my $summary = 'deleting marked page';
		# wiki dependent
		my $delete_cat = 'Category:Löschen';
		utf8::decode($delete_cat);
		my @pages = grep {!/Template:L.schen/}
			$self->{'mw_bot'}->get_pages_in_category($delete_cat);
		$self->msg(1, 'found '.(@pages+0).' pages.');
		$self->msg(2, 'adding manually set pages');
		push @pages, ();
		for my $page(@pages){
			if($self->{'verbosity'} >= 3 or $self->{'simulation'}){
				$self->msg(1, " $page");
			}
			$self->time_management();
			$self->delete_wiki_page($page, $summary);
		}
	}

	sub delete_wiki_page{
		my $self        = shift;
		my $page        = shift;
		my $summary     = shift;
		if((not defined $self->{'simulation'}) or !$self->{'simulation'}){
			my $user_input;
			if(defined $self->{'ask_user'} and $self->{'ask_user'}){
				print "execute? ('y' = yes, else = no) ";
				chomp($user_input = <STDIN>);
			}
			if((not defined $self->{'ask_user'}) or !$self->{'ask_user'} 
					or $user_input=~/y(?:es)?/ 
					or ($user_input eq '' and $self->{'user_answer'} eq 'y')){
				$self->{'user_answer'} = 'y';
				$self->msg(1, "  deleting page [[$page]] ...");
				push @{$self->{'time_stack'}}, time;
				$self->{'mw_api'}->edit({
					'action' => 'delete',
					'minor'  => $self->{'minor'},
					'title'  => $page,
					'reason' => "Bot: $summary"
				}) or die $self->{'mw_api'}->{'error'}->{'code'} . ': ' . 
				$self->{'mw_api'}->{'error'}->{'details'};
			}else{
				$self->{'user_answer'} = '';
			}
		}
	}

	sub download_css{
		my $self   = shift;
		my $cssurl = shift;
		$cssurl =~s/&amp;/&/g;
		my $css_dir = shift;
		my $images_dir = shift; # todo: save url{...}
		my $filename = $cssurl;
		$filename =~s/[^a-zA-Z0-9~_-]/_/g;
		$filename = "$css_dir/$filename.css";
		my $mech = WWW::Mechanize->new(
			'keep_alive' => 1, 'autocheck' => 0, 'timeout' => 10);
		$mech->agent_alias('Linux Mozilla');
		$mech->get($cssurl);
		if($mech->status() != 200){
			$self->msg(0, 'download of css failed. status: '.$mech->status(), 'error');
		}else{
			my $css_content = $mech->content();
			write_file($filename, $css_content);
		}
		return $filename;
	}

	sub download_files{
		my $self          = shift;
		my $files         = shift;
		my $no_warn_files = shift;
		my $files_prefix  = shift // '';
		for my $file(@$files){
			$self->msg(2, "getting '$file' from wiki");
			my $img_data = $self->{'mw_bot'}->get_image("File:$file");
			unless(defined $img_data){
				$self->msg(0, "could not read file '$file' from wiki", 'error');
				next;
			}
			$file = $self->title2filename($file);
			if(-e $file){
				unless(grep {$_ eq $file} @$no_warn_files){
					$self->msg(0, "file '$file' exists already", 'warning');
					push @$no_warn_files, $file;
				}
				next;
			};
			push @$no_warn_files, $file;
			$self->msg(2, "writing file '$file'");
			write_file($files_prefix.$file, {binmode => ':raw'}, \$img_data);
		}
		return 1;
	}

	sub download_pages_by_prefix{
		my $self    = shift;
		my $prefix  = shift;
		my @prefix_pages = grep {!$_->{'redirect'} and $_ = $_->{'title'}} 
			$self->{'mw_bot'}->prefixindex($prefix);
		die "error: no pages found with prefix '$prefix}'.\n" if @prefix_pages==0;
		my $page_texts = $self->{'mw_bot'}->get_pages(\@prefix_pages);
		if($self->{'verbosity'} >= 1){
			map {$self->msg(1, "getting '$_'")} sort keys %$page_texts;
		}
		my $no_warn_files = [];
		my $files_prefix = '';
		while(my ($title, $text) = each %$page_texts){
			$title = $self->title2filename($title).'.wikitext';
			$self->msg(2, "saving '$title'");
			die "error: file '$title' exists already\n" if -e $title;
			write_file($title, {binmode => ':utf8'}, $text);
			my @images = $text=~/
				\[\[
					(?:[fF]ile|[iI]mage|[mM]edia):
					(
						(?:
							(?!\]\])
							[^|]
						)*
					)/xg;
			$self->download_files(\@images, $no_warn_files, $files_prefix);
		}
		return 1;
	}

	sub get_abuse_filter_info{
		my $self   = shift;
		# api.php?action=query&list=abusefilters&abfprop=id|description
		# &abfstartid=1&abflimit=500
		# could be extended to 
		# api.php?action=query&list=abusefilters&abfprop=
		# id|description|pattern|actions|hits|comments|lasteditor|lastedittime|status|
		# private&abflimit=500
		my $query = {
			'action'         => 'query',
			'continue'       => '',
			'list'           => 'abusefilters', 
			'abflimit'       => '500',
			'abfprop'        => 'id|description',
			'abfstartid'     => '1',
		};
		my $mw_options = {'max' => 1};
		my $finished = 0;
		my @results;
		while(!$finished){
			my ($pages, $finished) = $self->api_cont($query, $mw_options);
			map {$results[$_->{'id'}] = $_->{'description'}} @{$pages->{'query'}->{'abusefilters'}};
		}
		return \@results;
	}

	sub get_http_status{
		my $self = shift;
		my $url = shift;
		my $lwp = LWP::UserAgent->new(
			'keep_alive' => 1, 
			'timeout' => 10, 
			'agent' => 'Mozilla/5.0'
		);
		my $response = $lwp->head($url);
		return $response->code;
	}

	sub get_namespace_id{
		my $self = shift;
		my $page = shift;
		my $ns_id = 0;
		if($page =~ /^([^:]+):/){
			$ns_id = $self->convert_ns($1);
		}
		return $ns_id;
	}

	sub get_page_info{
		my $self   = shift;
		my $titles = shift;
		$titles = join '|', @$titles if ref $titles eq 'ARRAY';
		# api.php?action=query&prop=info&titles=Wikipedia:Bearbeitungsfilter/latest%20topics&continue=
		my $query = {
			'action'         => 'query',
			'continue'       => '',
			'prop'           => 'info', 
			'titles'         => $titles, 
		};
		my $mw_options = {'max' => 1};
		my $pages = $self->{'mw_api'}->api($query, $mw_options) 
			or die $self->{'mw_api'}->{'error'}->{'code'}.': ' . 
				$self->{'mw_api'}->{'error'}->{'details'};
		my $page_infos;
		while(my ($page_id, $page_info) = each %{$pages->{'query'}->{'pages'}}){
			$page_infos->{$page_info->{'title'}} = $page_info;
		}
		return $page_infos;
	}

	sub get_pages_by_prefix{
		my $self   = shift;
		my $params = shift;
		my $query = {
			'action'         => 'query',
			'continue'       => '',
			'generator'      => 'allpages', 
			'gapnamespace'   => $params->{'namespace_id'}, 
			'gapprefix'      => $params->{'prefix'},
			'gapfilterredir' => 'nonredirects',
			'gaplimit'       => '500',
			'prop'           => 'info|revisions',
			'rvprop'         => 'content|timestamp',
		};
		my $mw_options = {'max' => 1};
		my $finished = 0;
		my @results;
		my $rvcont_old;
		while(!$finished){
			my $rvcont;
			my $pages = $self->{'mw_api'}->api($query, $mw_options) 
				or die $self->{'mw_api'}->{'error'}->{'code'}.': ' . 
					$self->{'mw_api'}->{'error'}->{'details'};
			#print Dumper [keys $pages];
			if(defined $pages->{'continue'}){
				#print Dumper $pages->{'continue'};
				while(my ($k, $v) = each(%{$pages->{'continue'}})){
					$query->{$k} = $v;
				}
				if(defined $pages->{'continue'}->{'rvcontinue'}){
					$rvcont = $pages->{'continue'}->{'rvcontinue'};
					$rvcont =~s/\|.*//;
				}
			}else{
				$finished = 1;
			}
			push @results, grep {
				(not defined $params->{'title_re'} 
					or $_->{'title'} =~ /$params->{'title_re'}/)
				and (not defined $rvcont or $_->{'pageid'} < $rvcont)
				and (not defined $rvcont_old or $_->{'pageid'} >= $rvcont_old)
			} values %{$pages->{'query'}->{'pages'}};
			$rvcont_old = $rvcont;
			#if(defined $pages->{'batchcomplete'}){
			#	print Dumper $pages->{'batchcomplete'};
			#}
		}
		return \@results;
	}

	sub get_tables_from_wikitext{
		my $self = shift;
		my $wikitext = shift;
		return [grep /^\{\|/, split /^(?=\{\|)|^\|\}\K$/m, $$wikitext];
	}

	sub get_user_contribs{
		my $self    = shift;
		my $options = shift;
		my $ucstart = $options->{'ucstart'};
		my $ucend   = $options->{'ucend'};
		my $uclimit = $options->{'uclimit'};
		my $user    = $options->{'username'} // $options->{'usernameprefix'};
		$self->msg(1, "searching for contributions of user: $user");
		#my $ref = $self->{'mw_bot'}->contributions($options->{'username'}, 0, undef);
		$self->{'mw_api'}->{config}->{api_url} = $self->{'mw_apiurl'};
		my $query = {
			action      => 'query',
			list        => 'usercontribs', 
			ucprop      => 'ids|title',
			ucstart     => $ucstart,
			'uclimit'   => defined $uclimit ? $uclimit : 100,
		};
		if(exists $options->{'username'}){
			$query->{'ucuser'}  = $user;
		}else{
			$query->{'ucuserprefix'}  = $user;
		}
		if(defined $options->{'namespace'}){
			$query->{'ucnamespace'}  = $options->{'namespace'};
		}
		$query->{'ucend'}   = $ucend if defined $ucend;
		my $mw_options = {'max' => 1};
		my %list_of_articles;
		#while((keys %list_of_articles)<$uclimit and 
		# (defined $ucend and $ucstart gt $ucend or not defined $ucend)){
			#if(defined $ucstart and defined $ucend){
			#	$self->msg(1, "ucstart = $ucstart, ucend = $ucend") 
			#}
			my $tmp = $self->{'mw_api'}->api($query, $mw_options);
			$ucstart = $tmp->{'query-continue'}->{'usercontribs'}->{'ucstart'};
			$query->{'ucstart'} = $ucstart;
			my $usercontribs = $tmp->{'query'}->{'usercontribs'};
			if(defined $usercontribs){
				for my $uc(@$usercontribs){
					next if exists $list_of_articles{$uc->{'title'}};
					$list_of_articles{$uc->{'title'}} = 1;
				}
			}
		#}
		return \%list_of_articles;
	}

	sub is_allowed{
		my $self = shift;
		my $textref = shift;
		my $page = shift // 'unknown';
		my $allowed = 1; # for vim: { { { { { { { {
		if($$textref =~ /{{[nN]obots}}|
			[bB]ots\s*+\|\s*+(?:
				deny\s*+=\s*+(?:
					all\s*+}}|
					(?<deny>.*?)}}
				)|
				allow\s*+=\s*+(?:
					none\s*+}}|
					(?<allow>.*?)}}
				)
			)/x){
			if(defined $+{'allow'}){
				$allowed = 0 if $+{'allow'} !~/(?:^|,\s*)$self->{'mw_username'}(?:\s*,|$)/;
			}elsif(defined $+{'deny'}){
				$allowed = 0 if $+{'deny'} =~/(?:^|,\s*)$self->{'mw_username'}(?:\s*,|$)/;
			}else{
				$allowed = 0;
			}
		}
		$self->msg(1, "bot is not allowed on page '$page'.") if $allowed==0;
		return $allowed;
	}

	sub link_replacement{
		my $self = shift;
		my $params = shift;
		# regexp pattern of protocol, e.g., /http:\/\//
		my $re_prot_part          = $params->{'re_prot_part'};
		# regexp pattern of searched url (without protocol), 
		# e.g., /(?:www\.)?example\.com/
		my $re_url_part           = $params->{'re_url_part'};
		# replacement function
		my $replacement           = $params->{'replacement'};
		my %options;
		# 1 = delete links
		$options{'delete_link'}   = $params->{'delete_link'};
		# 1 = replace links in refs by 'dead link' template.
		$options{'ref2deadlink'}  = $params->{'ref2deadlink'};
		# link to use in api link search
		$options{'searched_link'} = $params->{'searched_link'};
		# if user contributions shall be searched...
		$options{'user_contribs'} = $params->{'user_contribs'};
		# wiki summary of page edit
		$options{'summary'}       = $params->{'summary'};
		# 0 = don't append on ns 0, 1 = append on ns 0
		$options{'articles'}      = $params->{'articles'}    //  0;
		# 0 = don't append on ns!=0, 1 = append on ns!=0
		$options{'nonarticles'}   = $params->{'nonarticles'} //  0;
		# 0 = don't touch refs; 1 = replace refs
		$options{'refs'}          = $params->{'refs'}        //  1;
		# 0 = don't touch non-refs; 1 = replace non-refs
		$options{'nonrefs'}       = $params->{'nonrefs'}     //  1;
		# maximum number of edits (-1 = inf)
		$options{'max_edits'}     = $params->{'max_edits'}   // -1;
		# skip first n pages.
		$options{'skip_edits'}    = $params->{'skip_edits'}  //  0;
		# print results (1) or don't (0)
		$options{'results'}       = $params->{'results'}     //  0;
		my $edit_counter = 0;          # edit counter
		my $old_page_0 = {};           # list of edited pages in ns==0
		my $old_page_1 = {};           # list of edited pages in ns!=0
		# some regexp vars should make source more readable 
		my $re_url = $re_prot_part.$re_url_part.$self->{'re_url_class'};
		my $re_descr_class = qr/[^\]\x00-\x08\x0a-\x1F]/;
		my $re_ref_tag_name = qr/<ref [^>]*name\s*=\s*['"]?([^>]*?)['"]?>/;
		my $re_ref_inner_part = qr/(?s:(?!<ref)(?!<\/ref>).)+?$re_url_part.*?<\/ref>/;
		my $re_deadlink = qr/\Q{{dead link|date=March 2011}}\E/;
		my %results = (
			'num_ns0_links'  => 0,
			'num_ns!0_links' => 0,
		);

		if($options{'articles'}){
			$self->msg(1, "starting linksearch in ns=0...");
			my @links;
			if(defined $options{'searched_link'}){
				@links = $self->{'mw_bot'}->linksearch($options{'searched_link'}, 0, undef);
			}elsif(defined $options{'user_contribs'}){
				$self->msg(1, 'searching for contributions of user:' . 
					$options{'user_contribs'}->{'user'});
				#my $ref = $self->{'mw_bot'}->contributions(
				#	$options{'user_contribs'}->{'user'}, 0, undef);
				#$options{'ucstart'}  = '2011-03-18T00:00:00Z';
				#$options{'ucend'}    = '2011-03-16T00:00:00Z';
				#$options{'uclimit'}  = '10';
				#$options{'username'} = $options{'user_contribs'}->{'user'};
				#$options{'namespace'} = 0;
				#$self->get_user_contribs($options);
				#my $mwapi = MediaWiki::API->new();
				#$mw->{config}->{api_url} = 'https://de.wikipedia.org/w/api.php';
				my $ucstart = '2011-03-18T00:00:00Z';
				my $ucend = '2011-03-16T00:00:00Z';
				my $query = {
					action      => 'query',
					list        => 'usercontribs', 
					ucuser      => $options{'user_contribs'}->{'user'},
					ucnamespace => 0,
					ucprop      => 'ids|title',
					ucstart     => $ucstart,
					ucend       => $ucend,
					uclimit     => '10',
				};
				my $mw_options = {'max' => 1};
				while($ucstart gt $ucend){
					$self->msg(1, "ucstart = $ucstart, ucend = $ucend");
					my $tmp = $self->{'mw_api'}->api($query, $mw_options);
					$ucstart = $tmp->{'query-continue'}->{'usercontribs'}->{'ucstart'};
					$query->{'ucstart'} = $ucstart;
					my $usercontribs = $tmp->{'query'}->{'usercontribs'};
					if(defined $usercontribs){
						for(@$usercontribs){
							next if exists $old_page_0->{$_->{'title'}};
							$old_page_0->{$_->{'title'}} = 1;
							# $self->msg(1, $_->{'title'});
							my $diff = $self->{'mw_bot'}->diff({
								'revid' => $_->{'revid'},
								'oldid' => 'prev',
							});
							# my $found = 0;
							while($diff=~/<td class=\"diff-deletedline\">(.*?)<\/td>/g){
								if($1=~/($re_url*)/){
									push @links, {
										'title' => $_->{'title'},
										'url' => $1,
									};
									# $found = 1;
									last;
								}
							}
							# print Dumper($diff) if $found;
						}
					}
				}
			}
			$self->msg(1, 'results (found '.@links.' links in '.
				$self->num_unique_elem(map {$_->{'title'}} @links)." pages):");
			$results{'num_ns0_links'} = 0+@links;
			#push @links, {'url' => 'scheissegal', 'title' => 'LyX'};
			for my $hash (@links){
				my $page = $hash->{'title'};
				next if exists $old_page_0->{$page};
				$old_page_0->{$page} = 1;
				if(++$edit_counter < $options{'skip_edits'} or $options{'max_edits'} == 0){
					next;
				}elsif($edit_counter > $options{'max_edits'} 
						and $options{'max_edits'} != -1){
					last;
				}
				#next unless $hash->{'url'}=~/$re_prot_part$re_url_part/;
				$self->msg(1, "$edit_counter: page= $page, url= ".$hash->{'url'});
				my $text = $self->{'mw_bot'}->get_text($page);
				next unless $self->is_allowed(\$text, $page);
				my $text_bak = $text;
				$self->cleanup_wiki_page(\$text, $page);
				if(defined $replacement){
					#	$self->test_and_replace(
					#		\$text, 
					#		qr/$re_prot_part$re_url_part(?<trail>$self->{'re_url_class'}*)(?:(?<posturl>.*?)$re_deadlink)?/m, 
					#		$replacement, 
					#	);
				}elsif($options{'delete_link'}){
					if($options{'refs'}){
						if($options{'ref2deadlink'}){
							if($text!~/url\s*=\s*$re_url/){ # special templates
								$self->test_and_replace(\$text, 
									qr/
										(
											<(?i:ref)\b[^>]*>\s*          # <ref>
											(?s:(?!<ref)(?!<\/ref>).)*?   # blabla
										)(?|
											\[($re_url*+)(\x20[^\]]+|)\]| # [url] or 
											($re_url*+)()                # url (not preceeded by "url = ")
										)(
											.*?                           # blabla
											<\/ref>                       # <\/ref>
										)/x,
									'"${1}[{{dead link|inline=yes|bot=' . $self->{'mw_username'} . 
									'|date=' . strftime("%Y-%m-%d", gmtime()).'|url=$2}}$3]$4"'
								);
							}
						}else{
							# backrefs
							if($text=~/$re_ref_tag_name$re_ref_inner_part/){
								my $ref_name = $1;
								$self->msg(1, $ref_name, 'debug');
								$self->test_and_replace(
									\$text, 
									qr/<(?i:ref) name\s*=\s*['"]?\s*$ref_name\s*['"]?\s*\/\s*>/, 
									'""', 
								);
							}
							# refs
							$self->test_and_replace(
								\$text, 
								qr/<(?i:ref)\b[^>]*>\s*$re_ref_inner_part/, 
								'""', 
							);
						}
					}
					# <references />
					if($text!~/<(?i:ref)(?:>| name)|EinwohnerOrtQuelle|EinwohnerRef/){
						my $text_tmp = $text;
						$self->test_and_replace(
							\$text_tmp, 
							qr/^=(=+)\s*(?:Einzelnachweise|Quellen)\s*\1=[\s\n]*\n
							<references(?:\x20?\/)?>\s*/mx, 
							'""', 
						);
						my $parsed = $self->parse_wikitext($text_tmp);
						if($parsed !~ /class="error mw-ext-cite-error"/){
							$self->test_and_replace(
								\$text, 
								qr/^=(=+)\s*(?:Einzelnachweise|Quellen)\s*\1=[\s\n]*\n
								<references(?:\x20?\/)?>\s*/mx, 
								'""', 
							);
						}else{
							$self->msg(1, '<references /> not deleted, because it would lead ' .
								'to an error.', 'warning');
						}
					}
					if($options{'nonrefs'}){
						# links like [link descr] in lists (as in "external links")
						$self->test_and_replace(
							\$text, 
							qr/^\*(?i:.(?!<ref))*\[$re_url*?\s*$re_descr_class*\].*\n?/m, 
							'""', 
						);
						# plain links in lists (as in "external links")
						$self->test_and_replace(
							\$text, 
							qr/^\*(?i:.(?!<ref))*$re_url*?.*\n?/m, 
							'""', 
						);
					}
					# heading "weblinks" without entries
					my $weblinks_heading_re = qr/^=(=+)\s*Weblinks?\s*=\1[\s\n]*\n
						(\[\[[Kk]atego|\{\{
							(?:Coordinate|DEFAULTSORT|Hinweis\x20|(?:Vorlage:)?Navigations
								|Normdaten|(?:Vorlage:)?Orden,|SORTIERUNG)
							|(?:<!--\x20?)?==)/mx;
					$self->test_and_replace(\$text, $weblinks_heading_re, '"$2"');
					if($text=~/(.*(?<!\|url=$re_prot_part)$re_url_part.*)/){
						$self->msg(1, " not resolved: $1");
					}
				}
				$self->time_management();
				$self->save_wiki_page($page, $options{'summary'}, \$text, \$text_bak);
			}
			$results{'num_ns0_unique_links'} = $edit_counter;
		}

		if($options{'nonarticles'}){
			my $namespaces = {$self->{'mw_bot'}->get_namespace_names()}; # get namespaces
			$namespaces = [grep {$_!=0} keys %$namespaces]; # delete namespace 0 from list
			$self->msg(1, "starting linksearch in ns!=0...");
			my @links;
			if(defined $options{'searched_link'}){
				@links = $self->{'mw_bot'}->linksearch(
					$options{'searched_link'}, $namespaces, undef);
			}elsif(defined $options{'user_contribs'}){
			}
			$self->msg(1, 'results (found '.@links." links in ".
				$self->num_unique_elem(map {$_->{'title'}} @links)." pages):");
			$results{'num_ns!0_links'} = 0 + @links;
			for my $hash (@links){
				my $page = $hash->{'title'};
				next if exists $old_page_1->{$page};
				$old_page_1->{$page} = 1;
				if(++$edit_counter < $options{'skip_edits'} or $options{'max_edits'} == 0){
					next;
				}elsif($edit_counter > $options{'max_edits'} 
						and $options{'max_edits'} != -1){
					last;
				}
				$self->msg(1, "$edit_counter: page= $page, url= ".$hash->{'url'});
				my $text = $self->{'mw_bot'}->get_text($page);
				next unless $self->is_allowed(\$text, $page);
				my $text_bak = $text;

				#$self->cleanup_wiki_page(\$text, $page);
				if(defined $replacement){
					#	$self->test_and_replace(
					#		\$text, 
					#		qr/$re_prot_part$re_url_part(?<trail>$self->{'re_url_class'}*)(?:(?<posturl>.*?)$re_deadlink)?/m, 
					#		$replacement, 
					#	);
				}elsif($options{'delete_link'}){
					if($options{'nonrefs'}){
						# [link]
						$self->test_and_replace(\$text, 
							qr/\[$re_prot_part($re_url_part$self->{'re_url_class'}*?)\]/, '$1');
						# [link descr]
						$self->test_and_replace(\$text, 
							qr/(?<!<nowiki>)(\[$re_url*\s*$re_descr_class*\])/, '"<nowiki>".$1."<\/nowiki>"');
						# plain_link or word:plainlink
						$self->test_and_replace(\$text, 
							qr/(?<!$self->{'re_url_class'})(?<!<nowiki>\[)([a-zA-Z0-9]*:|)$re_prot_part($re_url_part$self->{'re_url_class'}*)/, '$1.$2');
						# ==plain_link==
						$self->test_and_replace(\$text, 
							qr/^(=+) *$re_prot_part($re_url_part$self->{'re_url_class'}*) *\1 */m, '$1." ".$2." ".$1');
						# :*plain_link
						$self->test_and_replace(\$text, 
							qr/^([:\s*]*)$re_prot_part($re_url_part$self->{'re_url_class'}*)/m, '$1.$2');
						# (plain_link)
						$self->test_and_replace(\$text, 
							qr/([()]\s*)$re_prot_part($re_url_part$self->{'re_url_class'}*)(\s*\)|\s)/, '$1.$2.$3');
	#					# {|plain_link|}
	#					$self->test_and_replace(\$text, qr/([|{]\s*)$re_prot_part($re_url_part$self->{'re_url_class'}*)(\s*[|}])/, '$1.$2.$3');
					}
					# not good for large pages:
					if($text=~/^(.*(?<!<nowiki>)(?<!<nowiki>\[)$re_prot_part$re_url_part.*)/m){
						$self->msg(1, " not resolved: $1");
					}
					# better:
	#				while($text=~/(^.*$re_prot_part$re_url_part.*)/gmo){
	#					my $temp = $1;
	#					if($temp=~/(?<!<nowiki>)(?<!<nowiki>\[)$re_prot_part$re_url_part/){
	#						$self->msg(1, " not resolved: $temp");
	#					}
	#				}
				}
				$self->time_management();
				$self->save_wiki_page($page, $options{'summary'}, \$text, \$text_bak);
			}
			$results{'num_ns!0_unique_links'} = $edit_counter-($results{'num_ns0_unique_links'} // 0);
		}
		if($options{'results'}){
			print "\n";
			$self->msg(1, "results:");
			print " found " . ($results{'num_ns0_links'}+$results{'num_ns!0_links'}) .
				' links in ' . (0 + keys(%$old_page_0) + keys(%$old_page_1)) . " pages:\n";
			if($options{'articles'} && $options{'nonarticles'}){
				print "       ".$results{'num_ns0_links'}." links in ".$results{'num_ns0_unique_links'}." pages in ns==0\n";
				print "       ".$results{'num_ns!0_links'}." links in ".$results{'num_ns!0_unique_links'}." pages in ns!=0\n";
			}
			if($options{'articles'}){
				print "\n pages in ns==0:\n";
				print "  $_\n" for sort keys %$old_page_0;
			}
			if($options{'nonarticles'}){
				print "\n pages in ns!=0:\n";
				print "  $_\n" for sort keys %$old_page_1;
			}
		}
	}

	sub login{
		my $self = shift;
		my $wiki_password = shift;
		# if wiki user name is not defined, let user type in wiki user name
		if(not defined $self->{'mw_username'}){
			$self->msg(1, "enter wiki username:");
			$self->{'mw_username'} = <STDIN>;
			chomp($self->{'mw_username'});
		}
		# cope with auto-normalization (bug in MediaWiki::Bot)
		$self->{'mw_username'} = ucfirst($self->{'mw_username'});
		$self->{'mw_username'} =~y/_/ /;
		$self->msg(1, "login user name is '$self->{'mw_username'}'.");
		my $cookie = '.mediawiki-bot-'.$self->{'mw_username'}.'-cookies';
		# if wiki user password is not defined, search typical password places or let 
		# user type in wiki user password
		if(not defined $wiki_password){
			# /bot/ needed for toolserver (for historical reasons)
			my $pwfile = [".password", "$ENV{HOME}/.password", "$ENV{HOME}/bot/.password"];
			for(@$pwfile){
				if(-e $_){
					$self->msg(1, "using standard password file $_");
					open(my $INFILE, '<', $_) or die $!;
						chomp($wiki_password = <$INFILE>);
					close($INFILE);
					last;
				}
			}
			unless(defined $wiki_password or
					(	# $self->{'mw_username'} =~/^(?:CamelBot)$/ and 
						-e $cookie and
						not ($self->{'cliparams'}->{'delete'} or $self->{'cliparams'}->{'upload'})
					)
			){
				if($self->{'verbosity'} >= 1){
					$self->msg(1, 'could not read password file at places: ');
					print '  '.$pwfile->[$_].', ' for 0..($#$pwfile-1);
					print $pwfile->[-1]."\n";
				}
				$self->msg(0, "enter password (will not be echoed):");
				ReadMode 'raw';
				$wiki_password = ReadLine(0);
				chomp($wiki_password);
				ReadMode 'restore';
				while($wiki_password=~/\x{007f}/){
					$wiki_password =~ s/(?:^|[^\x{007f}])\x{007f}//g;
				}
			}
		}
		my %address_params = (
			'protocol' => 'https',
			# wiki dependent
			'host'     => 'de.wikipedia.org',
			'path'     => 'w',
			#'host'     => 'wiki.selfhtml.org',
			#'path'     => 'mediawiki',
		);
		$self->msg(2, "logging in at $address_params{'host'}...");
		$self->{'mw_bot'} = MediaWiki::Bot->new({
			assert      => 'bot',
			protocol    => $address_params{'protocol'},
			host        => $address_params{'host'},
			path        => $address_params{'path'},
			operator    => 'lustiger_seth',
			debug       => 1,
			login_data  => {
				username => $self->{'mw_username'},
				password => $wiki_password,
				#lgdomain => ''
			},
		});
		$self->{'mw_api'} = MediaWiki::API->new();
		my $wiki_base_url = $address_params{'protocol'}.'://'.$address_params{'host'};
		$self->{'mw_apiurl'} = $wiki_base_url.'/'.$address_params{'path'}.'/api.php';
		if(    $self->{'cliparams'}->{'delete'} 
				or $self->{'cliparams'}->{'upload'} 
				or $self->{'cliparams'}->{'link-replacement'}
				or $self->{'cliparams'}->{'search-sbl-blocked-log'}
				or $self->{'cliparams'}->{'search-sbl-blocked-log-lang'}
		){
			$self->{'mw_api'}->{'config'}->{'api_url'} = $self->{'mw_apiurl'};
			$self->{'mw_api'}->{'config'}->{'upload_url'} = $wiki_base_url.'/wiki/Special:Upload';
			$self->{'mw_api'}->{'config'}->{'files_url'} = '';
			$self->{'mw_api'}->login({
				'lgname' => $self->{'mw_username'},
				'lgpassword' => $wiki_password,
				#'lgdomain' => ''
			}) or die $self->{'mw_api'}->{'error'}->{'code'}.': '.$self->{'mw_api'}->{'error'}->{'details'};
		}else{
			unless(defined $self->{'mw_bot'}){
				$self->msg(0, 'maybe there\'s an expired cookie file. please delete \''. 
					$cookie. '\' and try again.', 'error') if -e $cookie;
				die("error: could not login.\n");
			}
		}
		$self->msg(2, "logged in...");
		return 1;
	}

	sub msg{
		my $self           = shift;
		my $verb_threshold = shift;
		my $msg            = shift;
		my $type           = shift;
		return 0 if $self->{'verbosity'} < $verb_threshold;
		$type = (defined $type ? "$type in ": '');
		my $timestamp = strftime("%Y-%m-%d %H:%M:%S", gmtime());
		# my ($package, $filename, $line, $subr, $has_args, $wantarray, $evaltext, 
		# $is_require, $hints, $bitmask, $hinthash) = caller(0);
		my @callers = caller(0);
		my $line = $callers[2];
		@callers = caller(1);
		my $subr = $callers[3] // '[no sub]';
		print "$timestamp $type$subr:$line: $msg\n";
		return 1;
	}

	sub newest_post_info{
		my $self     = shift;
		my $wikitext = shift;
		my $threads = [grep {$_!~/^=+$/} split /\n(?=(=++)[^=].*\1(?:\n|$))/, $wikitext];
		my $newest_thread = {
			'date'   => 0, 
			'author' => '',
		};
		for my $t(@$threads){
			next unless $t=~/^(=++)\s*(.*?)\s*\1/;
			my $thread_title = $2;
			$newest_thread->{'thread'} = $thread_title unless defined $newest_thread->{'thread'};
			my $date;
			while($t=~/(?<pre_sig>.*)(?<hour>\d\d):(?<min>\d\d),\x20
				(?<mday>[1-9]|[123][0-9])\.\x20(?<mon>[a-zA-Zä\xe4]{3,4})\.?\x20
				(?<year>20\d\d)\x20\((?<tz>CES?T)\)/gx){
				my $month = 
					$+{'mon'} eq 'Jan' ? 0 : 
					$+{'mon'} eq 'Feb' ? 1 : 
					($+{'mon'} eq "Mär" or $+{'mon'} eq "M\xe4r") ? 2 : 
					$+{'mon'} eq 'Apr' ? 3 : 
					$+{'mon'} eq 'Mai' ? 4 : 
					$+{'mon'} eq 'Jun' ? 5 : 
					$+{'mon'} eq 'Jul' ? 6 : 
					$+{'mon'} eq 'Aug' ? 7 : 
					$+{'mon'} eq 'Sep' ? 8 : 
					$+{'mon'} eq 'Okt' ? 9 : 
					$+{'mon'} eq 'Nov' ? 10 : 
					$+{'mon'} eq 'Dez' ? 11 : 12
				;
				next if $month == 12;
				$date = timegm(0, $+{'min'}, $+{'hour'}, $+{'mday'}, $month, $+{'year'});
				$date += $+{'tz'} eq 'CEST' ? 2 : 1;
				if($date>$newest_thread->{'date'}){
					$newest_thread = {
						'thread' => $thread_title,
						'author' => $+{'pre_sig'},
						'date'   => $date,
					};
				}
			}
		}
		my $username;
		while($newest_thread->{'author'} =~/\[\[
			(?:
				(?:[Bb]enutzer(?:in)?|[uU]ser)(?:\x20talk|Diskussion)?:| # 'user:...' or 'user talk:...' or
				[Ss]pe[zc]ial:(?:Beitr..?ge|[Cc]ontributions)\/   # 'special:contributions...'
			)
			([^|\/\]]+)
			/gx){
			$username = $1;
		}
		$newest_thread->{'author'} = $username if defined $username;
		return $newest_thread;
	}

	sub notifier{
		my $self        = shift;
		my $page        = shift;
		utf8::decode($page); # is this necessary?
		my @hist = $self->{'mw_bot'}->get_history($page, 2);
		if(@hist==0){
			$self->msg(1, "page '$page' doesn't seem to exist anymore.", 'notice');
			return 0;
		}
		my $text = $self->{'mw_bot'}->get_text($page, $hist[0]->{'revid'}); # newest version
		if(defined $text){
			# ASINs are unwanted in w:de
			my $notify = 0;
			my $num_matches =()= $text =~ /$self->{'re_maintenance'}->{'ASIN'}/g;
			if($num_matches > 0){
				# compare with older version
				if(@hist > 1 and defined $hist[1]->{'revid'}){
					my $text_prev_version = $self->{'mw_bot'}->get_text($page, $hist[1]->{'revid'});
					my $num_matches_pv =()= $text_prev_version =~ /$self->{'re_maintenance'}->{'ASIN'}/g;
					if($num_matches_pv < $num_matches){
						$notify = 1;
					}
				}else{
					$notify = 1;
				}
			}
			if($notify){
				my $user_to_inform = $hist[0]->{'user'};
				my $userpage = 'User talk:'.$user_to_inform;
				$text = $self->{'mw_bot'}->get_text($userpage);
				if($self->is_allowed(\$text, $userpage)){
					my $text_bak = $text;
					my $heading = 'hinweis auf katalog-nummern von amazon';
					if($text =~ /\Q$heading\E/){
						$self->msg(1, "user '$user_to_inform' seems to be informed already.", 
							'notice');
					}else{
						my $notice = $self->{'mw_bot'}->get_text('user:CamelBot/notice-ASIN');
						my $title = $self->title2url_part($page);
						$notice =~ s/\$diff\b/\/\/de.wikipedia.org\/w\/index.php?title=$title&diff=prev&oldid=$hist[0]->{'revid'}/g;
						$notice =~ s/\$article\b/$page/g;
						$notice =~ s/\$signature\b.*/-- ~~~~/sg;
						$text .= "\n== $heading ==\n$notice\n";
						#if($text_bak ne $text){
							$self->time_management();
							$self->msg(2, "notifying on '$userpage'");
							$self->save_wiki_page($userpage, $heading, \$text, \$text_bak);
						#}
					}
				}
			}
		}else{
			$self->msg(1, "could not fetch page '$page'. maybe deleted already", 'notice');
		}
		return 1;
	}

	sub num_unique_elem{
		my $self = shift;
		my %hash;
		@hash{@_} = 1;
		return ''.(keys %hash);
	}

	sub parse_page{
		my $self = shift;
		my $page = shift;
		my $text = $self->{'mw_api'}->api( {
			action => 'parse',
			page => $page,
			prop => 'text|headhtml',
		}) or die $self->{'mw_api'}->{error}->{code} . ': ' . 
				$self->{'mw_api'}->{error}->{details};
		my $title = $text->{'parse'}->{'title'};
		my $html = $text->{'parse'}->{'text'}->{'*'};
		my $headhtml = $text->{'parse'}->{'headhtml'}->{'*'};
		# delete final comments
		$html =~s/\n\n<!--\s+NewPP.*?\n-->$//s;
		return ($html, $title, $headhtml);
	}

	sub parse_irc_rc{
		my $self = shift;
		my $msg = shift;
		my %parsed_msg;
		if($msg=~/^\cC14
			\[\[\cC07([^\]]+)\cC14]]\cC4\x20   # page
			([a-z0-9_-]+|[!NMB]*)\cC10\x20     # flags
			\cC02([^\cC]*)\cC\x20\cC5\*\cC\x20 # url
			\cC03([^\cC]*)\cC\x20\cC5\*\cC\x20 # user
			(\(\cB?[+-]\d+\cB?\)|)\x20         # diffbytes
			\cC10(.*[^\cC]|)\cC?\z/x){         # summary
			$parsed_msg{'page_with_ns'} = $1;
			$parsed_msg{'flags'} = $2;
			$parsed_msg{'url'} = $3;
			$parsed_msg{'user'} = $4;
			$parsed_msg{'diffbytes'} = $5;
			$parsed_msg{'summary'} = $6;
			if($parsed_msg{'diffbytes'} =~ /^\(\cB?([+-]\d+)\cB?\)$/){
				$parsed_msg{'diffbytes'} = $1;
			}
			$parsed_msg{'ns_id'} = $self->get_namespace_id($parsed_msg{'page_with_ns'});
			$parsed_msg{'page'} = $parsed_msg{'page_with_ns'};
			$parsed_msg{'page'} =~ s/^[^:]+:// if $parsed_msg{'ns_id'} != 0;
			$parsed_msg{'timestamp_unix'} = time();
			$parsed_msg{'timestamp'} = strftime("%Y%m%d%H%M%S", 
				gmtime($parsed_msg{'timestamp_unix'}));
			print Data::Dumper->Dump([\%parsed_msg]) if $self->{'verbosity'} >= 5;
		}else{
			$self->msg(2, $msg);
		}
		return \%parsed_msg;
	}

	sub parse_wikitext{
		my $self = shift;
		my $wikitext = shift;
		my $text = $self->{'mw_api'}->api( {
			action => 'parse',
			text => $wikitext,
			prop => 'text',
		}) or die $self->{'mw_api'}->{error}->{code} . 
			': ' . $self->{'mw_api'}->{error}->{details};
		my $html = $text->{'parse'}->{'text'}->{'*'};
		# delete final comments
		$html =~s/\n\n<!--\s+NewPP.*?\n-->$//s;
		return $html;
	}

	# searches for all links in a given piece of wikitext
	# original code got from 
	# http://svn.wikimedia.org/viewvc/mediawiki/trunk/phase3/includes/parser/Parser.php?view=markup&pathrev=74981
	# ported to perl because phps regexp-handling (\\\\\\\\\\\\\\\\\) sucks.
	sub php_code_unused{
		my $self = shift;
		# get wikitext as param
		my $wikitext = shift; # $text in orig code
		# building regexp for matching of "[...]"-like links
		my @wgUrlProtocols = (
			'http://',
			'https://',
			'ftp://',
			'irc://',
			'gopher://',
			'telnet://',
			'nntp://',
			'worldwind://',
			'mailto:',
			'news:',
			'svn://',
			'git://',
			'mms://'
		);
		my $re_prot = join('|', @wgUrlProtocols);
		my $ext_link_url_class = qr/[^\][<>"\x00-\x20\x7F\p{Zs}]/; 
		my $re_url_wo_prot =     qr/$ext_link_url_class+/;
		my $re_descr = qr/[^\]\x00-\x08\x0a-\x1F]*?/;
		my $mExtLinkBracketedRegex = qr/\[(($re_prot)$re_url_wo_prot) *($re_descr)\]'/; 
		# $1 = url, $2 = prot, $3 = descr
		# split wikitext by links
		my @bits = split /$mExtLinkBracketedRegex/, $wikitext;
		my $s = shift @bits; # first part is something non-link-like (i.e. an empty string or some wikitext).
		# loop over parts (of 4 different classes: wikitext, url, prot, descr, wikitext, url, prot, descr, ...)
		for(my $i=0; $i<@bits;){
			my $url = $bits[++$i];
			my $prot = $bits[++$i];
			my $descr = $bits[++$i]; # $text in orig code
			my $wtext = $bits[++$i] // ''; # $trail in orig code
			# will continue later on
		}
	}

	sub post_process_html{
		my $self = shift;
		my $html = shift;
		my $image_prefix = shift // '';
		#  images
		$html=~s/<a\x20                                 # <a
					href="\/wiki\/File:(?<filename>[^"]++)"   # href="\/wiki\/File:..."
				[^>]*+>                                     # ...>
				(?<imgpre><img\x20[^>]*src=")               # <img ... src="
					[^"]++                                    # ...
				(?<imgpost>"[^>]*+>)                        # ...>
			<\/a>                                         # <\/a>
			/$+{'imgpre'}$image_prefix$+{'filename'}$+{'imgpost'}/gx;
		$html=~s/ srcset="[^"]++"//g;
		#  html tidy
		my $html_cleaned = '';
		my $err = '';
		IPC::Run::run([qw(tidy -iq --tidy-mark 0 -f /dev/null -w 5000 -utf8)], 
			\$html, \$html_cleaned, \$err);
		return $html_cleaned;
	}

	sub read_file_binary{
		my $self = shift;
		my $filename = shift;
		open(my $FILE, '<:encoding(UTF-8)', $filename) or die $!;
			binmode $FILE;
			my $buffer;
			my $data = '';
			while(read($FILE, $buffer, 65536)){
				$data .= $buffer;
			}
		close($FILE);
		return \$data;
	}

	sub rebuild_table{
		my $self      = shift;
		my $page      = shift;         # wiki page
		my $tables    = shift;         # tables to insert
		my $summary   = shift;         # wiki summary
		my $text = $self->{'mw_bot'}->get_text($page);
		if(not defined $text){
			$self->msg(1, "page $page does not exist.", 'notice');
			$text = '';
		}elsif($text eq ''){
			$self->msg(1, "page $page is empty.", 'notice');
		}else{
			return unless $self->is_allowed(\$text, $page);
		}
		my $text_bak = $text;
		# convert table to wikitext string
		my $tablestyle_default = {
			'table'  => 'class="wikitable sortable"',
			#'header' => 'class="hintergrundfarbe6"',
		};
		my $tablestyle;
		if($text eq ''){
			for(@$tables){
				%$tablestyle = %$tablestyle_default;
				$tablestyle->{'table'} = 'class="'.$_->{'class'}.'"' if exists $_->{'class'};
				my $wikitable = $self->table2wikitext($_, $tablestyle, 1);
				$text .= "== ".$_->{'section'}." ==\n" if defined $_->{'section'};
				$text .= $wikitable;
			}
		}else{
			# find place to insert table and replace table
			for(@$tables){
				%$tablestyle = %$tablestyle_default;
				$tablestyle->{'table'} = 'class="'.$_->{'class'}.'"' if exists $_->{'class'};
				my $wikitable = $self->table2wikitext($_, $tablestyle, 1);
				if(defined $_->{'section'}){
					my $section = $_->{'section'};
					if($text =~/^=(=+) ?\Q$section\E ?\1=\n(?:(?!\n=|\n\[\[[cC]ategory:).)*/ms){
						$text =~ s/^=(=+) ?\Q$section\E ?\1=\n\K(?:(?!\n=|\n\[\[[cC]ategory:).)*/$wikitable/ms;
					}else{
						die "error: could not create/modify section. please ensure that wiki page [[$page]] either is empty or there exists the section '$section'!\n";
					}
				}else{
					if($text =~/(?:^<!--.*?-->\n)?^\{\|.*\|\}/ms){
						$text =~ s/(?:^<!--.*?-->\n)?^\{\|.*\|\}(\n|)\n*/$wikitable$1/ms;
					}else{
						die "error: could not create/modify section. please ensure that wiki page [[$page]] either is empty or there exists the appropriate section!\n";
					}
				}
			}
		}
		$self->msg(2, $text) if $self->{'simulation'};
		$self->time_management();
		$self->save_wiki_page($page, $summary, \$text, \$text_bak);
	}

	sub save_wiki_page{
		my $self = shift;
		my $page_utf8 = shift;
		my $summary = shift;
		my $text = shift;
		my $orig_text = shift;
		if($$text ne $$orig_text){
			my $user_answer_diff;
			# comment the following 3 lines if no X/tk is available.
			if(defined $self->{'showdiff'} and $self->{'showdiff'} > 0){
				$user_answer_diff = $self->show_diff($text, $orig_text);
			}
			if(not defined $self->{'simulation'} or !$self->{'simulation'}){
				my $user_input;
				$user_input = $user_answer_diff ? 'yes' : 'no' if defined $user_answer_diff;
				if(not defined $user_input and defined $self->{'ask_user'} and $self->{'ask_user'}){
					print "execute? ('y' = yes, else = no) ";
					chomp($user_input = <STDIN>);
				}
				if(not defined $user_input or $user_input=~/y(?:es)?/ or 
					($user_input eq '' and $self->{'user_answer'} eq 'y')){
					$self->{'user_answer'} = 'y';
					my $page = $page_utf8;
					utf8::encode($page);
					$self->msg(1, "saving page [[$page]] ... ");
					push @{$self->{'time_stack'}}, time;
					$self->{'mw_bot'}->edit({
							is_minor => $self->{'minor'},
							page     => $page_utf8,
							text     => $$text,
							bot      => 1,
							summary  => "Bot: $summary",
					});
				}else{
					$self->{'user_answer'} = '';
				}
			}else{
				my $page = $page_utf8;
				utf8::encode($page);
				$self->msg(1, "simulation of saving of page [[$page]] with summary '$summary'");
			}
		}
		return 1;
	}

	sub show_diff{
		my $self      = shift;
		my $text      = shift;
		my $orig_text = shift;
		my $diff      = diff($text, $orig_text);
		my $result;
		print $diff if $self->{'showdiff'} & 1;
		if($self->{'showdiff'} & 2){
			my @text_lines = grep {s/^\s*(.*?)\s*$/$1/} split /\n/, $$text;
			my @orig_text_lines = grep {s/^\s*(.*?)\s*$/$1/} split /\n/, $$orig_text;
			my @lines_to_show = ();
			while($diff=~/@@ -(\d+),(\d+) \+\d+,\d+ @@/g){
				push @lines_to_show, [$1-1, $1+$2-2];
			}
			#print Dumper(\@lines_to_show);
			#print Dumper(\@text_lines);
			my $shortend_text = '';
			my $shortend_orig_text = '';
			for(@lines_to_show){
				for($$_[0]..$$_[1]){
					$shortend_text.=$text_lines[$_]."\n";
					$shortend_orig_text.=$orig_text_lines[$_]."\n";
				}
			}
			chomp($shortend_text);
			chomp($shortend_orig_text);
			my $mw = new MainWindow;
			my $w = $mw->DiffText(
				-width=>1500, -height=>500, -orient => 'horizontal', -map => 'scrolled'
			)->pack();
			$w->load('a' => $shortend_text);
			$w->load('b' => $shortend_orig_text);
			$w->compare(-granularity => 'word');
			$result = '0';
			$mw->bind('<KeyPress-y>' => sub {$result = '1'; $mw->destroy;} );
			$mw->bind('<KeyPress-q>' => sub {exit 0;} );
			$mw->bind('<KeyPress>' => sub {$result = '0'; $mw->destroy;} );
			MainLoop;
		}
		return $result;
	}

	sub search_sbl_blocked_log{
		my $self   = shift;
		my $regexp = shift;
		my $lang   = shift // 'de';
		my @results = ();
		my $db_table = $self->db_fetch_sbl_log($lang, $regexp);
		if(defined $db_table){
			for my $dataset(@$db_table){
				push @results, {
					'ns' => $dataset->[1],
					'title' => $dataset->[2],
					'url' => $dataset->[4],
					'user' => $dataset->[5],
					'timestamp' => $dataset->[0],
					'comment' => $dataset->[3],
				};
				$results[-1]->{'url'} =~ s/^.*?"(http[^"]*)".*$/$1/;
			}
			$self->msg(1, 'found '.scalar(@results).' matching blocked edits.');
		}else{ # use api
			# api.php?action=query&list=logevents&letype=spamblacklist&leprop=ids|title|type|user|timestamp|comment|details&continue=
			my $query = {
				'action'         => 'query',
				'continue'       => '',
				'list'           => 'logevents', 
				'letype'         => 'spamblacklist',
				'lelimit'        => '500',
				'leprop'         => 'title|type|user|timestamp|comment|details', #ids
			};
			my $mw_options = {'max' => 1};
			my $finished = 0;
			my $num_results = 0;
			my $log_counter = 0;
			while(!$finished){
				(my $pages, $finished) = $self->api_cont($query, $mw_options);
				my $curr_counter = scalar(@{$pages->{'query'}->{'logevents'}});
				$log_counter += $curr_counter;
				push @results, grep {
						$_->{'url'} =~ /$regexp/
				} @{$pages->{'query'}->{'logevents'}};
				if($self->{'verbosity'} >= 3 and $curr_counter > 0){
					my $min_ts = $pages->{'query'}->{'logevents'}->[0]->{'timestamp'};
					my $max_ts = $min_ts;
					map {
						my $ts = $_->{'timestamp'};
						$min_ts = $ts if $ts lt $min_ts;
						$max_ts = $ts if $ts gt $max_ts;
					} @{$pages->{'query'}->{'logevents'}};
					$self->msg(3, 'found ' . (scalar(@results) - $num_results) . 
						" blocked edits ($min_ts till $max_ts).");
				}
				$num_results = scalar(@results);
			}
			map {
				delete $_->{'type'};
				delete $_->{'anon'};
				delete $_->{'action'};
			} @results;
			$self->msg(1, "log contains $log_counter entries. found $num_results matching blocked edits.");
		}
		return \@results;
	}

	sub table2wikitext{
		my $self      = shift;
		my $table     = shift;
		my $style     = shift;
		my $full_auto = shift // 1;
		$style->{'table'} = '' if not defined $style->{'table'};
		my $wikitable = '';
		if($full_auto){
			$wikitable .= "<!-- this table is generated automatically. " . 
				"any manual modifications will be deleted on next update. -->\n";
		}
		$wikitable .= "{| ".$style->{'table'}."\n";
		my $row = 0;
		if(exists $table->{header} and $table->{'header'}){
			$wikitable .= '|- '.$style->{'header'}."\n" if defined $style->{'header'};
			$wikitable .= '! '.$_."\n" for @{$table->{'body'}[$row]};
			++$row;
		}
		for(;$row<@{$table->{'body'}}; ++$row){
			$wikitable .= "|-\n";
			for(my $col = 0;$col<@{$table->{'body'}[$row]}; ++$col){
				$wikitable .= '| '.$table->{'body'}[$row][$col]."\n";
			}
		}
		$wikitable .= "|}\n";
		return $wikitable;
	}

	sub table_body2array{
		my $self = shift;
		my $table_body = shift;
		# truncate table body
		$$table_body =~ s/\n\z//;
		$$table_body =~ s/^\s*\|-\s*|\s*\|-\s*$//g;
		return [
			map {
				s/^\s*\| ?//;
				s/\s+$//;
				if(index($_,'||')>-1){
					[split / *+\|\| *+/, $_];
				}else{
					[split /\s*\n\| *+/m, $_];
				}
			} split /\s*\n\|- *+\n/, $$table_body
		];
	}

	sub test_and_replace{
		my $self     = shift;
		my $text     = shift;
		my $regexp_s = shift;
		my $regexp_r = shift;
		my $strpos = 0;
		my $array_of_changes = [];
		$self->msg(3, "   ".$regexp_s);
		my $numChanges = $$text=~s/$regexp_s/
			my $match = ${^MATCH};
			my $prematch = ${^PREMATCH};
			my $replaced = eval($regexp_r);
			# cope with links
			if(!$self->check_external_link(\$prematch, \$replaced)){
				$replaced = $match;
			}
			push(@$array_of_changes, [$match, $replaced]); 
			$replaced;
			/gpme;
		$numChanges = 0 if $numChanges eq '';
		if($self->{'verbosity'} >= 1){
			for my $repl(@$array_of_changes){
				$self->msg(1, "   ".$repl->[0]);
				$self->msg(1, " ->".$repl->[1]);
			}
		}
		#$test=~s/$regexp_s/$regexp_r/gmee; # just for debugging
		#$self->msg(1, "wrong replacing with '$regexp_r'?", 'error') if $$text ne $test;
		return $numChanges;
	}

	sub test_and_replace_with_funcref{
		my $self        = shift;
		my $replacement = shift; # function ref
		my $pre_url     = shift;
		my $old_url     = shift;
		my $post_url    = shift;
		my $correction  = '';
		# cope with templates like {{something|url=http://example.org|...}} or {{something|url=http://example.org}} 
		if($pre_url =~/url\s*=\s*$/ and $old_url =~/^[^{|}]+([{|}].*)/){
			$correction = $1;
			$post_url = $1.$post_url;
			$old_url = substr($old_url, 0, -length($1));
		}
		# call function reference
		my $new_url = $replacement->($old_url, $post_url);
		# print changes to stdout
		if($self->{'verbosity'} >= 1){
			print "   ".$old_url."\n"; # print old text
			print " ->".$new_url."\n"; # print replaced text
		}
		# check http status; avoid change, if target not reachable
		if($new_url=~/^https?:\/\//){
			my $response_code = $self->get_http_status($new_url);
			$self->msg(1, '  status: '.$response_code);
			$new_url = $old_url if $response_code != 200;
		}
		return $new_url.$correction;
	}

	sub text_replacement{
		my $self       = shift;
		my $params     = shift;
		my $pages      = $params->{'pages'};        # pages to search
		my $patterns   = $params->{'pattern'};      # patterns to search s/X//
		my $repls      = $params->{'replacement'};  # replacing texts s//X/e
		my $max_edits  = $params->{'max_edits'};
		my $skip_edits = $params->{'skip_edits'};
		my $summary    = $params->{'summary'};
		$self->msg(1, "loop over pages and replace text");
		my $edit_counter = -1;
		if(@$patterns == 0+@$repls){
			for my $page(@$pages){
				if(++$edit_counter < $skip_edits or $max_edits == 0){
					next;
				}elsif($edit_counter >= $max_edits and $max_edits != -1){
					last;
				}
				$self->msg(1, "$edit_counter: page = '$page'");
				my $text = $self->{'mw_bot'}->get_text($page);
				unless(defined $text){
					$self->msg(0, "page content not defined", 'error');
					next;
				}
				if(not $self->is_allowed(\$text, $page)){
					$self->msg(2, "  not allowed");
					next;
				}
				my $text_bak = $text;
				$self->cleanup_wiki_page(\$text, $page);
				my $found = 0;
				for(my $i=0; $i<@$patterns; ++$i){
					if($self->test_and_replace(\$text, $patterns->[$i], $repls->[$i]) > 0){
						++$found;
					}
				}
				$self->msg(2, "  $found replacements");
				$self->time_management();
				$self->save_wiki_page($page, $summary, \$text, \$text_bak);
			}
		}
	}

	sub time_management{
		my $self = shift;
		while($self->{'max_edits_per_min'} != -1 and 
			@{$self->{'time_stack'}} >= $self->{'max_edits_per_min'}){
			my $time_diff = time - $self->{'time_stack'}->[0];
			if($time_diff < 60){
				$self->msg(1, '  ...waiting '.(60-$time_diff)." seconds");
				sleep(60 - $time_diff);
			}
			shift @{$self->{'time_stack'}};
		}
		return 1;
	}

	sub title2filename{
		my $self = shift;
		my $title = shift;
		$title =~s/ß/ss/g;
		$title =~s/ä/ae/g;
		$title =~s/Ä/Ae/g;
		$title =~s/ö/oe/g;
		$title =~s/Ö/Oe/g;
		$title =~s/ü/ue/g;
		$title =~s/Ü/Ue/g;
		$title =~s/[^a-zA-Z0-9_.~!-]/_/g;
		return $title;
	}

	sub title2url_part{
		my $self = shift;
		my $title = shift;
		$title =~s/ /_/g;
		my $url_part = uri_escape($title);
		return $url_part;
	}

	sub update_edit_filter_index{
		my $self = shift;
		# first get recent changes
		my $max_num_talk_pages = 25;
		my $max_num_days_age = 5;
		my $edit_filter_name = 'Bearbeitungsfilter';
		my $lt_page    = 'Wikipedia:'.$edit_filter_name.'/latest topics';
		# api.php?action=query&generator=allpages&gapnamespace=4&gapprefix=Bearbeitungsfilter/&gapfilterredir=nonredirects&gaplimit=500&prop=info|revisions&rvprop=content|timestamp
		$self->{'mw_api'}->{config}->{api_url} = $self->{'mw_apiurl'};
		my $pages = [
			sort {$b->{'revisions'}->[0]->{'timestamp'} cmp $a->{'revisions'}->[0]->{'timestamp'}}
			@{$self->get_pages_by_prefix({
				'namespace_id' => 4,
				'prefix'       => $edit_filter_name.'/',
				'title_re'     => $self->{'re_editfilter'},
			})}
		];
		# last change of latest_topics
		my $lt_last_change = 
			$self->get_page_info($lt_page)->{$lt_page}->{'touched'};
		my $i = 0;
		my $tables = [{
			'header' => 1,
			'body' => [['Regel', "letzte Seiten\xe4nderung", 'Thread', 'letzter Autor']],
		}];
		my %new_topics;
		for my $page(@$pages){
			#$self->msg(2, " $page->{'revisions'}->[0]->{'timestamp'}: $page->{title}");
			my $npost = $self->newest_post_info($page->{'revisions'}->[0]->{'*'});
			next unless defined $npost->{'thread'};
			++$i;
			# date of last change of rule
			my $date_of_last_change = 
				'{{#timel:Y-m-d H:i:s|'.$page->{'revisions'}->[0]->{'timestamp'}.'}}';
			$tables->[0]{'body'}[$i][1] = $date_of_last_change;
			# page title
			if($page->{'title'}=~/\D(\d+)$/){
				$tables->[0]{'body'}[$i][0] = "[[$page->{'title'}|#$1]]";
				# get description for rule
			}elsif($page->{'title'}=~/\/([^\/]+)$/){
				$tables->[0]{'body'}[$i][0] = "[[$page->{'title'}|$1]]";
			}
			my $page_short_title = $1 // 'unknown';
			# thread title
			my $thread_anchor = "{{subst:anchorencode:$npost->{'thread'}}}";
			my $thread_title = $npost->{'thread'};
			# remove links from title
			$thread_title =~s/\[\[[^|\]]*\|([^|\]]*)\]\]/$1/g;
			$thread_title =~s/\[\[|\]\]//g;
			# add thread title to table
			$tables->[0]{'body'}[$i][2] = 
				"[[$page->{'title'}#$thread_anchor|$thread_title]]";
			# remember newest topics
			if(not defined $lt_last_change 
					or $lt_last_change lt $page->{'revisions'}->[0]->{'timestamp'}){
				$new_topics{$page_short_title} = $thread_title;
			}
			# author
			$tables->[0]{'body'}[$i][3] = $npost->{'author'} eq '' ? '' : 
				"[[special:contributions/$npost->{'author'}|$npost->{'author'}]]";
			last if $page->{'revisions'}->[0]->{'timestamp'} lt 
				strftime("%Y-%m-%d", gmtime(time-60*60*24*$max_num_days_age)) 
				and $i>=$max_num_talk_pages;
		}
		#print table2wikitext($tables->[0], {'table' => 'class="wikitable sortable"'}, 1);
		if(keys %new_topics > 0){
			my $filter_descriptions = $self->get_abuse_filter_info();
			# build summary
			my $summary_thread_part = '';
			my $summary = '('.(
				join ', ', 
				map {
					$summary_thread_part .= $filter_descriptions->[$_] // $new_topics{$_};
					$summary_thread_part .= '; ';
					/^[0-9]+$/ ? "#$_" : $_;
				} # add '#' on rule numbers
				sort {
					# todo: there's some bug inhere. strings are recognized as numbers?
					my $result;
					if($a =~ /^[0-9]+$/ and $b =~ /^[0-9]+$/){ 
						# numerical sorting on rule numbers
						# print "$a X $b\n" if $a !~ /^[0-9]+$/ or $b !~ /^[0-9]+$/;
						$result = ($a <=> $b);
					}elsif($a !~ /^[0-9]+$/ and $b !~ /^[0-9]+$/){ 
						# alphabetical sorting on other pages
						$result = ($a cmp $b);
					}else{ 
						# when sorting rule number against other page, use counter-sort
						$result = ($b cmp $a);
					}
					$result;
				} 
				keys %new_topics
			).'): ';
			$summary .= substr($summary_thread_part, 0, -2); # delete last '; '
			# now update page
			$self->rebuild_table($lt_page, $tables, $summary);
		}
	}

	sub update_maintenance_lists{
		my $self          = shift;
		my $page          = shift; # page title or undef; undef means: force update without checking conditions
		my $text          = shift; # ref to string (or undef if $page is undef)
		my $type          = shift; # type of maintenance list
		my $maintenance_lists = shift; # maintenance lists
		if(defined $page and not defined $text){
			my $text = $self->{'mw_bot'}->get_text($page);
			$text = \$text if defined $text;
		}
		if(not defined $page or defined $text){
			my %intros = (
				'ASIN' => "dies ist eine durch CamelBot befuellte wartungsliste, die kuerzlich bearbeitete artikel auflistet, die mind. eine [[Amazon Standard Identification Number|ASIN]] enthalten.\n
CamelBot lauscht auf den recent changes, d.h. er schaut sich alle artikel an, die aktuell geaendert wurden. faellt ihm dabei auf, dass in einem artikel eine ASIN steht, meldet er das nach ca. 30 minuten hier auf dieser seite. dabei fuegt er gefundene artikel nur der liste hinzu. auch das loeschen von eintraegen, die ASIN nicht mehr enthalten, uebernimmt CamelBot.\n
== liste ==\n",
				'cat dead' => "dies ist eine durch CamelBot befuellte wartungsliste, die kuerzlich bearbeitete artikel auflistet, die der category ''gestorben " . (strftime('%Y', gmtime())) . "'' hinzugefuegt wurden.\n
CamelBot lauscht auf den recent changes, d.h. er schaut sich alle artikel an, die aktuell geaendert wurden. faellt ihm dabei auf, dass ein artikel dieser category hinzugefuegt wird, meldet er das nach ca. 30 minuten hier auf dieser seite. dabei fuegt er gefundene artikel nur der liste hinzu. loeschen muss man manuell.\n
== liste ==\n",
			);
			for my $mt_type(keys %{$self->{'re_maintenance'}}){
				if(($type eq $mt_type or $type eq 'all_types') and 
					(not defined $page or $$text=~/$self->{'re_maintenance'}->{$mt_type}/)
				){
					# todo: how to cope with cat dead?
					next if $mt_type eq 'cat dead';# and defined $$text 
					#	and $$text =~/(?:[cC]ategory|[kK]ategorie)\s*:\s*[nN]ekrolog/;
					$self->msg(1, "check '$mt_type'");
					my $ml = "User:CamelBot/maintenance list/$mt_type";
					my $ml_text = $self->{'mw_bot'}->get_text($ml) // '';
					my $ml_text_bak = $ml_text;
					my $intro = $intros{$mt_type};
					my $outro = "\n\n[[Kategorie:Wikipedia:Wartungskategorie|$mt_type]]";
					my $table;
					my $wikitable;
					my $tablestyle = {
						'table'  => 'class="wikitable sortable"',
					};
					if($ml_text =~ /^(.*\n== [lL]iste? ==\n+.*?)(\{\|.*?\|\})\n(.*)$/s){ # extract (save) old intro
						$intro = $1;
						$wikitable = $2;
						$outro = $3;
					}else{
						$self->msg(1, 'could not fetch table. resetting page.', 'notice');
						$wikitable = '{| '.$tablestyle->{'table'}."\n".
							"! page !! detected by CamelBot\n".
							'|}';
					}
					$table = $self->wikitable2array(\$wikitable);
					# check all old entries; search new entry, if not in array, then push
					my $already_in_list = 0;
					my $new_entry = defined $page ? "[[$page]]" : undef;
					$table->{'body'} = [grep {
							my $text = '';
							if(defined $new_entry and $_->[0] eq $new_entry){
								$self->msg(1, "page '$page' already in list");
								$already_in_list = 1;
							}elsif($_->[0] =~ /^\[\[(.*)\]\]$/){
								my $page_with_asin = $1;
								$text = $self->{'mw_bot'}->get_text($page_with_asin);
							}
							$text =~ /$self->{'re_maintenance'}->{$mt_type}/ or $already_in_list;
						} @{$table->{'body'}}
					];
					if(defined $new_entry and $already_in_list==0){
						# add new list entry
						$self->msg(1, "add new list entry '$page'");
						push @{$table->{'body'}}, [$new_entry, strftime("%Y-%m-%d", gmtime())];
					}
					$maintenance_lists->{$mt_type} = {}; # reset and refill list
					for my $entry(@{$table->{'body'}}){
						$maintenance_lists->{$mt_type}->{substr($entry->[0], 2, -2)} = 1;
					}
					unshift @{$table->{'body'}}, [@{$table->{'header'}}];
					$table->{'header'} = 1;
					my $full_auto = 0;
					$wikitable = $self->table2wikitext($table, $tablestyle, $full_auto);
					# rebuild page
					$ml_text = $intro.$wikitable.$outro;
					$self->time_management();
					$self->save_wiki_page($ml, 'update maintenance list', \$ml_text, \$ml_text_bak);
				}
			}
			return 1;
		}else{
			if(defined $page){
				$self->msg(1, "could not fetch page '$page'. maybe deleted already", 'notice');
			}else{
				$self->msg(1, 'no page and no text defined. this seems senseless.', 'notice');
			}
			return 0;
		}
	}

	sub upload_file{
		my $self = shift;
		my $filename_src = shift;
		my $summary = shift;
		my $filename_dest = shift // $filename_src;
		my $data = $self->read_file_binary($filename_src);
		# check, whether file exists already. If so, compare. don't upload, if same.
		$self->msg(2, " downloading file if existant");
		my $oldfile = $self->{'mw_api'}->download({'title' => 'File:'.$filename_dest});
		if(not defined $oldfile or $self->{'mw_api'}->{error}->{code}!=0){
			$self->msg(0, $self->{'mw_api'}->{error}->{code} . ': ' . 
				$self->{'mw_api'}->{error}->{details});
			return 0;
		}
		$self->msg(3, " downloaded file (if existant)");
		if($oldfile eq '' or $oldfile ne $$data){
			$self->msg(3, " trying to upload new file");
			if(!$self->{'simulation'}){
				# upload
				#$mw->edit({
				# 'action'   => 'upload',
				# 'filename' => $filename_src,
				# 'comment'  => $summary,
				# 'file'     => [undef, $filename_dest, 'Content'=>$$data],
				# })
				$self->{'mw_api'}->upload({
						'data'    => $$data,
						'summary' => $summary,
						'title'   => $filename_dest,
					}) or die $self->{'mw_api'}->{error}->{code} . ': ' . $self->{'mw_api'}->{error}->{details};
			}
			return 1;
		}else{
			$self->msg(1, "won't force uploading. file is already online.");
			return 0;
		}
	}

	sub url2title{
		my $self   = shift;
		my $url    = shift;
		$url =~ /^(?:https?:\/\/de\.wikipedia\.org\/wiki\/)?(.*)/;
		my $article_name = uri_unescape($1);
		$article_name =~s/_/ /g;
		# 2014-12-23, seth: "https://de.wikipedia.org/wiki/K%C3%B6nigliche_und_barmherzige_Vereinigung_der_Ordens-_und_Medaillentr%C3%A4ger_von_Belgien" became: "KÃ¶nigliche und barmherzige Vereinigung der Ordens- und MedaillentrÃ¤ger von Belgien"
		utf8::decode($article_name);
		$article_name =~ s/^(?=Datei:|Bild:|Image:|File:|Category:|Kategorie:)/:/;
		return $article_name;
	}

	sub wikitableHeader2array{
		my $self = shift;
		my $header = shift;
		$header =~s/^\s*!\s*//s;
		$header =~s/\s+$//s;
		if(index($header, '!!') > -1){
			$header = [split /\s*!!\s*/, $header];
		}else{
			$header = [split /\s*!\s*/, $header];
		}
		return $header;
	}

	sub wikitable2array{
		my $self = shift;
		my $wikitable = shift;
		$$wikitable =~/^\{\|   # begin of table
			(?-s:.*)\n+          # class, id, ...
			((?:!\s*[^\n]*\n+)*) # table header
			(.*)                 # table body
			(?<=\n)\|\}$         # end of table
		/sx;
		my $table = {
			'header' => $self->wikitableHeader2array($1),
			'body' => $2,
		};
		# decomposing wiki-table to array
		$table->{'body'} = $self->table_body2array(\$table->{'body'});
		return $table;
	}

	sub write_csv{
		my $self = shift;
		my $array = shift;
		my $filename = shift;
		my $separator = shift // ';';
		open(my $OUTFILE, '>:encoding(UTF-8)', $filename) or die "$!\n";
			for my $row(@$array){
				map {
					s/([\\'])/\\$1/g; # escape \ and '
					$_ = "'$_'";  # entry -> 'entry'
				} @$row;
				print $OUTFILE join $separator, @$row;
				print $OUTFILE "\n";
			}
		close($OUTFILE);
		return 1;
	}
}

{
	package CamelBotRC;
	use POSIX qw/strftime/;     # format timestamp
	use Data::Dumper;           # for debugging purposes
	use Time::Local;            # timegm
	$| = 1; # deactivate buffering, so flush all the time

	sub new{
		my $class    = shift; 
		my $camelbot = shift;
		my $self = bless {
			'camelbot'             => $camelbot,
			're_edit_filter_name'  => qr/(?:Bearbeitung|Missbrauch)sfilter/,
			'rc_pages_shortterm'   => {},    # rc pages to check
			'rc_pages_midterm'     => {},    # rc pages to check
			'rc_pages_maintenance' => {},    # rc special pages to check
			'delay'                => 30*60, # seconds
			'delay_short'          => 10,    # seconds
			'short_max_age'        => 60*60*3, # seconds
			'start_ts'             => time,  # only for debugging purposes
			're_namespaces'        => (join '|', grep {$_ ne ''} # get all namespaces!=0
				values {$camelbot->{'mw_bot'}->get_namespace_names()}),
		}, $class;
		$self->{'re_namespaces'} = qr/^$self->{'re_namespaces'}:/;
		# initially update and fetch maintenance lists 
		my $page = undef;
		my $text = undef;
		$camelbot->update_maintenance_lists(
			$page, $text, 'all_types', $self->{'rc_pages_maintenance'});
		return $self;
	}

	sub get_rc_via_db{
		my $self = shift;
		my $timestamp_begin = shift // strftime("%Y%m%d%H%M%S", gmtime(time()-60*60*12));
		my $table = $self->{'camelbot'}->db_fetch_recentchanges($timestamp_begin);
		my $rc_pages = [];
		return undef unless defined $table;
		for my $dataset(@$table){
			my %rc_msg;
			$rc_msg{'timestamp'} = $dataset->[0];
			next unless $rc_msg{'timestamp'} =~/^
				(?<year>[0-9]{4})
				(?<month>[0-9]{2})
				(?<mday>[0-9]{2})
				(?<hour>[0-9]{2})
				(?<min>[0-9]{2})
				(?<sec>[0-9]{2})
				$/x;
			my $timestamp_unix = timegm(
				$+{'sec'}, $+{'min'}, $+{'hour'}, $+{'mday'}, $+{'month'}-1, $+{'year'});
			#$self->{'rc_pages_midterm'}->{$dataset->[2]} = $timestamp_unix;
			#$self->{'rc_pages_shortterm'}->{$dataset->[2]} = $timestamp_unix;
			$rc_msg{'timestamp_unix'} = $timestamp_unix;
			$rc_msg{'user'}      = $dataset->[1];
			$rc_msg{'ns_id'}     = $dataset->[2];
			$rc_msg{'page'}      = $dataset->[3];
			$rc_msg{'page'} =~ s/_/ /g;
			$rc_msg{'summary'}   = $dataset->[4];
			$rc_msg{'page_with_ns'} = $dataset->[2] == 0 ? $dataset->[3] : 
				$self->{'camelbot'}->convert_ns($dataset->[2]) . ':' . $dataset->[3];
			#$rc_msg{'flags'}     = $dataset->[];
			#$rc_msg{'url'}       = $dataset->[];
			$rc_msg{'diffbytes'} = $dataset->[8] - $dataset->[7];
			push @$rc_pages, \%rc_msg;
		}
		if($self->{'camelbot'}->{'verbosity'} >= 2 or @$table > 50){
			$self->{'camelbot'}->msg(1, 'got '.(0+@$table).' entries.');
		}
		return $rc_pages;
	}

	sub handle_rc_pages{
		my $self   = shift;
		my $now    = time;
		my $camelbot = $self->{'camelbot'};
		# delayed clean up:
		my @updList_midterm;
		my @updList_shortterm;
		my $max_pages_per_call = 200;
		# after delay transfer pages from $rc_pages_shortterm to @updList_shortterm
		while(my ($title, $timestamp_unix) = each %{$self->{'rc_pages_shortterm'}}){
			if($timestamp_unix + $self->{'delay_short'} < $now){
				delete $self->{'rc_pages_shortterm'}->{$title};
				$camelbot->msg(2, ' short term update of page: '. $title);
				push @updList_shortterm, $title;
				last if @updList_shortterm >= $max_pages_per_call;
			}
		}
		# after delay transfer pages from $rc_pages_midterm to @updList_midterm
		while(my ($title, $timestamp_unix) = each %{$self->{'rc_pages_midterm'}}){
			if($timestamp_unix + $self->{'delay'} < $now){
				delete $self->{'rc_pages_midterm'}->{$title};
				$camelbot->msg(2, ' mid term update of page: '. $title);
				push @updList_midterm, $title;
				last if @updList_midterm >= $max_pages_per_call;
			}
		}
		my $numArticles = scalar(@updList_shortterm);
		if($numArticles >= 100){
			$camelbot->msg(1, "starting notification check of $numArticles (of " . 
				($numArticles + keys(%{$self->{'rc_pages_shortterm'}})) . ") articles.");
		}
		# all pages in @updList_shortterm shall be treated now.
		for my $page(@updList_shortterm){
			# notifier on particular edits; there needs to be some delay, because 
			# otherwise it may occur that the notification is faster than the saving 
			# process itself.
			$camelbot->msg(3, "fetch page '$page'.");
			$camelbot->notifier($page);
		}
		if($numArticles >= 100){
			$camelbot->msg(2, "end of notification check of $numArticles articles.");
		}
		# all pages in @updList_midterm shall be treated now.
		my $cnt = 0;
		$numArticles = scalar(@updList_midterm);
		if($numArticles >= 100){
			$camelbot->msg(1, "starting mid term checks of $numArticles (of " . 
				($numArticles + keys(%{$self->{'rc_pages_midterm'}})) . ") articles.");
		}
		for my $page(@updList_midterm){
			$camelbot->msg(3, "fetch page '$page' (".(++$cnt).").");
			my $text = $camelbot->{'mw_bot'}->get_text($page);
			if(defined $text and $camelbot->is_allowed(\$text, $page)){
				my $text_bak = $text;
				my $changes = $camelbot->cleanup_wiki_page(\$text, $page);
				if($text_bak ne $text){
					my $summary = 'kleinere korrekturen ('.
						(join ', ', grep {$changes->{$_} > 0} keys %$changes).
						'). feedback bitte auf [[user talk:lustiger_seth]] melden.';
					$camelbot->time_management();
					$camelbot->save_wiki_page($page, $summary, \$text, \$text_bak);
				}
				$camelbot->update_maintenance_lists($page, \$text, 'all_types', 
					$self->{'rc_pages_maintenance'});
			}else{
				$camelbot->msg(2, "could not fetch page '$page'. maybe deleted already", 
					'notice');
			}
		}
		if($numArticles >= 100){
			$camelbot->msg(2, "end of mid term checks of $numArticles articles.");
		}
	}

	sub put_new_rc{
		my $self     = shift;
		my $rc_pages = shift;
		my $camelbot = $self->{'camelbot'};
		my %update = (
			'ns0'        => 0,
			'editfilter' => 0,
		);
		for my $rc_msg(@$rc_pages){
			if(# $rc_msg->{'user'} eq 'Lustiger seth' or # for debugging only
				$rc_msg->{'user'} ne $camelbot->{'mw_username'} # don't track own edit
					and $rc_msg->{'summary'} !~ /\bCamelBot\b.*\b(?:rückgängig gemacht|revertiert)\b/ # don't play edit war
					and $rc_msg->{'ns_id'} == 0
			){
				$self->{'rc_pages_midterm'}->{$rc_msg->{'page_with_ns'}} = 
					$rc_msg->{'timestamp_unix'};
				if(time - $rc_msg->{'timestamp_unix'} < $self->{'short_max_age'}){
					$self->{'rc_pages_shortterm'}->{$rc_msg->{'page_with_ns'}} = 
						$rc_msg->{'timestamp_unix'};
				}
				$camelbot->msg(2, 'new rc: ' . 
					($self->{'rc_pages_midterm'}->{$rc_msg->{'page_with_ns'}} - $self->{'start_ts'}) .
					' (#pages in stack: ' . scalar(keys %{$self->{'rc_pages_midterm'}}) . ') ' . 
					$rc_msg->{'page_with_ns'} . ', editor = ' . $rc_msg->{'user'}
				);
				# check if page is on a maintenance list
				my $text = undef;
				while(my ($type, $pages) = each %{$self->{'rc_pages_maintenance'}}){
					if(exists $pages->{$rc_msg->{'page_with_ns'}}){
						$camelbot->update_maintenance_lists($rc_msg->{'page_with_ns'}, $text, $type, 
							$self->{'rc_pages_maintenance'});
					}
				}
				$update{'ns0'} = 1;
			}
			# update edit filter index
			if($rc_msg->{'ns_id'} == 4 and $update{'editfilter'} == 0
					and $rc_msg->{'page'} =~ /^$self->{'re_edit_filter_name'}\//
			){
				$update{'editfilter'} = 1;
			}
		}
		if($update{'ns0'}){
			$self->handle_rc_pages();
		}
		if($update{'editfilter'}){
			$camelbot->update_edit_filter_index();
		}
	}

	sub db_rc_monitoring{
		my $self = shift;
		my $last_check = 0;
		my $continue_timestamp = undef;
		my $camelbot = $self->{'camelbot'};
		my $sleep_delay = 30; # seconds
		while(1){
			# wait for $sleep_delay seconds before sending a query to db again
			if($last_check + $sleep_delay > time){
				my $sleep_time = $sleep_delay + $last_check - time;
				$camelbot->msg(2, "sleep $sleep_time seconds");
				sleep($sleep_time);
			}
			$last_check = time;
			$camelbot->msg(2, 'get new rc pages from db');
			my $new_rc_pages = $self->get_rc_via_db($continue_timestamp);
			# delete multiple entries of same page
			$camelbot->msg(2, 'delete multiple entries');
			my %dbl_counter;
			map {
				if(defined $dbl_counter{$_->{'page_with_ns'}}){
					++$dbl_counter{$_->{'page_with_ns'}};
				}else{
					$dbl_counter{$_->{'page_with_ns'}} = 1;
				}
			} @$new_rc_pages;
			$new_rc_pages = [
				grep {
					--$dbl_counter{$_->{'page_with_ns'}} ==0 
				} @$new_rc_pages
			];
			if(@$new_rc_pages > 0){
				$continue_timestamp = $new_rc_pages->[-1]->{'timestamp'};
				while(@$new_rc_pages > 0 
						and $new_rc_pages->[-1]->{'timestamp'} eq $continue_timestamp){
					pop @$new_rc_pages;
				}
				$camelbot->msg(1, 'got '.(0+@$new_rc_pages).' new pages');
				$self->put_new_rc($new_rc_pages);
			}
		}
	}
}

{
	package CamelBotIRC;
	use parent qw(Bot::BasicBot); # irc bot
	use Data::Dumper;           # for debugging purposes
	use Time::Local;            # timegm
	$| = 1; # deactivate buffering, so flush all the time

	sub new{
		my $class    = shift; 
		my $camelbot = shift;
		my $params   = shift;
		my $self = $class->SUPER::new(%$params);
		$self->{'rc'} = CamelBotRC->new($camelbot);
		$self->{'camelbot'} = $camelbot;
		return $self;
	}

	# bot help
	sub help {
		my $self = shift;
		return 'i\'m just listening to recent changes in w:de and sometimes those changes trigger some of my functions. if you have any questions, please ask my master at https://de.wikipedia.org/wiki/user_talk:lustiger_seth.';
	}

	# bot handler: if somebody said something
	sub said {
		my ($self, $message) = @_;
		my $camelbot = $self->{'camelbot'};
		# print everything what is said to you
		$camelbot->msg(4, "$message->{who}: $message->{body}");

		# general
		if(defined $message->{address} and $message->{address} eq $self->{nick}){
			$camelbot->msg(2, $message->{who}.': '.$message->{body});
			# help
			if($message->{body}=~/^(?:help|was machst du\?|was kannst du\?)/){
				$self->reply($message, $self->help());
			}
		}

		# rc-channel
		if($self->{'alias'} eq 'rc'){
			# reporting of specific edits
			if($message->{who} =~/^rc-\w+$/){
				# parse rc message
				my $rc_msg = $camelbot->parse_irc_rc($message->{body});
				$self->{'rc'}->put_new_rc([$rc_msg]);
				#if(# $rc_msg->{'user'} eq 'Lustiger seth' or # for debugging only
				#	$rc_msg->{'user'} ne $camelbot->{'mw_username'} and # don't track own edit
				#	$rc_msg->{'summary'} !~ /\bCamelBot\b.*\b(?:rückgängig gemacht|revertiert)\b/ and # don't play edit war
				#	$rc_msg->{'ns_id'} == 0
				#){
				#	$self->forkit(
				#		run => \&{$self->put_new_rc}, 
				#		channel => $message->{'channel'}, 
				#		#handler => \&_fork_said_handler,
				#		body => $self,
				#		arguments => [$rc_msg, time()],
				#	);
				#}
				##print Dumper $rc_msg;
				## update edit filter index
				#if($rc_msg->{'ns_id'} == 4 and $rc_msg->{'page'} =~ /^$self->{'rc'}->{'re_edit_filter_name'}\//
				#	$camelbot->update_edit_filter_index();
				#}
			}
		}
	}
}

# main
my $params = syntaxCheck(@ARGV);
my $camelbot = CamelBot->new({
	'ask_user'    => 0,
	'max_edits_per_min' => 5,
	'minor'       => $params->{'minor'},
	'mw_username' => $params->{'username'}, 
	#'mw_password' => undef, 
	'simulation'  => $params->{'test'},
	'showdiff'    => 0,
	'verbosity'   => $params->{'verbose'},
	'cliparams'   => $params,
});

#used for debugging only:
#my $text = $camelbot->{'mw_bot'}->get_text('...');
#my $changes = $camelbot->cleanup_wiki_page(\$text, 'moep');
#print Dumper $changes;
#exit 1;

if($params->{'cat-add'}){
	my $pages = [
		# list of articles {{{
		# }}}
	];
	my $category = 'safety data sheet';
	$camelbot->cat_add($pages, $category);
}

if($params->{'cat-change'}){
	$camelbot->cat_rename('Software tool', 'Library or package');
}

if($params->{'delete'}){
	$camelbot->delete_marked_pages();
}

if(defined $params->{'download-by-prefix'}){
	$camelbot->download_pages_by_prefix($params->{'download-by-prefix'});
}

if(defined $params->{'http-status'}){
	my @urls = (
		#'http://de.wikipedia.org/example',
		split / /, $params->{'http-status'}
	);
	for my $url(@urls){
		my $response_code = $camelbot->get_http_status($url);
		$camelbot->msg(1, $response_code.' '.$url);
		sleep 5;
	}
}

if($params->{'link-replacement'}){
	my %options;
	$options{'articles'}      = 1;  # 0 = don't work on namespace 0; 1 = work on namespace 0
	$options{'nonarticles'}   = 1;  # 0 = don't work on namespace!=0; 1 = work on namespace!=0
	$options{'refs'}          = 1;  # 0 = don't touch refs; 1 = replace refs
	$options{'nonrefs'}       = 1;  # 0 = don't touch non-refs; 1 = replace non-refs
	$options{'max_edits'}     = -1;  # maximum number of edits (-1 = inf)
	$options{'skip_edits'}    = 0;  # skip number of pages
	$options{'results'}       = 1;  # print a summary at the end
	# user to use in api contributions
	$options{'user_contribs'} = undef; # {'user' => 'Gary Dee'};
	# searched_link = link to use in api link search
	# re_prot_part  = regexp pattern of protocol
	# re_url_part   = regexp pattern of searched url (without protocol)
	# replacement   = replacement part of s///
	# summary       = summary of page edit
	
	$options{'re_prot_part'}  = qr/http:\/\//;
	# some typical cases:
	# 1. linkfix
	#$options{'searched_link'} = '*.bgblportal.de/BGBL/bgbl1f/';
	#$options{'searched_link'} = '217.160.60.235';
	#$options{'searched_link'} = 'eur-lex.europa.eu/legal-content/';
	#$options{'searched_link'} = 'eur-lex.europa.eu/LexUriServ/site';
	##$options{'re_url_part'}   = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/bgbl(\d+s\d+[a-z]?)\.?pdf/;
	##$options{'re_url_part'}   = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/b(\d+)[a-z]\.?pdf/;
	#$options{'re_url_part'}   = qr/217.160.60.235\/BGBL\/bgbl1f\/b(?:gbl)?(\d+s\d+[a-z]?|\d+)[a-z]?\.?pdf/;
	#$options{'re_url_part'}   = qr/eur-lex.europa.eu\/legal-content\/.*?uniserv/;
	#$options{'re_url_part'}   = qr/eur-lex.europa.eu\/LexUriServ\/site\//;
	#$options{'delete_link'}   = undef;
	#$options{'replacement'}   = # function disabled, has to be re-implemented
	# $from = qr/(\Qeur-lex.europa.eu\/legal-content\/\E[A-Z]{2}\/TXT\/PDF\/\?uri=)uniserv(:OJ\.[A-Z_]+\.[0-9]{4}\.[0-9]+\.01)([0-9]{4}\.01\.[A-Z]{3})$/;
	# $to   = 'http://'.$1.'uriserv'.$2.'.'.$3;
	# $from = qr/www\.bgblportal\.de\/BGBL\/bgbl1f\/bgbl(\d+s\d+[a-z]?)\.?pdf/;
	# $to   = 'http://www.bgbl.de/Xaver/start.xav?startbk=Bundesanzeiger_BGBl&start=//*%5B\@attr_id=%27bgbl'.$1.'.pdf%27%5D';
	#$options{'summary'}       = 'link fixes, see [[WP:FZW#Mehrere_Hundert_Weblinks_auf_EUR-Lex_defekt]]';

	# 2. linkfix
	#$options{'searched_link'} = '*.google.com';
	#$options{'re_url_part'}   = qr/[a-z0-9]+\.google\.[a-z]+\/.*?[?&]url=[^&]+/;
	#$options{'replacement'}   = sub {
	# $from = qr/[a-z0-9]+\.google\.[a-z]+\/.*?[?&]url=([^&]+)/;
	# $to   = $1;
	#$options{'summary'}       = 'resolve google redirects, see [[mTalk:Spam_blacklist#Google_redirect_spam]]';
	
	# 3. linkfix: remove double http://
	#$options{'searched_link'} = "http//";
	#$options{'summary'}       = 'link fixes, siehe [[Wikipedia:Bots/Anfragen/Archiv/2011-1#fehlerhafte_externe_links_mit_doppeltem_protokoll]]';
	#$options{'re_url_part'}   = qr/./;
	
	# 4. delete/unlink blacklisted links
	my $simple_domain         = 'example.com';
	$options{'searched_link'} = "*.$simple_domain";
	$options{'re_url_part'}   = qr/(?:[a-zA-Z0-9-]+\.)?(?:\Q$simple_domain\E)/;
	$options{'delete_link'}   = 1;
	#$options{'ref2deadlink'}  = 0;
	#$options{'summary'}       = "website is not compatible with WP:EL and content has changed, see [[WP:SBL#$simple_domain]]";
	$options{'summary'}       = "domain is on blacklist, see [[WP:SBL#$simple_domain]]";
	#$options{'summary'}       = "1. link fixes; 2. domain $simple_domain is on blacklist, see [[WP:SBL#$simple_domain]]";
	$camelbot->link_replacement(\%options);
}

if(defined $params->{'parse'}){
	# filenames
	my $filename = $params->{'parse'};
	my $htmlfilename = $filename;
	$htmlfilename =~s/\.[a-zA-Z0-9_-]*$/.html/;
	$htmlfilename = $filename.'.html' if $htmlfilename eq $filename;
	# read wikitext
	my $wikitext = slurp $filename;
	# convert to html
	$camelbot->{'mw_api'}->{config}->{api_url} = $camelbot->{'mw_apiurl'};
	my $html = $camelbot->parse_wikitext($wikitext);
	my $html_cleaned = $camelbot->post_process_html($html);
	$html_cleaned=~s/\n\K\n+//g;
	$html_cleaned=~s/.*?<body>\n?//sg;
	$html_cleaned=~s/<\/body>\n<\/html>\s*$//sg;
	# write result
	write_file($htmlfilename, {binmode => ':utf8'}, $html_cleaned);
}

if($params->{'rc-monitoring'}){
	$camelbot->{'mw_api'}->{config}->{api_url} = $camelbot->{'mw_apiurl'};
	if($params->{'rc-monitoring'} eq 'irc'){
		$camelbot->update_edit_filter_index();
		my $username = 'CamelBot';
		my $bot_irc = CamelBotIRC->new($camelbot,{
			server   => 'irc.wikimedia.org',
			channels => ['#de.wikipedia'],
			nick     => $username,
			username => $username,
			name     => $username,
			alias    => 'rc',
			# no_run => 1,
		});
		#$bot_irc->get_rc_via_db();
		$bot_irc->run();
		#use POE;
		#$poe_kernel->run();
	}elsif($params->{'rc-monitoring'} eq 'db'){
		#$camelbot->update_edit_filter_index();
		my $camelbot_rc = CamelBotRC->new($camelbot);
		$camelbot_rc->db_rc_monitoring();
	}else{
		$camelbot->msg(0, 'not implemented', 'error');
	}
	$camelbot->msg(1, 'leaving');
}

if(defined $params->{'save-as-html'}){
	$camelbot->{'mw_api'}->{config}->{api_url} = $camelbot->{'mw_apiurl'};
	my ($html, $title, $headhtml) = $camelbot->parse_page($params->{'save-as-html'});
	# set html title
	$headhtml=~s/<title>\K[^<]+(?=<\/title>)/$title/;
	# delete js and other useless stuff
	$headhtml =~ s/<script\b[^>]*>.*?<\/script>//gs;
	$headhtml =~ s/<link rel="(?:search|alternate|EditURI|shortcut icon)" [^>]+>//g;
	$headhtml =~ s/<meta name="(?:generator|ResourceLoaderDynamicStyles)"[^>]+>//g;
	$headhtml =~ s/\n\/\* cache key: .*?\*\///g;
	$headhtml =~ s/\n\K\n+//g;
	# first heading
	$headhtml .= "\n".'<h1 id="firstHeading" class="firstHeading"><span dir="auto">'.$title.'</span></h1>'."\n";
	# get css files
	my $css_directory = 'css';
	my $images_directory = 'images';
	mkdir $images_directory unless -d $images_directory;
	mkdir $css_directory unless -d $css_directory;
	$headhtml =~ s/<link rel="stylesheet" href=\K"([^"]*)"/
		$camelbot->download_css($1, $css_directory, $images_directory)/ge;
	# get image files
	my @images = ($html =~ /<a href="\/wiki\/File:([^"]+)" [^>]*\bclass="image"/g);
	my $no_warn_files = [];
	$camelbot->download_files(\@images, $no_warn_files, "$images_directory/");
	# remove navbar
	$html =~ s/<tr>\s*<td[^>]*>\s*<div [^>]*class="[^"]*navbar[^"]*"[^>]*>\s*.*?<\/div>\s*<\/td>\s*<\/tr>//s;
	# remove edit-links
	$html =~ s/<span class="editsection">\[<a href="[^"]*\/index.php?[^"]*\baction=edit[^"]*"[^>]*>(?i:edit|bearbeiten)<\/a>\]<\/span> *//g;
	# clean up html source
	my $html_cleaned = $camelbot->post_process_html($headhtml.$html, "$images_directory/");
	# remove icon-like images ("magnify")
	$html_cleaned =~ s/<div class="magnify">\s*<img src="[^"]*"[^>]*>\s*<\/div>//gs;
	# change internal links
	$html_cleaned =~ s/<a [^>]*\bhref="\K\/wiki\/([^"]+)/$1.html/g;
	# save to file
	my $htmlfilename = $params->{'save-as-html'}.'.html';
	$htmlfilename =~y/ /_/;
	# create directory if necessary
	make_path($1) if $htmlfilename =~/^(.*)\/[^\/]*$/;
	write_file($htmlfilename, {binmode => ':utf8'}, $html_cleaned);
}

if(defined $params->{'search-sbl-blocked-log'}){
	my $blocked_edits = $camelbot->search_sbl_blocked_log(
		qr/$params->{'search-sbl-blocked-log'}/);
	for my $edit(@$blocked_edits){
		print "$edit->{'timestamp'}, $edit->{'url'}, $edit->{'user'}; $edit->{'title'}\n";
	}
}

if(defined $params->{'search-sbl-blocked-log-lang'}){
	my @langs = qw(ca ceb de en es fr it ja nl no pl pt ru sv uk vi war zh);
	my %results = ();
	for my $lang(@langs){
		$results{$lang} = $camelbot->search_sbl_blocked_log(
			qr/$params->{'search-sbl-blocked-log-lang'}/, $lang);
	}
	while(my ($lang, $blocked_edits) = each %results){
		$camelbot->msg(1, "$lang: ".scalar(@$blocked_edits));
		for my $edit(@$blocked_edits){
			print "$edit->{'timestamp'}, $edit->{'url'}, $edit->{'user'}; $edit->{'title'}\n"; 
		}
	}
}

if($params->{'text-replacement'}){
	my %options;
# choose pages
# by user
	#$options{'username'}      = 'seth';
	#$options{'ucstart'}       = '2011-12-14T00:00:00Z';
	#$options{'ucend'}         = undef; #'2010-12-14T00:00:00Z';
	#$options{'uclimit'}       = 269;
	#$options{'pages'}         = [keys %{$camelbot->get_user_contribs(\%options)}];
# by whatlinkshere
	use utf8;
	$options{'pages'}         = [
		map {$_->{'title'}} (
			$camelbot->{'mw_bot'}->what_links_here('Perl/Module/Einführung in Perl-Module'),
			$camelbot->{'mw_bot'}->what_links_here('Perl/Module/Einführung in Perl-Modul'),
			$camelbot->{'mw_bot'}->what_links_here('Perl/Module/Hinweise zum Arbeiten mit Modulen'),
			$camelbot->{'mw_bot'}->what_links_here('Perl/Listen bzw. Arrays (Variablen)'),
			$camelbot->{'mw_bot'}->what_links_here('Perl/Hashes (Variablen)'),
			$camelbot->{'mw_bot'}->what_links_here('Doku:Perl/Module/CPAN-Module'),
			$camelbot->{'mw_bot'}->what_links_here('Perl/Skalare (Variablen)'),
		)
	];
# or explicite
	#$options{'pages'}         = [
	#	'Perl',
	#	'Perl Dingens',
	#];

	print Dumper $options{'pages'};

# search and replace
#  case 0: link replacement
	#$options{'pattern'}       = [
	#	qr/<nowiki>(http:\/\/world.guns.ru\b[^\]]*?\].*?)<\/nowiki>&nbsp;<small>'''\(Achtung: Bitte diese Website nicht aufrufen, da sie gef.hrliche Software verbreitet!\)'''<\/small>/,
	#	qr/<nowiki>(http:\/\/world.guns.ru\b.*?)<\/nowiki>&nbsp;<small>'''\(Achtung: Bitte diese Website nicht aufrufen, da sie gef.hrliche Software verbreitet!\)'''<\/small>/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]]+\])/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)( - '''erledigt)/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+?)( auf Modern Firearms \('*en(?:gl.*?)?\))/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+?)( \('*en(?:gl.*?)?\))/,
	#	qr/(\*\s*)(http:\/\/world.guns.ru\b[^ \]]* [^\[\]\n]+)/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)\s*\n/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*),/,
	#	qr/(?<!\[)(http:\/\/world.guns.ru\b[^ \]]*)/,
	#]; # search patterns s/X//
	#$options{'replacement'}   = [
	#	'"[".$1',
	#	'$1',
	#	'"[".$1',
	#	'"[".$1."]".$2',
	#	'"[".$1."]".$2',
	#	'"[".$1."]".$2',
	#	'$1."[".$2."]"',
	#	'"[".$1."]"',
	#	'"[".$1."],"',
	#	'"[".$1."]"',
	#];   # replace texts s//X/e
#  case 1: internal linkfixes
	$options{'pattern'}       = [
		qr/\[\[Perl\/Module\/Standardmodule\K[_ ]von[_ ]Perl/,
		qr/\[\[Perl\/Module\/Einführung[_ ]in[_ ]Perl-Module?/,
		qr/\[\[Perl\/Module\/Hinweise[_ ]zum[_ ]Arbeiten[_ ]mit[_ ]Modulen/,
		qr/\[\[Doku:Perl\/Module\/CPAN-Module/,
		qr/\[\[Perl\/Skalare\K[_ ]\(Variablen\)/,
		qr/\[\[Perl\/Hashes\K[_ ]\(Variablen\)/,
		qr/\[\[Perl\/Listen[_ ]bzw\.[_ ]Arrays\K[_ ]\(Variablen\)/,
	]; # search patterns s/X//
	$options{'replacement'}   = [
		"''",
		"'[[Perl/Module/Einführung'",
		"'[[Perl/Module/Einführung'",
		"'[[Perl/Module/Einführung'",
		"''",
		"''",
		"''",
	]; # replace texts s//X/e

	# edit behaviour
	$options{'max_edits'}     = -1;  # maximum number of edits (-1 = inf)
	$options{'skip_edits'}    = 0;  # skip number of pages
	$options{'summary'}       = 'linkfixes (grmpf utf-8-gedoens)';
	$camelbot->text_replacement(\%options);
}

if($params->{'update-editfilter-index'}){
	$camelbot->update_edit_filter_index();
}

if($params->{'update-table'}){
	# get information somehow
	# write all information to a table structure
	my $tables;
	# table header
	$tables->[0]{'header'} = 1;
	$tables->[0]{'section'} = 'somesection';
	$tables->[0]{'body'}[0] = ['row0', 'row1', 'row2'];
	$tables->[0]{'class'} = 'wikitable';
	# table body
	my $col = 1;
	#for(sort keys %packages){
	#	$tables->[0]{'body'}[$col][0] = '[['.$packages{$_}{'name'}.']]';
	#	$tables->[0]{'body'}[$col][1] = $packages{$_}{'archs'};
	#	$tables->[0]{'body'}[$col][2] = $packages{$_}{'description'};
	#	++$col;
	#}
	# select page, section, ...
	my $page    = '';
	my $summary = 'table updated';
	$camelbot->rebuild_table($page, $tables, $summary);
}

if($params->{'upload'}){
	my $source  = $params->{'usource'};
	my $summary = $params->{'usummary'};
	my $dest    = $params->{'udest'};
	die "error in upload: source file not defined!\n" unless defined $source;
	die "error in upload: file '$source' not found!\n" unless -e $source;
	die "error in upload: summary not defined!\n" unless defined $summary;
	die "error in upload: destination filename not defined!\n" unless defined $dest;
	$camelbot->msg(1, "uploading file '$source' as '$dest' in wiki");
	$camelbot->upload_file($source, $summary, $dest);
}

if($params->{'usercontribs'}){
	my %options;
	$options{'username'}      = 'seth';
	$options{'ucstart'}       = '2011-12-14T00:00:00Z';
	$options{'ucend'}         = undef; #'2010-12-14T00:00:00Z';
	$options{'uclimit'}       = 269;
	$camelbot->get_user_contribs(\%options);
}

$camelbot->msg(2, 'finished');

__END__

=head1 NAME

camelbot manipulates wiki pages

=head1 DESCRIPTION

this program is a CLI tool for manipulation pages of a MediaWiki. One of the main 
tasks is the replacement of external links (urls)

=head1 SYNOPSIS

camelbot [options]

general options:

     --http-status=s           check http status of given url
                                (status 200 or 301)
 -t, --test                    don't change anything, just print possible changes

mediawiki/wikipedia options:

     --cat-add                 add a given bunch of pages to a given category
 -c, --cat-change              replaces categories 
     --delete                  delete some pre-defined pages
     --download-by-prefix=s    download pages (and used images) with given prefix
     --irc                     short for --rc-monitoring=irc
 -l, --link-replacement        replaces links 
     --text-replacement        replaces text 
 -m, --minor=[01]              mark edit(s) as minor (1) or not (0), default = 1
     --parse=s                 parses wikitext from file and saves result as new 
                                file
     --rc-monitoring=s         start monitoring of recent changes (in w:de) via
                                irc - an irc bot
                                db  - an db connection
     --save_as_html=s          saves a given wiki page as local html-file
     --search-sbl-blocked-log=s search the list of blocked edits (blocked by sbl) 
                                  for a given url-regexp
     --search-sbl-blocked-log-lang=s search the list of blocked edits (blocked by sbl)
                                  for a given url-regexp. search in several wikis.
 -t, --test                    don't change anything, just print possible changes
     --update-editfilter-index update edit filter index (at w:de)
     --upload                  upload a file
                               iff this param is set, you should additionally set 
                               the following params, too.
       --usource=s              source filename (e.g. '../somefile.txt')
       --usummary=s             a summary/description of the file
                                 (e.g. "robot cat with hat\\n\\n[[Category:Nonsense]]")
       --udest=s                destination filename (e.g. 'a_descriptive_name.txt')
     --usercontribs            fetch user contributions
     --username=s              login as different user (default = shell login name)

meta options:

 -V, --version                 display version and exit.
 -h, --help                    display brief help
     --man                     display long help (man page)
 -q, --silent                  same as --verbose=0
 -v, --verbose                 same as --verbose=1 (default)
 -vv,--very-verbose            same as --verbose=2
 -v, --verbose=x               grade of verbosity
                                x=0: no output
                                x=1: default output
                                x=2: much output

=head1 EXAMPLES

camelbot -cl
  replaces links and cats.

=head1 OPTIONS

=head2 GENERAL

=over 8

=item B<--cat-add>

add a given bunch of pages to a given category.

=item B<--cat-change>, B<-c>

replaces categories 

=item B<--delete>

delete some pre-defined pages

=item B<--download-by-prefix>=I<string>

download pages by given prefix and save the pages wieth the extension '.wikitext'.
all images used in those pages will be downloaded too.

=item B<--http-status>=I<string>

prints the http response status code of a given URL I<string>. If I<string> 
contains spaces, they are treated as separators between multiple URLs.

=item B<--irc>

same as B<--rc-monitoring>=irc

=item B<--link-replacement>, B<-l>

replace links

=item B<--minor>, B<--no-minor>

mark edit(s) as minor or not, default = B<--minor>.

=item B<--parse>=I<filename>

parses wikitext from file and saves result to file with same name but with 
extension .html.

=item B<--rc-monitoring>=I<type>

start monitoring of recent changes (in de-wikipedia) via an irc bot (I<type>==irc) 
or a db (I<type>==db) connection to a "real time" db.

=item B<--save-as-html>=I<pagename>

parses wikitext of given page and saves result to file with same name but with 
extension .html.

=item B<--search-sbl-blocked-log>=I<regexp>

search the log of edits blocked by sbl for a given url-regexp I<regexp>

=item B<--search-sbl-blocked-log-lang>=I<regexp>

search the log of edits blocked by sbl for a given url-regexp I<regexp>. search
several wikis.

=item B<--test>, B<-t>

don't change anything, just print possible changes.

=item B<--text-replacement>

replace content of pages

=item B<--update-editfilter-index>

update overview of discussions concerning single rules of edit filter (in w:de)

=item B<--upload>

upload a file iff this param is set, you should additionally set the following 
params, too.

=item B<--usource>=I<string>

requires B<--upload> to be set.
source filename (e.g. '../somefile.txt')

=item B<--usummary>=I<string>

requires B<--upload> to be set.
a summary/description of the file (e.g. "robot cat with hat\\n\\n[[Category:Nonsense]]")

=item B<--udest>=I<string>

requires B<--upload> to be set.
destination filename (e.g. 'a_descriptive_name.txt')

=item B<--username=>I<string>

login as different user. default: I<string> = shell login name

=back

=head2 META

=over 8

=item B<--version>, B<-V>

prints version and exits.

=item B<--help>, B<-h>, B<-?>

prints a brief help message and exits.

=item B<--man>

prints the manual page and exits.

=item B<--verbose>=I<number>, B<-v> I<number>

set grade of verbosity to I<number>. if I<number>==0 then no output
will be given, except hard errors. the higher I<number> is, the more 
output will be printed. default: I<number> = 1.

=item B<--silent, --quiet, -q>

same as B<--verbose=0>.

=item B<--very-verbose, -vv>

same as B<--verbose=2>. you may use B<-vvv> for B<--verbose=3> a.s.o.

=item B<--verbose, -v>

same as B<--verbose=1>.

=back

=head1 LICENCE

Copyright (c) 2014, seth
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

originally written by seth (see https://github.com/wp-seth/camelbot)

=cut

